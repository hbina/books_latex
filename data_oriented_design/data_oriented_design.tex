\documentclass[a4paper,12pt]{book}
\usepackage[a4paper, total={6in, 6in}]{geometry}
\usepackage[utf8]{inputenc}

\author{Hanif Bin Ariffin rewrites}
\title{Data Oriented Design}

\begin{document}

\maketitle
\tableofcontents

\newpage
\section{Data-Oriented Design}

Data-oriented design has been around for decades in one form or another but was only officially given a name by Noel Llopis in his September 2009 article [---link to article----] of the same name.
Whether it is, or not a programming paradigm is seen as contentious.
Many believe it can be used side by side with other programming paradigms such as object-oriented, procedural, or functional programming.
In one respect they are right, data-oriented design can function alongside the other paradigms, but that does not preclude it from being a way to approach programming in the large.
Other programming paradigms are known to function alongside each other to some extent as well.
A Lisp programmer knows that functional programming can coexist with objected-oriented programming and a C programmer is well aware that object-oriented programming can coexist with procedural programming.
We shall ignore these comments and claim data-oriented design as another important tool;
a tool just as capable of coexistence as the rest.

The time was right in 2009.
The hardware was ripe for a change in how to develop.
Potentially very fast computers were hindered by a hardware ignorant programming paradigm.
The way game programmers coded at the time made many engine programmers weep.
The time have changed.
Many mobile and desktop solutions now seem to need the data-oriented design approach less, not because the machines are better at mitigating an ineffective approach, but the games designed are less demanding and less complex.
The trend for mobile seems to be moving to AAA development, which should bring the return of a need for managing complexity and getting the most out of the hardware.

As we now live in a world where multi-core machines include the ones in our pockets, learning how to develop software in a less serial manner is important.
Moving away from objects messaging and getting responses immediately is part of the benefits available to the data-oriented programmer.
Programming, with a firm reliance on awareness of the data flow, sets you up to take the next step to GPGPU and other compute approaches.
This leads to handling workloads that bring game titles to life.
The need for data-oriented design will only grow.
It will grow because abstractions and serial thinking will be the bottleneck of your competitors, and those that embrace the data-oriented approach will thrive.

\subsection{It's All About The Data}

Data is all we have.
Data is what we need to transform in order to create a user experience.
Data is what we load when we open a document.
Data is the graphics on the screen, the pulses from the buttons on your gamepad, the cause of your speakers producing waves in the air, the method by which you level up and how the bad guy knew where you were so as to shoot at you.
Data is how long the dynamite took to explode and how many rings you dropped when you fell on spikes.
It is the current position and velocity of every particle in the beautiful scene that ended the game which was loaded off the disc and into your life via transformations by machinery driven by decoded instructions themselves ordered by assemblers instructed by compilers fed with source-code.

No application is anything without its data.
Adobe Photoshop without the images is nothing.
It's nothing without the brushes, the layers, the pen pressure.
Microsoft Word is nothing without the characters, the fonts, the page breaks.
FL Studio is worthless without the events.
Visual Studio is nothing without source.
All the applications that have ever been written, have been written to output data based on some input data.
The form of that data can be extremely complex, or so simple it requires no documentation at all, but all applications produce and need data.
If they don't need recognizable data, then they are toys or tech demos at best.

Instructions are data too.
Instructions take up memory, use up bandwidth, and can be transformed, loaded, saved and constructed.
It's natural for a developer to not think of instructions as being data, but there is very little differentiating them on older, less protective hardware.
Even though memory set aside for executables is protected from harm and modification on most contemporary hardware, this relatively new invention is still merely an invention, and the modified Harvard architecture relies on the same memory data as it does for instructions.
Instructions are therefore still data, and they are what we transform too.
We take instructions and turn them into actions.
The number, size, and frequency of them is something that matters.
The idea that we control over which instructions we use to solve problems leads us to optimizations.
Applying our knowledge of what the data is allows us to make decisions and which can be replaced with equivalent but less costly alternatives.

This forms the basis of the argument for a data-oriented approach to development, but leaves out one major element.
All this data and the transforming data, from strings, to images, to instructions, they all have to run on something.
Sometimes that thing is quite abstract, such as virtual machine running on unknown hardware.
Sometimes that thing is concrete, such as knowing which specific CPU and GPU you have, and the memory capacity and bandwidth you have available.
But in all cases, the data is not just data, but data that exists on some hardware somewhere, and it has to be transformed at the same hardware.
In essence, data-oriented design is the practice of designing software by developing transformation for well-formed data where the criteria for well-formed is guarded by the target hardware and the patterns and types of transformation that need to operate on it.
Sometimes the data isn't well defined, and sometimes the hardware is equally evasive, but in most cases a good background of hardware appreciation can help out almost every software project.

\subsection{Data Is Not The Problem Domain}

The first principle : Data is not the problem domain.

For some, it would seem that data-oriented design is the antithesis of most other programming paradigms because data-oriented design is a technique that does not readily allow the problem domain to enter into the software as written in source.
It does not promote the concept of an object as a mapping to the context of the user in any way, as data is intentionally and consistently without meaning.
Abstraction heavy paradigms try to pretend the computer and its data do not exist at every turn, abstracting away the idea that they are bytes, or CPU pipelines, or other hardware features, and instead bringing the model of the problem into the program.
They regularly bring either the model of the view into the code, or the model of the world as a context for the problem.
That is, either structure the code attrivutes of the expected solution, or they structure the code around the description of the problem domain.

Meaning can be applied to data to create information.
Meaning is not inherent in data.
When you say 4, it means very little, but say 4 miles, or 4 eggs, it means something.
When you have a list of positions in a game, they mean very little without context. Object-oriented design would likely have the positions as part of an object, and by the class name and neighbouring data (also named) you can get an idea of what thata data means.
Without the connected named contextualising data, the positions could be interpreted in a number of different ways, the data could be interpreted in a number of different ways, and though putting the numbers in context is good in some sense, it also blocks thinking about the positions as just sets of three numbers, which can be important for thinking of solutions to the real problems the programmers are trying to solve.

For an example of what can happen when you put data so deep inside an object that you forget its impact, consider the numerous games released, and in production, where a 2D or 3D grid system could have been used for the data layout, but for unknown reasons the developers kept with the object paradigm for each entity on the map.
This isn't a singular event, and real shipping games have seen this object-centric approach commit crimes against the hardware by having hundreds of objects placed in WorldSpace at grid coordinates at grid coordinates, rather than actually being driven by grid.
It's possible that programmers look at a grid, see the number of elements required to fulfil the request, and are hesitant to the idea of allocating it in a single lump of memory.
Consider a simple 256 by 256 tilemap requiring 65,536 tiles.
An object-oriented programmer may think about those sixty-five thousand objects as being quite expensive.
It might make more sense for them to allocate the objects for the tiles only when necessary, even to the point where there are literally 65, 0000 tiles created by hand in editor, but because they were placed by hand, their necessity has been established, and they are now something to be handled, rather than something potentially worrying.

Not only is this pervasive lack of an underlying form a poor way to handle rendering and simple element placement, but it leads to much higher complexity when interpreting locality of elements.
Gaining access to elements on a grid-free representations often requires jumping through hoops such as having neighbour links (which need to be kept up to date), running through the entire list of elements (inherently costly) or references to an auxillary augmented grid objects or spatial mapping connecting to the objects which are otherwise free to move, but won't, due to the design of the game.
This fake form of freedom introduced by the grid-free design presents issues with understanding the data, and has been the cause of some significant performance penalties in some titles.
Thus also causing significant waste of programmer mental resources in all.

Other than not having  grids where they make sense, many modern games also seem to carry instances for each and every item in the game.
An instance for each rather than a variable storing the number of items.
For some games this is an optimization, as creation and destruction of objects is a costly acitivty, but the trend is worrying, as these ways of stroing information about the world make the world impenetrable to simple interogation.

Many games seem to try to keep everything about the player in the player class.
If the player dies in-game, they have to hang around as dead object, otherwise, the lose access to their achievement data.
This linking of what the data is, to where it resides and what it shares lifetime with, causes monotlithic classes and hard to untangle relationships which frequenctly turn out to be the cause of bugs.
I will not name any of the games, but it's not just one title, nor just one studio, but an epidemic of poor technical design that seems to infect those who use off the shelf object-oriented engines more than those who develop their own regardless of paradigm.

\subsubsection{Summary}

There is no casual/easy mapping that can be made between real world objects and implementation details that programmers and computers have to deal with.
Concepts like bananas, gorillas, forests and the sort cannot be categorized easily.
Even if they can be, computations on them are not easily done by computers.
If you ever look at commercial code that utilize object-oriented paradigm, they can easily be bloated by multiple internal variables and functions that operate on 1 or more of these variables.
These will inevitably lead to states, and states make debugging and intention very hard to internalize.

One might argue that that is simply a case of \textit{poorly designed} structures and that is exactly the point.
Object-oriented programming allures programmers to think in a way that is not scalable and maintainable over time.
The object might have been obvious for the first 6 months of development.
However, after adding functions on top of functions in a single object, the object becomes an abomination.

Every functions and member variables added to a class will add a combinatorial amount of states to the object instance.
Consider debugging a segmentation fault (SEGFAULT) problem.
You have traced this SEGFAULT to a single pointer that have been deleted.
A simple solution would have been to check if this variable is null before proceeding to operate on it.
This is a perfectly acceptable solution for many, albeit it adds branching to your operation.

There is a bigger issue here however : there are states in your object, such that it is possible for \textit{someone} to perform an operation on a null pointer.
Thus, there are some cluster of states in your object that is malicious.

Even worse, it is possible that the pointer is not null, but the object it points to have been freed by memory.
This is even worse, because you want to operate on an object, but \textit{someone} else deleted your object!

\subsection{Data And Statistics}

The second principle: Data is the type, frequency, quantity, shape, and probability.

The second statement is that data is not just the structure.
A common misconception about data-oriented design is that it's all about cache misses.
Even if it was all about making sure you never missed the cache, and it was all about structuring your classes so the hot and cold data was split apart, it would be a generally useful addition to your programming toolkit, but data-oriented design is about all aspects of the data.
To write a book on how to avoid cache misses, you need more than just some tips on how to organise your structures, you need a grounding in what is really happening inside your computer when it is running your program.
Teaching that in a book is also impossible as it would only apply to one generation of hardware, and one generation of programming languages, however, data-oriented design is not rooted in just one language and just some unusual hardware, even though the language to best benefit from it is C++, and the hardware to benefit the approach the most is anything with unbalanced bottlenecks.
The schema of the data is important, but the values and how the data is transformed are as important, if not more so.
It is not enough to have some photographs of a cheetah to determine how fast it can run.
You need to see it in the wild and understand the true costs of being slow.

The data-oriented design model is centered around data.
It pivots on live data, real data, data that is also information.
Object-oriented design is centred around the problem definition.
Objects are not real things but abstract representations of the context in which the problem will be solved.
The objects manipulate the data needed to represent them without any consideration for the hardware or the real-world data patterns or quantities.
This is why object-oriented design allows you to quickly build up first versions of applications, allowing you to put the first version of the design document or problem definition directly into the code, and make a quick attempt at a solution.

Data-oriented design takes a different approach to the problem, instead of assuming we know nothing about the hardware, it assumes we know little about the true nature of our problem, and makes the schema of the data a second-class citizen.
Anyone who has written a sizeable piece of software may recognise that the technical structure and the design for a project often changes so much that there is barely any section from the first draft remaining unchanged in the final implementation.
Data-oriented design avoids wasting resources by never assuming the design needs to exist anywhere other than in a document.
It makes progress by providing a solution to the current problem through some high-level code controlling sequences of events and specifying schema in which to give temporary meaning to the data.

Data-oriented design takes its cues from the data which is seen or expected.
Instead of planning for all eventualities, or planning to make things adaptable, there is a preference for using the most probable input to direct the choice of algorithm.
Instead of planning to be extendable, it plans to be simple and replaceable, and get the job done.
Extendable can be added later, with the safety net of unit tests to ensure it remains working as it did while it was simple.
Luckily, there is a way to make your data layout extendable without requiring much thought, by utilising techniques developed many years ago for working with databases.

Database technology took a great turn for the positive when the relational model was introduced.
In the paper \textit{Out of the Tar Pit}, Functional Relational Programming takes it a step further when it references the idea of using relational model data-structures with functional transforms.
These are well defined, and much literature on how to adapt their form to match your requirements is available.

\subsection{Data Can Change}

Data-oriented design is current.
It is not a representation of the history of a problem or a solution that has been brought up to date, nor is it the future, with generic solutions made up to handle whatever will come along.
Holding onto the past will interfere with flexibility, and looking to the future is generally fruitless as programmers are not fortune tellers.
It's the opinion of the author, that future-proof systems rarely are.
Object-oriented design starts to show its weaknesses when designs change in the real-world.

Object-oriented design is known to handle changes to underlying implementation details very well, as these are the expected changes, the obvious changes, and the ones often cited in introductions to object-oriented design.
However, real world changes such as change of user's needs, changes to input format, quantity, frequency, and the route by which the information will travel, are not handled with grace.
It was introduced in \textit{On the Criteria To Be Used in Decomposing Systems into Modules} that the modularisation approach used by many at the time was rather like that of a production line, where elements of the implementation are caught up in the stages of the proposed solution.
These stages themselves would be identified with a current interpretation of the problem.
In the original document, the solution was to introduce a data hiding approach to modularisation, and though it was an improvement, in the later book \textit{Software Pioneers: Contributions to Software Engineering}, D. L. Parnas revisits the issue and reminds us that even though initial software development can be faster when making structural decisions based on business facts, it lays a burden on maintenance and evolutionary development.
Object-oriented design approaches suffer from this inertia inherent in keeping the problem domain coupled with the implementation.
As mentioned, the problem domain, when introduced into the implementation, can help with making decisions quickly, as you can immediately see the impact the implementation will have on getting closer to the goal of solving or working with the problem in its current form.
The problem with object-oriented design lies in the inevitability of change at a higher level.

Designs change for multiple reasons, occasionally including times when they actually haven't.
A misunderstanding of a design, or a misinterpretation of a design, will cause as much change in the implementation as a literal request for change of design.
A data-oriented approach to code design considers the change in design through the lens of understanding the change in the meaning of the data.
The data-oriented approach to design also allows for change to the code when the source of data changes, unlike the encapsulated internal state manipulations of the object-oriented approach.
In general, data-oriented design handles change better as pieces of data and transforms can be more simply coupled and decoupled than objects can be mutated and reused.

The reason this is so, comes from linking the intention, or the aspect, to the data.
When lumping data and functions in with concepts of objects, you find the objects are the schema of the data.
The aspect of the data is linked to that object, which means it's hard to think of the data from another point of view.
The use case of the data, and the real-world or design, are now linked to the data layout through a singular vision implied by the object definition.
If you link your data layout to the union of the required data for your expected manipulations, and your data manipulations are linked by aspects of your data, then you make it hard to unlink data related by aspect.
The difficulty comes when different aspects need different subsets of the data, and they overlap.
When they overlap, they create a larger and larger set of values that need to travel around the system as one unit.
It's common to refactor a class out into two or more classes, or give ownership of data to a different class.
This is what is meant by tying data to an aspect.
It is tied to the lens through which the data has purpose, but with static typed objects that purpose is predefined, a union of multiple purposes, and sometimes carries around defunct relationships.
Some purposes may no longer be required by the design.
Unfortunately, it's easier to see when a relationship needs to exist, than when it doesn't, and that leads to more connections, not fewer, over time.

If you link your operations by related data, such as when you put methods on a class, you make it hard to unlink your operations when the data changes or splits, and you make it hard to split data when an operation requires the data to be together for its own purposes.
If you keep your data in one place, operations in another place, and keep the aspects and roles of data intrinsic from how the operations and transforms are applied to the data, then you will find that many times when refactoring would have been large and difficult in object-oriented code, the task now becomes trivial or non-existent.
With this benefit comes a cost of keeping tabs on what data is required for each operation, and the potential danger of de-synchronisation.
This consideration can lead to keeping some cold code in an object-oriented style where objects are responsible for maintaining internal consistency over efficiency and mutability.
Examples of places where object-oriented design is far superior to data-oriented can be that of driver layers for systems or hardware.
Even though Vulkan and OpenGL are object-oriented, the granularity of the objects is large and linked to stable concepts in their space, just like the object-oriented approach of the FILE type or handle, in open, close, read, and write operations in filesystems.

A big misunderstanding for many new to the data-oriented design paradigm, a concept brought over from abstraction based development, is that we can design a static library or set of templates to provide generic solutions to everything presented in this book as a data-oriented solution.
Much like with domain driven design, data-oriented design is product and work-flow specific.
You learn how to do data-oriented design, not how to add it to your project.
The fundamental truth is that data, though it can be generic by type, is not generic in how it is used.
The values are different and often contain patterns we can turn to our advantage.
The idea that data can be generic is a false claim that data-oriented design attempts to rectify.
The transforms applied to data can be generic to some extent, but the order and selection of operations are literally the solution to the problem.
Source code is the recipe for conversion of data from one form into another.
There cannot be a library of templates for understanding and leveraging patterns in the data, and that's what drives a successful data-oriented design.
It's true we can build algorithms to find patterns in data, otherwise, how would it be possible to do compression, but the patterns we think about when it comes to data-oriented design are higher level, domain-specific, and not simple frequency mappings.

Our run-time benefits from specialisation through performance tricks that sometimes make the code harder to read, but it is frequently discouraged as being not object-oriented, or being too hard-coded.
It can be better to hard-code a transform than to pretend it's not hard-coded by wrapping it in a generic container and using less direct algorithms on it.
Using existing templates like this provides a benefit of an increase in readability for those who already know the library, and potentially fewer bugs if the functionality was in some way generic.
But, if the functionality was not well mapped to the existing generic solution, writing it with a function template and then extending will make the code harder to understand.
Hiding the fact that the technique had been changed subtly will introduced false assumptions.
Hard-coding a new algorithm is a better choice as long as it has sufficient tests, and is objectively new.
Tests will also be easier to write if you constrain yourself to the facts about concrete data and only test with real, but simple data for your problem, and not generic types on generic data.

\subsection{How Is Data Formed?}

The games we write have a lot of data, in a lot of different formats.
We have textures in multiple formats for multiple platforms.
There are animations, usually optimised for different skeletons or types of playback.
There are sounds, lights, and scripts.
Don't forget meshes, they consist of multiple buffers of attributes.
Only a very small proportion of meshes are old fixed function type with vertices containing positions, UVs, and normals.
The data in game development is hard to box, and getting harder to pin down as more ideas which were previously considered impossible have now become commonplace.
This is why we spend a lot of time working on editors and tool-chains, so we can take the free-form output from designers and artists and find a way to put it into our engines.
Without our tool-chains, editors, viewers, and tweaking tools, there would be no way we could produce a game with the time we have.
The object-oriented approach provides a good way to wrap our heads around all these different formats of data.
It gives a centralised view of where each type of data belongs and classifies it by what can be done to it.
This makes it very easy to add and use data quickly, but implementing all these different wrapper objects takes time.
Adding new functionality to these objects can sometimes require large amounts of refactoring as occasionally objects are classified in such a way that they don't allow for new features to exist.
For example, in many old engines, textures were always 1,2, or 4 bytes per pixel.
With the advent of floating point textures, all that code required a minor refactoring.
In the past, it was not possible to read a texture from the vertex shader, so when texture based skinning came along, many engine programmers had to refactor their render update.
They had to allow for a vertex shader texture upload because it might be necessary when uploading transforms for rendering a skinned mesh.
When the PlayStation2 came along, or an engine first used shaders, the very idea of what made a material had to change.
In the move from small 3D environments to large open worlds with level of detail caused many engineers to start thinking about what it meant for something to need rendering.
When newer hardware became more picky about alignment, other hard to inject changes had to be made.
In many engines, mesh data is optimised for rendering, but when you have to do mesh ray casting to see where bullets have hit, or for doing inverse kinematics (IK), or physics, then you need multiple representations of an entity.
At this point, the object-oriented approach starts to look cobbled together as there are fewer objects that represent real things, and more objects used as containers so programmers can think in larger building blocks.
These blocks hinder though, as they become the only blocks used in thought, and stop potential mental connections from happening.
We went from 2D sprites to 3D meshes, following the format of the hardware provider, to custom data streams and compute units turning the streams into rendered triangles.
Wave data, to banks, to envelope controlled grain tables and slews of layered sounds.
Tilemaps, to portals and rooms, to streamed, multiple levels of detail chunks of world, to hybrid mesh palette, props, and unique stitching assets.
From flip-book to Euler angle sequences, to quaternions and spherical interpolated animations, to animation trees and behaviour mapping/trees.
Change is the only constant.

All these types of data are pretty common if you've worked in games at all, and many engines do provide an abstraction to these more fundamental types.
When a new type of data becomes heavily used it is promoted into engines as a core type.
We normally consider the trade-off of new types being handled as special cases until they become ubiquitous to be one of usability vs performance.
We don't want to provide free access to the lesser understood elements of game development.
People who are not, or can not, invest time in finding out how best to use new features, are discouraged from using them.
The object-oriented game development way to do that is to not provide objects which represent them, and instead only offer the features to people who know how to utilise the more advanced tools.

Apart from the objects representing digital assets, there are also objects for internal game logic.
For every game, there are objects which only exist to further the game-play.
Collectable card games have a lot of textures, but they also have a great deal of rules, card stats, player decks, match records, with many objects to represent the current state of play.
All of these objects are completely custom designed for one game.
There may be sequels, but unless it's primarily a re-skin, it will use quite different game logic in many places, and therefore require different data, which would imply different methods on the now guaranteed to be internally different objects.

Game data is complex.
Any first layout of the data is inspired by the game's initial design.
Once development is underway, the layout needs to keep up with whichever way the game evolves.
Object-oriented techniques offer a quick way to implement any given design, are very quick at implementing each singular design in turn, but don't offer a clean or graceful way to migrate from one data schema to the next.
There are hacks, such as those used in version based asset handlers, or in frameworks backed by update systems and conversion scripts, but normally, game developers change the tool-chain and the engine at the same time, do a full re-export of all the assets, then commit to the next version all in one go.
This can be quite a painful experience if it has to happen over multiple sites at the same time, or if you have a lot of assets, or if you are trying to provide engine support for more than one title, and only one wants to change to the new revision.
An example of an object-oriented approach that handles migration of design with some grace is the Django framework, but the reason it handles the migration well is that the objects would appear to be views into data models, not the data itself.

There have not yet been any successful efforts to build a generic game asset solution.
This may be because all games differ in so many subtle ways that if you did provide a generic solution, it wouldn't be a game solution, just a new language.
There is no solution to be found in trying to provide all the possible types of object a game can use.
But, there is a solution if we go back to thinking about a game as merely running a set of computations on some data.
The closest we can get in 2018 is the FBX format, with some dependence on the current standard shader languages.
The current solutions appear to have excess baggage which does not seem easy to remove.
Due to the need to be generic, many details are lost through abstractions and strategies to present data in a non-confrontational way.

\subsection{What Can Provide A Computational Framework For Such Complex Data}

Game developers are notorious for thinking about game development from either a low level all out performance perspective or from a very high-level gameplay and interaction perspective.
This may have come about because of the widening gap between the amount of code that has to be high performance, and the amount of code to make the game complete.
Object-oriented techniques provide good coverage of the high-level aspect, so the high-level programmers are content with their tools.
The performance specialists have been finding ways of doing more with the hardware, so much so that a lot of the time content creators think they don't have a part in the optimisation process.
There has never been much of a middle ground in game development, which is probably the primary reason why the structure and performance techniques employed by big-iron companies didn't seem useful.
The secondary reason could be that game developers don't normally develop systems and applications which have decade-long maintenance expectations and therefore are less likely to be concerned about why their code should be encapsulated and protected or at least well documented.
When game development was first flourishing into larger studios in the late 1990s, academic or corporate software engineering practices were seen as suspicious because wherever they were employed, there was a dramatic drop in game performance, and whenever any prospective employees came from those industries, they failed to impress.
As games machines became more like the standard micro-computers, and standard micro-computers drew closer in design to the mainframes of old, the more apparent it became that some of those standard professional software engineering practices could be useful.
Now the scale of games has grown to match the hardware, but the games industry has stopped looking at where those non-game development practices led.
As an industry, we should be looking to where others have gone before us, and the closest set of academic and professional development techniques seem to be grounded in simulation and high volume data analysis.
We still have industry-specific challenges such as the problems of high frequency highly heterogeneous transformational requirements that we experience in sufficiently voluminous AI environments, and we have the issue of user proximity in networked environments, such as the problems faced by MMOs when they have location-based events, and bandwidth starts to hit $n^2$ issues as everyone is trying to message everyone else.

With each successive generation, the number of developer hours to create a game has grown, which is why project management and software engineering practices have become standardised at the larger games companies.
There was a time when game developers were seen as cutting-edge programmers, inventing new technology as the need arises, but with the advent of less adventurous hardware (most notably in the x86 based recent 8th generations), there has been a shift away from ingenious coding practices, and towards a standardised process.
This means game development can be tuned to ensure the release date will coincide with marketing dates.
There will always be an element of randomness in high profile game development.
There will always be an element of innovation that virtually guarantees you will not be able to predict how long the project, or at least one part of the project, will take.
Even if data-oriented design isn't needed to make your game go faster, it can be used to make your game development schedule more regular.

Part of the difficulty in adding new and innovative features to a game is the data layout.
If you need to change the data layout for a game, it will need objects to be redesigned or extended in order to work within the existing framework.
If there is no new data, then a feature might require that previously separate systems suddenly be able to talk to each other quite intimately.
This coupling can often cause system-wide confusion with additional temporal coupling and corner cases so obscure they can only be reproduced one time in a million.
These odds might sound fine to some developers, but if you're expecting to sell five to fifty million copies of your game, at one in a million, that's five to fifty people who will experience the problem, can take a video of your game behaving oddly, post it on the YouTube, and call your company rubbish, or your developers lazy, because they hadn't fixed an obvious bug.
Worse, what if the one in a million issue was a way to circumvent in-app-purchases, and was reproducible if you knew what to do and the steps start spreading on Twitter, or maybe created an economy-destroying influx of resources in a live MMO universe.
In the past, if you had sold five to fifty million copies of your game, you wouldn't care, but with the advent of free-to-play games, five million players might be considered a good start, and poor reviews coming in will curb the growth.
IAP circumventions will kill your income, and economy destruction will end you.

Big iron developers had these same concerns back in the 1970s.
Their software had to be built to high standards because their programs would frequently be working on data concerned with real money transactions.
They needed to write business logic that operated on the data, but most important of all, they had to make sure the data was updated through a provably careful set of operations in order to maintain its integrity.
Database technology grew from the need to process stored data, to do complex analysis on it, to store and update it, and be able to guarantee it was valid at all times.
To do this, the ACID test was used to ensure atomicity, consistency, isolation, and durability.
Atomicity was the test to ensure all transactions would either complete or do nothing.
It could be very bad for a database to update only one account in a financial transaction.
There could be money lost or created if a transaction was not atomic.
Consistency was added to ensure all the resultant state changes which should happen during a transaction do happen, that is, all triggers which should fire, do fire, even if the triggers cause triggers recursively, with no limit.
This would be highly important if an account should be blocked after it has triggered a form of fraud detection.
If a trigger has not fired, then the company using the database could risk being liable for even more than if they had stopped the account when they first detected fraud.
Isolation is concerned with ensuring all transactions which occur cannot cause any other transactions to differ in behaviour.
Normally this means that if two transactions appear to work on the same data, they have to queue up and not try to operate at the same time.
Although this is generally good, it does cause concurrency problems.
Finally, durability.
This was the second most important element of the four, as it has always been important to ensure that once a transaction has completed, it remains so.
In database terminology, durability meant the transaction would be guaranteed to have been stored in such a way that it would survive server crashes or power outages.
This was important for networked computers where it would be important to know what transactions had definitely happened when a server crashed or a connection dropped.

Modern networked games also have to worry about highly important data like this.
With non-free downloadable content, consumers care about consistency.
With consumable downloadable content, users care a great deal about every transaction.
To provide much of the functionality required of the database ACID test, game developers have gone back to looking at how databases were designed to cope with these strict requirements and found reference to staged commits, idempotent functions, techniques for concurrent development, and a vast literature base on how to design tables for a database.

\subsection{Conclusions And Takeaways}

We've talked about data-oriented design being a way to think about and lay out your data and to make decisions about your architecture.
We have two principles that can drive many of the decisions we need to make when doing data-oriented design.
To finish the chapter, there are some takeaways you can use immediately to begin your journey.

Consider how your data is being influenced by what it's called.
Consider the possibility that the proximity of other data can influence the meaning of your data, and in doing so, trap it in a model that inhibits flexibility.
For the consideration of the first principle, data is not the problem domain, it's worth thinking about the following items :

\begin{enumerate}
    \item
          What is tying your data together, is it a concept or implied meaning?
    \item
          Is your data layout defined by a single interpretation from a single point of view?
    \item
          Think about how the data could be reinterpreted and cut along those lines.
    \item
          What is it about the data that makes it uniquely important?
\end{enumerate}

You are not targeting an unknown device with unknowable characteristics.
Know your data, and know your target hardware.
To some extent, understand how much each stream of data matters, and who is consuming it.
Understand the cost and potential value of improvements.
Access patterns matter, as you cannot hit the cache if you're accessing things in a burst, then not touching them again for a whole cycle of the application.
For the consideration of the second principle, data is the type, frequency, quantity, shape, and probability, it's worth thinking about the following items :

\begin{enumerate}
    \item
          What is the smallest unit of memory on your target platform?1.6
    \item
          When you read data, how much of it are you using?
    \item
          How often do you need the data? Is it once, or a thousand times a frame?
    \item
          How do you access the data? At random, or in a burst?
    \item
          Are you always modifying the data, or just reading it? Are you modifying all of it?
    \item
          Who does the data matter to, and what about it matters?
    \item
          Find out the quality constraints of your solutions, in terms of bandwidth and latency.
    \item
          What information do you have that isn't in the data per-se? What is implicit?
\end{enumerate}

\newpage
\section{Relational Databases}

In order to lay your data out better, it's useful to have an understanding of the methods available to convert your existing structures into something linear.
The problems we face when applying data-oriented approaches to existing code and data layouts usually stem from the complexity of state inherent in data-hiding or encapsulating programming paradigms.
These paradigms hide away internal state so you don't have to think about it, but they hinder when it comes to reconfiguring data layouts.
This is not because they don't abstract enough to allow changes to the underlying structure without impacting the correctness of the code that uses it, but instead because they have connected and given meaning to the structure of the data.
That type of coupling can be hard to remove.

In this chapter, we go over some of the pertinent parts of the relational model, relational database technology, and normalisation, as these are examples of converting highly complex data structures and relationships into very clean collections of linear storable data entries.

You certainly don't have to move your data to a database style to do data-oriented design, but there are many places where you will wish you had a simple array to work with, and this chapter will help you by giving you an example of how you can migrate from a web of connected complex objects to a simpler to reason about relational model of arrays.

\subsection{Complex State}

When you think about the data present in most software, it has some qualities of complexity or interconnectedness.
When it comes to game development, there are many ways in which the game entities interact, and many ways in which their attached resources will need to feed through different stages of processes to achieve the audio, visual and sometimes haptic feedback necessary to fully immerse the player.
For many programmers brought up on object-oriented design, the idea of reducing the types of structure available down to just simple arrays, is virtually unthinkable.
It's very hard to go from working with objects, classes, templates, and methods on encapsulated data to a world where you only have access to linear containers.

In \textit{A Relational Model of Data for Large Shared Data Banks}, Edgar F. Codd proposed the relational model to handle the current and future needs of agents interacting with data.
He proposed a solution to structuring data for insert, update, delete, and query operations.
His proposal claimed to reduce the need to maintain a deep understanding of how the data was laid out to use it well.
His proposal also claimed to reduce the likelihood of introducing internal inconsistencies.

The relational model provided a framework, and in \textit{Further Normalization of the Data Base Relational Model}, Edgar F. Codd introduced the fundamental terms of normalisation we use to this day in a systematic approach to reducing the most complex of interconnected state information to linear lists of unique independent tuples.

\subsection{What Can Provide A Computational Framework For Complex Data}

Databases store highly complex data in a structured way and provide a language for transforming and generating reports based on that data.
 The language, SQL, invented in the 1970's by Donald D. Chamberlin and Raymond F. Boyce at IBM, provides a method by which it is possible to store computable data while also maintaining data relationships following in the form of the relational model. 
 Games don't have simple computable data, they have classes and objects. 
 They have guns, swords, cars, gems, daily events, textures, sounds, and achievements.
  It is very easy to conclude that database technology doesn't work for the object-oriented approach game developers use.

The data relationships in games can be highly complex, it would seem at first glance that it doesn't neatly fit into database rows.
 A CD collection easily fits in a database, with your albums neatly arranged in a single table.
  But, many game objects won't fit into rows of columns.
   For the uninitiated, it can be hard to find the right table columns to describe a level file.
    Trying to find the right columns to describe a car in a racing game can be a puzzle.
     Do you need a column for each wheel? Do you need a column for each collision primitive, or just a column for the collision mesh?

An obvious answer could be that game data doesn't fit neatly into the database way of thinking.
 However, that's only because we've not normalised the data.
  To show how you can convert from a network model, or hierarchical model to what we need, we will work through these normalisation steps.
   We'll start with a level file as we find out how these decades-old techniques can provide a very useful insight into what game data is really doing.

We shall discover that everything we do is already in a database, but it wasn't obvious to us because of how we store our data.
 The structure of any data is a trade-off between performance, readability, maintenance, future proofing, extendibility, and reuse.
  For example, the most flexible database in common use is your filesystem.
   It has one table with two columns. A primary key of the file path, and a string for the data.
    This simple database system is the perfect fit for a completely future proof system.
     There's nothing that can't be stored in a file.
      The more complex the tables get, the less future proof, and the less maintainable, but the higher the performance and readability.
       For example, a file has no documentation of its own, but the schema of a database could be all that is required to understand a sufficiently well-designed database.
        That's how games don't even appear to have databases.
         They are so complex, for the sake of performance, they have forgotten they are merely a data transform.
          This sliding scale of complexity affects scalability too, which is why some people have moved towards NoSQL databases, and document store types of data storage.
           These systems are more like a filesystem where the documents are accessed by name, and have fewer limits on how they are structured.
            This has been good for horizontal scalability, as it's simpler to add more hardware when you don't have to keep your data consistent across multiple tables that might be on different machines.
             There may come a day when memory is so tightly tied to the closest physical CPU, or when memory chips themselves get more processing power, or running 100 SoCs inside your desktop rig is more effective than a single monolithic CPU, that moving to document store at the high-level could be beneficial inside your app, but for now, there do not seem to be any benefits in that processing model for tasks on local hardware.

We're not going to go into the details of the lowest level of how we utilise large data primitives such as meshes, textures, sounds and such.
 For now, think of these raw assets (sounds, textures, vertex buffers, etc.) as primitives, much like the integers, floating point numbers, strings and boolean values we shall be working with.
  We do this because the relational model calls for atomicity when working with data.
   What is and is not atomic has been debated without an absolute answer becoming clear, but for the intents of developing software intended for human consumption, the granularity can be rooted in considering the data from the perspective of human perception.
    There are existing APIs that present strings in various ways depending on how they are used, for example the difference between human-readable strings (usually UTF-8) and ASCII strings for debugging.
     Adding sounds, textures, and meshes to this seems quite natural once you realise all these things are resources which if cut into smaller pieces begin to lose what it is that makes them what they are.
      For example, half of a sentence is a lot less useful than a whole one, and loses integrity by disassociation.
       A slice of a sentence is clearly not reusable in any meaningful way with another random slice of a different sentence.
        Even subtitles are split along meaningful boundaries, and it's this idea of meaningful boundary that gives us our definition of atomicity for software developed for humans.
         To this end, when working with your data, when you're normalising, try to stay at the level of nouns, the nameable pieces.
          A whole song can be an atom, but so is a single tick sound of a clock.
           A whole page of text is an atom, but so is the player's gamer-tag. 

\subsection{Normalising Your Data}

 TODO : Codify certain keywords.

All tables are made up of rows and columns.
 In a database, each row must be unique.
  This constraint has important consequences.
   When you have normalised your data, it becomes clear why duplicate rows don't make sense, but for now, from a computer programming point of view, consider tables to be more like sets, where the whole row is the set value.
    This is very close to reality, as sets are also not ordered, and a database table is not ordered either.
     There is always some differentiation between rows, even if a database management system (DBMS) has to rely on hidden row ID values.
      It is better to not rely on this as databases work more efficiently when the way in which they are used matches their design.
       All tables need a key.
        The key is often used to order the sorting of the table in physical media, to help optimise queries.
         For this reason, the key needs to be unique, but as small as possible.
          You can think of the key as the key in a map or dictionary.
           Because of the uniqueness rule, every table has an implicit key because the table can use the combination of all the columns at once to identify each row uniquely.
            That is, the key, or the unique lookup, which is the primary key for a table, can be defined as the totality of the whole row.
             If the row is unique, then the primary key is unique.
              Normally, we try to avoid using the whole row as the primary key, but sometimes, it's actually our only choice.
               We will come across examples of that later.

For example, in the mesh table, the combination of meshID and filename is guaranteed to be unique.
 However, currently it's only guaranteed to be unique because we have presumed that the meshID is unique.
  If it was the same mesh, loaded from the same file, it could still have a different meshID.
   The same can be said for the textureID and filename in the textures table.
    From the table [ TODO : TABLE IN LATEX? ] it's possible to see how we could use the type, mesh, texture, tint and animation to uniquely define each Pickup prototype.

Now consider rooms.
 If you use all the columns other than the RoomID of the room table, you will find the combination can be used to uniquely define the room.
  If you consider an alternative, where a row had the same combination of values making up the room, it would in fact be describing the same room.
   From this, it can be claimed that the RoomID is being used as an alias for the rest of the data.
    We have stuck the RoomID in the table, but where did it come from?
     To start with, it came from the setup script.
      The script had a RoomID, but we didn't need it at that stage.
       We needed it for the destination of the doors.
        In another situation, where nothing connected logically to the room, we would not need a RoomID as we would not need an alias to it.

A primary key must be unique.
 RoomID is an example of a primary key because it uniquely describes the room.
  It is an alias in this sense as it contains no data in and of itself, but merely acts as a handle.
   In some cases the primary key is information too, which again, we will meet later.

As a bit of an aside, the idea that a row in a database is also the key can be a core concept worth spending time thinking about.
 If a database table is a set, when you insert a record, you're actually just asking that one particular combination of data is being recorded as existing.
  It is as if a database table is a very sparse set from an extremely large domain of possible values.
   This can be useful because you may notice that under some circumstances, the set of possible values isn't very large, and your table can be more easily defined as a bit set.
    As an example, consider a table which lists the players in an MMO that are online right now.
     For an MMO that shards its servers, there can be limits in the early thousands for the number of unique players on each server.
      In that case, it may be easier to store the currently online players as a bit set.
       If there are at most 10,000 players online, and only 1000 players online at any one time, then the bitset representation would take up 1.25kb of memory, whereas storing the online players as a list of IDs, would require at least 2kb of data if their IDs were shrunk to shorts, or 4kb if they had 32bit IDs to keep them unique across multiple servers.
        The other benefit in this case is the performance of queries into the data
        . To quickly access the ID in the list, you need it to remain sorted.
         The best case then is $ \mathcal{O}(\log{}n)$. In the bitset variant, it's $ \mathcal{O}(1)$

\subsection{Normalisation}
\subsubsection{Primary Keys}
\subsubsection{1st Normal Form}
\subsubsection{2nd Normal Form}
\subsubsection{3rd Normal Form}
\subsubsection{Boyce-Cold Normal Form}
\subsubsection{Domain Key/Knowledge}
\subsubsection{Reflections}
\subsection{Operations}
\subsection{Summing Up}
\subsection{Stream Processing}
\subsection{Why Does Database Technology Matter?}
\section{Existential Processing}
\subsection{Complexity}
\subsection{Debugging}
\subsection{Why Use An If}

\end{document}
