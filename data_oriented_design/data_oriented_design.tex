\documentclass[a4paper,12pt]{book}
\usepackage[a4paper, total={6in, 6in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{listings}

\author{Hanif Bin Ariffin rewrites}
\title{Data Oriented Design}

\begin{document}

\maketitle
\tableofcontents

\newpage
\section{Data-Oriented Design}

Data-oriented design has been around for decades in one form or another but was only officially given a name by Noel Llopis in his September 2009 article [---link to article----] of the same name.
Whether it is, or not a programming paradigm is seen as contentious.
Many believe it can be used side by side with other programming paradigms such as object-oriented, procedural, or functional programming.
In one respect they are right, data-oriented design can function alongside the other paradigms, but that does not preclude it from being a way to approach programming in the large.
Other programming paradigms are known to function alongside each other to some extent as well.
A Lisp programmer knows that functional programming can coexist with objected-oriented programming and a C programmer is well aware that object-oriented programming can coexist with procedural programming.
We shall ignore these comments and claim data-oriented design as another important tool;
a tool just as capable of coexistence as the rest.

The time was right in 2009.
The hardware was ripe for a change in how to develop.
Potentially very fast computers were hindered by a hardware ignorant programming paradigm.
The way game programmers coded at the time made many engine programmers weep.
The time have changed.
Many mobile and desktop solutions now seem to need the data-oriented design approach less, not because the machines are better at mitigating an ineffective approach, but the games designed are less demanding and less complex.
The trend for mobile seems to be moving to AAA development, which should bring the return of a need for managing complexity and getting the most out of the hardware.

As we now live in a world where multi-core machines include the ones in our pockets, learning how to develop software in a less serial manner is important.
Moving away from objects messaging and getting responses immediately is part of the benefits available to the data-oriented programmer.
Programming, with a firm reliance on awareness of the data flow, sets you up to take the next step to GPGPU and other compute approaches.
This leads to handling workloads that bring game titles to life.
The need for data-oriented design will only grow.
It will grow because abstractions and serial thinking will be the bottleneck of your competitors, and those that embrace the data-oriented approach will thrive.

\subsection{It's All About The Data}

Data is all we have.
Data is what we need to transform in order to create a user experience.
Data is what we load when we open a document.
Data is the graphics on the screen, the pulses from the buttons on your gamepad, the cause of your speakers producing waves in the air, the method by which you level up and how the bad guy knew where you were so as to shoot at you.
Data is how long the dynamite took to explode and how many rings you dropped when you fell on spikes.
It is the current position and velocity of every particle in the beautiful scene that ended the game which was loaded off the disc and into your life via transformations by machinery driven by decoded instructions themselves ordered by assemblers instructed by compilers fed with source-code.

No application is anything without its data.
Adobe Photoshop without the images is nothing.
It's nothing without the brushes, the layers, the pen pressure.
Microsoft Word is nothing without the characters, the fonts, the page breaks.
FL Studio is worthless without the events.
Visual Studio is nothing without source.
All the applications that have ever been written, have been written to output data based on some input data.
The form of that data can be extremely complex, or so simple it requires no documentation at all, but all applications produce and need data.
If they don't need recognizable data, then they are toys or tech demos at best.

Instructions are data too.
Instructions take up memory, use up bandwidth, and can be transformed, loaded, saved and constructed.
It's natural for a developer to not think of instructions as being data, but there is very little differentiating them on older, less protective hardware.
Even though memory set aside for executables is protected from harm and modification on most contemporary hardware, this relatively new invention is still merely an invention, and the modified Harvard architecture relies on the same memory data as it does for instructions.
Instructions are therefore still data, and they are what we transform too.
We take instructions and turn them into actions.
The number, size, and frequency of them is something that matters.
The idea that we control over which instructions we use to solve problems leads us to optimizations.
Applying our knowledge of what the data is allows us to make decisions and which can be replaced with equivalent but less costly alternatives.

This forms the basis of the argument for a data-oriented approach to development, but leaves out one major element.
All this data and the transforming data, from strings, to images, to instructions, they all have to run on something.
Sometimes that thing is quite abstract, such as virtual machine running on unknown hardware.
Sometimes that thing is concrete, such as knowing which specific CPU and GPU you have, and the memory capacity and bandwidth you have available.
But in all cases, the data is not just data, but data that exists on some hardware somewhere, and it has to be transformed at the same hardware.
In essence, data-oriented design is the practice of designing software by developing transformation for well-formed data where the criteria for well-formed is guarded by the target hardware and the patterns and types of transformation that need to operate on it.
Sometimes the data isn't well defined, and sometimes the hardware is equally evasive, but in most cases a good background of hardware appreciation can help out almost every software project.

\subsection{Data Is Not The Problem Domain}

The first principle : Data is not the problem domain.

For some, it would seem that data-oriented design is the antithesis of most other programming paradigms because data-oriented design is a technique that does not readily allow the problem domain to enter into the software as written in source.
It does not promote the concept of an object as a mapping to the context of the user in any way, as data is intentionally and consistently without meaning.
Abstraction heavy paradigms try to pretend the computer and its data do not exist at every turn, abstracting away the idea that they are bytes, or CPU pipelines, or other hardware features, and instead bringing the model of the problem into the program.
They regularly bring either the model of the view into the code, or the model of the world as a context for the problem.
That is, either structure the code attrivutes of the expected solution, or they structure the code around the description of the problem domain.

Meaning can be applied to data to create information.
Meaning is not inherent in data.
When you say 4, it means very little, but say 4 miles, or 4 eggs, it means something.
When you have a list of positions in a game, they mean very little without context. Object-oriented design would likely have the positions as part of an object, and by the class name and neighbouring data (also named) you can get an idea of what thata data means.
Without the connected named contextualising data, the positions could be interpreted in a number of different ways, the data could be interpreted in a number of different ways, and though putting the numbers in context is good in some sense, it also blocks thinking about the positions as just sets of three numbers, which can be important for thinking of solutions to the real problems the programmers are trying to solve.

For an example of what can happen when you put data so deep inside an object that you forget its impact, consider the numerous games released, and in production, where a 2D or 3D grid system could have been used for the data layout, but for unknown reasons the developers kept with the object paradigm for each entity on the map.
This isn't a singular event, and real shipping games have seen this object-centric approach commit crimes against the hardware by having hundreds of objects placed in WorldSpace at grid coordinates at grid coordinates, rather than actually being driven by grid.
It's possible that programmers look at a grid, see the number of elements required to fulfil the request, and are hesitant to the idea of allocating it in a single lump of memory.
Consider a simple 256 by 256 tilemap requiring 65,536 tiles.
An object-oriented programmer may think about those sixty-five thousand objects as being quite expensive.
It might make more sense for them to allocate the objects for the tiles only when necessary, even to the point where there are literally 65, 0000 tiles created by hand in editor, but because they were placed by hand, their necessity has been established, and they are now something to be handled, rather than something potentially worrying.

Not only is this pervasive lack of an underlying form a poor way to handle rendering and simple element placement, but it leads to much higher complexity when interpreting locality of elements.
Gaining access to elements on a grid-free representations often requires jumping through hoops such as having neighbour links (which need to be kept up to date), running through the entire list of elements (inherently costly) or references to an auxillary augmented grid objects or spatial mapping connecting to the objects which are otherwise free to move, but won't, due to the design of the game.
This fake form of freedom introduced by the grid-free design presents issues with understanding the data, and has been the cause of some significant performance penalties in some titles.
Thus also causing significant waste of programmer mental resources in all.

Other than not having  grids where they make sense, many modern games also seem to carry instances for each and every item in the game.
An instance for each rather than a variable storing the number of items.
For some games this is an optimization, as creation and destruction of objects is a costly acitivty, but the trend is worrying, as these ways of stroing information about the world make the world impenetrable to simple interogation.

Many games seem to try to keep everything about the player in the player class.
If the player dies in-game, they have to hang around as dead object, otherwise, the lose access to their achievement data.
This linking of what the data is, to where it resides and what it shares lifetime with, causes monotlithic classes and hard to untangle relationships which frequenctly turn out to be the cause of bugs.
I will not name any of the games, but it's not just one title, nor just one studio, but an epidemic of poor technical design that seems to infect those who use off the shelf object-oriented engines more than those who develop their own regardless of paradigm.

\subsubsection{Summary}

There is no casual/easy mapping that can be made between real world objects and implementation details that programmers and computers have to deal with.
Concepts like bananas, gorillas, forests and the sort cannot be categorized easily.
Even if they can be, computations on them are not easily done by computers.
If you ever look at commercial code that utilize object-oriented paradigm, they can easily be bloated by multiple internal variables and functions that operate on 1 or more of these variables.
These will inevitably lead to states, and states make debugging and intention very hard to internalize.

One might argue that that is simply a case of \textit{poorly designed} structures and that is exactly the point.
Object-oriented programming allures programmers to think in a way that is not scalable and maintainable over time.
The object might have been obvious for the first 6 months of development.
However, after adding functions on top of functions in a single object, the object becomes an abomination.

Every functions and member variables added to a class will add a combinatorial amount of states to the object instance.
Consider debugging a segmentation fault (SEGFAULT) problem.
You have traced this SEGFAULT to a single pointer that have been deleted.
A simple solution would have been to check if this variable is null before proceeding to operate on it.
This is a perfectly acceptable solution for many, albeit it adds branching to your operation.

There is a bigger issue here however : there are states in your object, such that it is possible for \textit{someone} to perform an operation on a null pointer.
Thus, there are some cluster of states in your object that is malicious.

Even worse, it is possible that the pointer is not null, but the object it points to have been freed by memory.
This is even worse, because you want to operate on an object, but \textit{someone} else deleted your object!

\subsection{Data And Statistics}

The second principle: Data is the type, frequency, quantity, shape, and probability.

The second statement is that data is not just the structure.
A common misconception about data-oriented design is that it's all about cache misses.
Even if it was all about making sure you never missed the cache, and it was all about structuring your classes so the hot and cold data was split apart, it would be a generally useful addition to your programming toolkit, but data-oriented design is about all aspects of the data.
To write a book on how to avoid cache misses, you need more than just some tips on how to organise your structures, you need a grounding in what is really happening inside your computer when it is running your program.
Teaching that in a book is also impossible as it would only apply to one generation of hardware, and one generation of programming languages, however, data-oriented design is not rooted in just one language and just some unusual hardware, even though the language to best benefit from it is C++, and the hardware to benefit the approach the most is anything with unbalanced bottlenecks.
The schema of the data is important, but the values and how the data is transformed are as important, if not more so.
It is not enough to have some photographs of a cheetah to determine how fast it can run.
You need to see it in the wild and understand the true costs of being slow.

The data-oriented design model is centered around data.
It pivots on live data, real data, data that is also information.
Object-oriented design is centred around the problem definition.
Objects are not real things but abstract representations of the context in which the problem will be solved.
The objects manipulate the data needed to represent them without any consideration for the hardware or the real-world data patterns or quantities.
This is why object-oriented design allows you to quickly build up first versions of applications, allowing you to put the first version of the design document or problem definition directly into the code, and make a quick attempt at a solution.

Data-oriented design takes a different approach to the problem, instead of assuming we know nothing about the hardware, it assumes we know little about the true nature of our problem, and makes the schema of the data a second-class citizen.
Anyone who has written a sizeable piece of software may recognise that the technical structure and the design for a project often changes so much that there is barely any section from the first draft remaining unchanged in the final implementation.
Data-oriented design avoids wasting resources by never assuming the design needs to exist anywhere other than in a document.
It makes progress by providing a solution to the current problem through some high-level code controlling sequences of events and specifying schema in which to give temporary meaning to the data.

Data-oriented design takes its cues from the data which is seen or expected.
Instead of planning for all eventualities, or planning to make things adaptable, there is a preference for using the most probable input to direct the choice of algorithm.
Instead of planning to be extendable, it plans to be simple and replaceable, and get the job done.
Extendable can be added later, with the safety net of unit tests to ensure it remains working as it did while it was simple.
Luckily, there is a way to make your data layout extendable without requiring much thought, by utilising techniques developed many years ago for working with databases.

Database technology took a great turn for the positive when the relational model was introduced.
In the paper \textit{Out of the Tar Pit}, Functional Relational Programming takes it a step further when it references the idea of using relational model data-structures with functional transforms.
These are well defined, and much literature on how to adapt their form to match your requirements is available.

\subsection{Data Can Change}

Data-oriented design is current.
It is not a representation of the history of a problem or a solution that has been brought up to date, nor is it the future, with generic solutions made up to handle whatever will come along.
Holding onto the past will interfere with flexibility, and looking to the future is generally fruitless as programmers are not fortune tellers.
It's the opinion of the author, that future-proof systems rarely are.
Object-oriented design starts to show its weaknesses when designs change in the real-world.

Object-oriented design is known to handle changes to underlying implementation details very well, as these are the expected changes, the obvious changes, and the ones often cited in introductions to object-oriented design.
However, real world changes such as change of user's needs, changes to input format, quantity, frequency, and the route by which the information will travel, are not handled with grace.
It was introduced in \textit{On the Criteria To Be Used in Decomposing Systems into Modules} that the modularisation approach used by many at the time was rather like that of a production line, where elements of the implementation are caught up in the stages of the proposed solution.
These stages themselves would be identified with a current interpretation of the problem.
In the original document, the solution was to introduce a data hiding approach to modularisation, and though it was an improvement, in the later book \textit{Software Pioneers: Contributions to Software Engineering}, D. L. Parnas revisits the issue and reminds us that even though initial software development can be faster when making structural decisions based on business facts, it lays a burden on maintenance and evolutionary development.
Object-oriented design approaches suffer from this inertia inherent in keeping the problem domain coupled with the implementation.
As mentioned, the problem domain, when introduced into the implementation, can help with making decisions quickly, as you can immediately see the impact the implementation will have on getting closer to the goal of solving or working with the problem in its current form.
The problem with object-oriented design lies in the inevitability of change at a higher level.

Designs change for multiple reasons, occasionally including times when they actually haven't.
A misunderstanding of a design, or a misinterpretation of a design, will cause as much change in the implementation as a literal request for change of design.
A data-oriented approach to code design considers the change in design through the lens of understanding the change in the meaning of the data.
The data-oriented approach to design also allows for change to the code when the source of data changes, unlike the encapsulated internal state manipulations of the object-oriented approach.
In general, data-oriented design handles change better as pieces of data and transforms can be more simply coupled and decoupled than objects can be mutated and reused.

The reason this is so, comes from linking the intention, or the aspect, to the data.
When lumping data and functions in with concepts of objects, you find the objects are the schema of the data.
The aspect of the data is linked to that object, which means it's hard to think of the data from another point of view.
The use case of the data, and the real-world or design, are now linked to the data layout through a singular vision implied by the object definition.
If you link your data layout to the union of the required data for your expected manipulations, and your data manipulations are linked by aspects of your data, then you make it hard to unlink data related by aspect.
The difficulty comes when different aspects need different subsets of the data, and they overlap.
When they overlap, they create a larger and larger set of values that need to travel around the system as one unit.
It's common to refactor a class out into two or more classes, or give ownership of data to a different class.
This is what is meant by tying data to an aspect.
It is tied to the lens through which the data has purpose, but with static typed objects that purpose is predefined, a union of multiple purposes, and sometimes carries around defunct relationships.
Some purposes may no longer be required by the design.
Unfortunately, it's easier to see when a relationship needs to exist, than when it doesn't, and that leads to more connections, not fewer, over time.

If you link your operations by related data, such as when you put methods on a class, you make it hard to unlink your operations when the data changes or splits, and you make it hard to split data when an operation requires the data to be together for its own purposes.
If you keep your data in one place, operations in another place, and keep the aspects and roles of data intrinsic from how the operations and transforms are applied to the data, then you will find that many times when refactoring would have been large and difficult in object-oriented code, the task now becomes trivial or non-existent.
With this benefit comes a cost of keeping tabs on what data is required for each operation, and the potential danger of de-synchronisation.
This consideration can lead to keeping some cold code in an object-oriented style where objects are responsible for maintaining internal consistency over efficiency and mutability.
Examples of places where object-oriented design is far superior to data-oriented can be that of driver layers for systems or hardware.
Even though Vulkan and OpenGL are object-oriented, the granularity of the objects is large and linked to stable concepts in their space, just like the object-oriented approach of the FILE type or handle, in open, close, read, and write operations in filesystems.

A big misunderstanding for many new to the data-oriented design paradigm, a concept brought over from abstraction based development, is that we can design a static library or set of templates to provide generic solutions to everything presented in this book as a data-oriented solution.
Much like with domain driven design, data-oriented design is product and work-flow specific.
You learn how to do data-oriented design, not how to add it to your project.
The fundamental truth is that data, though it can be generic by type, is not generic in how it is used.
The values are different and often contain patterns we can turn to our advantage.
The idea that data can be generic is a false claim that data-oriented design attempts to rectify.
The transforms applied to data can be generic to some extent, but the order and selection of operations are literally the solution to the problem.
Source code is the recipe for conversion of data from one form into another.
There cannot be a library of templates for understanding and leveraging patterns in the data, and that's what drives a successful data-oriented design.
It's true we can build algorithms to find patterns in data, otherwise, how would it be possible to do compression, but the patterns we think about when it comes to data-oriented design are higher level, domain-specific, and not simple frequency mappings.

Our run-time benefits from specialisation through performance tricks that sometimes make the code harder to read, but it is frequently discouraged as being not object-oriented, or being too hard-coded.
It can be better to hard-code a transform than to pretend it's not hard-coded by wrapping it in a generic container and using less direct algorithms on it.
Using existing templates like this provides a benefit of an increase in readability for those who already know the library, and potentially fewer bugs if the functionality was in some way generic.
But, if the functionality was not well mapped to the existing generic solution, writing it with a function template and then extending will make the code harder to understand.
Hiding the fact that the technique had been changed subtly will introduced false assumptions.
Hard-coding a new algorithm is a better choice as long as it has sufficient tests, and is objectively new.
Tests will also be easier to write if you constrain yourself to the facts about concrete data and only test with real, but simple data for your problem, and not generic types on generic data.

\subsection{How Is Data Formed?}

The games we write have a lot of data, in a lot of different formats.
We have textures in multiple formats for multiple platforms.
There are animations, usually optimised for different skeletons or types of playback.
There are sounds, lights, and scripts.
Don't forget meshes, they consist of multiple buffers of attributes.
Only a very small proportion of meshes are old fixed function type with vertices containing positions, UVs, and normals.
The data in game development is hard to box, and getting harder to pin down as more ideas which were previously considered impossible have now become commonplace.
This is why we spend a lot of time working on editors and tool-chains, so we can take the free-form output from designers and artists and find a way to put it into our engines.
Without our tool-chains, editors, viewers, and tweaking tools, there would be no way we could produce a game with the time we have.
The object-oriented approach provides a good way to wrap our heads around all these different formats of data.
It gives a centralised view of where each type of data belongs and classifies it by what can be done to it.
This makes it very easy to add and use data quickly, but implementing all these different wrapper objects takes time.
Adding new functionality to these objects can sometimes require large amounts of refactoring as occasionally objects are classified in such a way that they don't allow for new features to exist.
For example, in many old engines, textures were always 1,2, or 4 bytes per pixel.
With the advent of floating point textures, all that code required a minor refactoring.
In the past, it was not possible to read a texture from the vertex shader, so when texture based skinning came along, many engine programmers had to refactor their render update.
They had to allow for a vertex shader texture upload because it might be necessary when uploading transforms for rendering a skinned mesh.
When the PlayStation2 came along, or an engine first used shaders, the very idea of what made a material had to change.
In the move from small 3D environments to large open worlds with level of detail caused many engineers to start thinking about what it meant for something to need rendering.
When newer hardware became more picky about alignment, other hard to inject changes had to be made.
In many engines, mesh data is optimised for rendering, but when you have to do mesh ray casting to see where bullets have hit, or for doing inverse kinematics (IK), or physics, then you need multiple representations of an entity.
At this point, the object-oriented approach starts to look cobbled together as there are fewer objects that represent real things, and more objects used as containers so programmers can think in larger building blocks.
These blocks hinder though, as they become the only blocks used in thought, and stop potential mental connections from happening.
We went from 2D sprites to 3D meshes, following the format of the hardware provider, to custom data streams and compute units turning the streams into rendered triangles.
Wave data, to banks, to envelope controlled grain tables and slews of layered sounds.
Tilemaps, to portals and rooms, to streamed, multiple levels of detail chunks of world, to hybrid mesh palette, props, and unique stitching assets.
From flip-book to Euler angle sequences, to quaternions and spherical interpolated animations, to animation trees and behaviour mapping/trees.
Change is the only constant.

All these types of data are pretty common if you've worked in games at all, and many engines do provide an abstraction to these more fundamental types.
When a new type of data becomes heavily used it is promoted into engines as a core type.
We normally consider the trade-off of new types being handled as special cases until they become ubiquitous to be one of usability vs performance.
We don't want to provide free access to the lesser understood elements of game development.
People who are not, or can not, invest time in finding out how best to use new features, are discouraged from using them.
The object-oriented game development way to do that is to not provide objects which represent them, and instead only offer the features to people who know how to utilise the more advanced tools.

Apart from the objects representing digital assets, there are also objects for internal game logic.
For every game, there are objects which only exist to further the game-play.
Collectable card games have a lot of textures, but they also have a great deal of rules, card stats, player decks, match records, with many objects to represent the current state of play.
All of these objects are completely custom designed for one game.
There may be sequels, but unless it's primarily a re-skin, it will use quite different game logic in many places, and therefore require different data, which would imply different methods on the now guaranteed to be internally different objects.

Game data is complex.
Any first layout of the data is inspired by the game's initial design.
Once development is underway, the layout needs to keep up with whichever way the game evolves.
Object-oriented techniques offer a quick way to implement any given design, are very quick at implementing each singular design in turn, but don't offer a clean or graceful way to migrate from one data schema to the next.
There are hacks, such as those used in version based asset handlers, or in frameworks backed by update systems and conversion scripts, but normally, game developers change the tool-chain and the engine at the same time, do a full re-export of all the assets, then commit to the next version all in one go.
This can be quite a painful experience if it has to happen over multiple sites at the same time, or if you have a lot of assets, or if you are trying to provide engine support for more than one title, and only one wants to change to the new revision.
An example of an object-oriented approach that handles migration of design with some grace is the Django framework, but the reason it handles the migration well is that the objects would appear to be views into data models, not the data itself.

There have not yet been any successful efforts to build a generic game asset solution.
This may be because all games differ in so many subtle ways that if you did provide a generic solution, it wouldn't be a game solution, just a new language.
There is no solution to be found in trying to provide all the possible types of object a game can use.
But, there is a solution if we go back to thinking about a game as merely running a set of computations on some data.
The closest we can get in 2018 is the FBX format, with some dependence on the current standard shader languages.
The current solutions appear to have excess baggage which does not seem easy to remove.
Due to the need to be generic, many details are lost through abstractions and strategies to present data in a non-confrontational way.

\subsection{What Can Provide A Computational Framework For Such Complex Data}

Game developers are notorious for thinking about game development from either a low level all out performance perspective or from a very high-level gameplay and interaction perspective.
This may have come about because of the widening gap between the amount of code that has to be high performance, and the amount of code to make the game complete.
Object-oriented techniques provide good coverage of the high-level aspect, so the high-level programmers are content with their tools.
The performance specialists have been finding ways of doing more with the hardware, so much so that a lot of the time content creators think they don't have a part in the optimisation process.
There has never been much of a middle ground in game development, which is probably the primary reason why the structure and performance techniques employed by big-iron companies didn't seem useful.
The secondary reason could be that game developers don't normally develop systems and applications which have decade-long maintenance expectations and therefore are less likely to be concerned about why their code should be encapsulated and protected or at least well documented.
When game development was first flourishing into larger studios in the late 1990s, academic or corporate software engineering practices were seen as suspicious because wherever they were employed, there was a dramatic drop in game performance, and whenever any prospective employees came from those industries, they failed to impress.
As games machines became more like the standard micro-computers, and standard micro-computers drew closer in design to the mainframes of old, the more apparent it became that some of those standard professional software engineering practices could be useful.
Now the scale of games has grown to match the hardware, but the games industry has stopped looking at where those non-game development practices led.
As an industry, we should be looking to where others have gone before us, and the closest set of academic and professional development techniques seem to be grounded in simulation and high volume data analysis.
We still have industry-specific challenges such as the problems of high frequency highly heterogeneous transformational requirements that we experience in sufficiently voluminous AI environments, and we have the issue of user proximity in networked environments, such as the problems faced by MMOs when they have location-based events, and bandwidth starts to hit $n^2$ issues as everyone is trying to message everyone else.

With each successive generation, the number of developer hours to create a game has grown, which is why project management and software engineering practices have become standardised at the larger games companies.
There was a time when game developers were seen as cutting-edge programmers, inventing new technology as the need arises, but with the advent of less adventurous hardware (most notably in the x86 based recent 8th generations), there has been a shift away from ingenious coding practices, and towards a standardised process.
This means game development can be tuned to ensure the release date will coincide with marketing dates.
There will always be an element of randomness in high profile game development.
There will always be an element of innovation that virtually guarantees you will not be able to predict how long the project, or at least one part of the project, will take.
Even if data-oriented design isn't needed to make your game go faster, it can be used to make your game development schedule more regular.

Part of the difficulty in adding new and innovative features to a game is the data layout.
If you need to change the data layout for a game, it will need objects to be redesigned or extended in order to work within the existing framework.
If there is no new data, then a feature might require that previously separate systems suddenly be able to talk to each other quite intimately.
This coupling can often cause system-wide confusion with additional temporal coupling and corner cases so obscure they can only be reproduced one time in a million.
These odds might sound fine to some developers, but if you're expecting to sell five to fifty million copies of your game, at one in a million, that's five to fifty people who will experience the problem, can take a video of your game behaving oddly, post it on the YouTube, and call your company rubbish, or your developers lazy, because they hadn't fixed an obvious bug.
Worse, what if the one in a million issue was a way to circumvent in-app-purchases, and was reproducible if you knew what to do and the steps start spreading on Twitter, or maybe created an economy-destroying influx of resources in a live MMO universe.
In the past, if you had sold five to fifty million copies of your game, you wouldn't care, but with the advent of free-to-play games, five million players might be considered a good start, and poor reviews coming in will curb the growth.
IAP circumventions will kill your income, and economy destruction will end you.

Big iron developers had these same concerns back in the 1970s.
Their software had to be built to high standards because their programs would frequently be working on data concerned with real money transactions.
They needed to write business logic that operated on the data, but most important of all, they had to make sure the data was updated through a provably careful set of operations in order to maintain its integrity.
Database technology grew from the need to process stored data, to do complex analysis on it, to store and update it, and be able to guarantee it was valid at all times.
To do this, the ACID test was used to ensure atomicity, consistency, isolation, and durability.
Atomicity was the test to ensure all transactions would either complete or do nothing.
It could be very bad for a database to update only one account in a financial transaction.
There could be money lost or created if a transaction was not atomic.
Consistency was added to ensure all the resultant state changes which should happen during a transaction do happen, that is, all triggers which should fire, do fire, even if the triggers cause triggers recursively, with no limit.
This would be highly important if an account should be blocked after it has triggered a form of fraud detection.
If a trigger has not fired, then the company using the database could risk being liable for even more than if they had stopped the account when they first detected fraud.
Isolation is concerned with ensuring all transactions which occur cannot cause any other transactions to differ in behaviour.
Normally this means that if two transactions appear to work on the same data, they have to queue up and not try to operate at the same time.
Although this is generally good, it does cause concurrency problems.
Finally, durability.
This was the second most important element of the four, as it has always been important to ensure that once a transaction has completed, it remains so.
In database terminology, durability meant the transaction would be guaranteed to have been stored in such a way that it would survive server crashes or power outages.
This was important for networked computers where it would be important to know what transactions had definitely happened when a server crashed or a connection dropped.

Modern networked games also have to worry about highly important data like this.
With non-free downloadable content, consumers care about consistency.
With consumable downloadable content, users care a great deal about every transaction.
To provide much of the functionality required of the database ACID test, game developers have gone back to looking at how databases were designed to cope with these strict requirements and found reference to staged commits, idempotent functions, techniques for concurrent development, and a vast literature base on how to design tables for a database.

\subsection{Conclusions And Takeaways}

We've talked about data-oriented design being a way to think about and lay out your data and to make decisions about your architecture.
We have two principles that can drive many of the decisions we need to make when doing data-oriented design.
To finish the chapter, there are some takeaways you can use immediately to begin your journey.

Consider how your data is being influenced by what it's called.
Consider the possibility that the proximity of other data can influence the meaning of your data, and in doing so, trap it in a model that inhibits flexibility.
For the consideration of the first principle, data is not the problem domain, it's worth thinking about the following items :

\begin{enumerate}
      \item
            What is tying your data together, is it a concept or implied meaning?
      \item
            Is your data layout defined by a single interpretation from a single point of view?
      \item
            Think about how the data could be reinterpreted and cut along those lines.
      \item
            What is it about the data that makes it uniquely important?
\end{enumerate}

You are not targeting an unknown device with unknowable characteristics.
Know your data, and know your target hardware.
To some extent, understand how much each stream of data matters, and who is consuming it.
Understand the cost and potential value of improvements.
Access patterns matter, as you cannot hit the cache if you're accessing things in a burst, then not touching them again for a whole cycle of the application.
For the consideration of the second principle, data is the type, frequency, quantity, shape, and probability, it's worth thinking about the following items :

\begin{enumerate}
      \item
            What is the smallest unit of memory on your target platform?1.6
      \item
            When you read data, how much of it are you using?
      \item
            How often do you need the data? Is it once, or a thousand times a frame?
      \item
            How do you access the data? At random, or in a burst?
      \item
            Are you always modifying the data, or just reading it? Are you modifying all of it?
      \item
            Who does the data matter to, and what about it matters?
      \item
            Find out the quality constraints of your solutions, in terms of bandwidth and latency.
      \item
            What information do you have that isn't in the data per-se? What is implicit?
\end{enumerate}

\newpage
\section{Relational Databases}

In order to lay your data out better, it's useful to have an understanding of the methods available to convert your existing structures into something linear.
The problems we face when applying data-oriented approaches to existing code and data layouts usually stem from the complexity of state inherent in data-hiding or encapsulating programming paradigms.
These paradigms hide away internal state so you don't have to think about it, but they hinder when it comes to reconfiguring data layouts.
This is not because they don't abstract enough to allow changes to the underlying structure without impacting the correctness of the code that uses it, but instead because they have connected and given meaning to the structure of the data.
That type of coupling can be hard to remove.

In this chapter, we go over some of the pertinent parts of the relational model, relational database technology, and normalisation, as these are examples of converting highly complex data structures and relationships into very clean collections of linear storable data entries.

You certainly don't have to move your data to a database style to do data-oriented design, but there are many places where you will wish you had a simple array to work with, and this chapter will help you by giving you an example of how you can migrate from a web of connected complex objects to a simpler to reason about relational model of arrays.

\subsection{Complex State}

When you think about the data present in most software, it has some qualities of complexity or interconnectedness.
When it comes to game development, there are many ways in which the game entities interact, and many ways in which their attached resources will need to feed through different stages of processes to achieve the audio, visual and sometimes haptic feedback necessary to fully immerse the player.
For many programmers brought up on object-oriented design, the idea of reducing the types of structure available down to just simple arrays, is virtually unthinkable.
It's very hard to go from working with objects, classes, templates, and methods on encapsulated data to a world where you only have access to linear containers.

In \textit{A Relational Model of Data for Large Shared Data Banks}, Edgar F. Codd proposed the relational model to handle the current and future needs of agents interacting with data.
He proposed a solution to structuring data for insert, update, delete, and query operations.
His proposal claimed to reduce the need to maintain a deep understanding of how the data was laid out to use it well.
His proposal also claimed to reduce the likelihood of introducing internal inconsistencies.

The relational model provided a framework, and in \textit{Further Normalization of the Data Base Relational Model}, Edgar F. Codd introduced the fundamental terms of normalisation we use to this day in a systematic approach to reducing the most complex of interconnected state information to linear lists of unique independent tuples.

\subsection{What Can Provide A Computational Framework For Complex Data}

Databases store highly complex data in a structured way and provide a language for transforming and generating reports based on that data.
The language, SQL, invented in the 1970's by Donald D. Chamberlin and Raymond F. Boyce at IBM, provides a method by which it is possible to store computable data while also maintaining data relationships following in the form of the relational model.
Games don't have simple computable data, they have classes and objects.
They have guns, swords, cars, gems, daily events, textures, sounds, and achievements.
It is very easy to conclude that database technology doesn't work for the object-oriented approach game developers use.

The data relationships in games can be highly complex, it would seem at first glance that it doesn't neatly fit into database rows.
A CD collection easily fits in a database, with your albums neatly arranged in a single table.
But, many game objects won't fit into rows of columns.
For the uninitiated, it can be hard to find the right table columns to describe a level file.
Trying to find the right columns to describe a car in a racing game can be a puzzle.
Do you need a column for each wheel? Do you need a column for each collision primitive, or just a column for the collision mesh?

An obvious answer could be that game data doesn't fit neatly into the database way of thinking.
However, that's only because we've not normalised the data.
To show how you can convert from a network model, or hierarchical model to what we need, we will work through these normalisation steps.
We'll start with a level file as we find out how these decades-old techniques can provide a very useful insight into what game data is really doing.

We shall discover that everything we do is already in a database, but it wasn't obvious to us because of how we store our data.
The structure of any data is a trade-off between performance, readability, maintenance, future proofing, extendibility, and reuse.
For example, the most flexible database in common use is your filesystem.
It has one table with two columns. A primary key of the file path, and a string for the data.
This simple database system is the perfect fit for a completely future proof system.
There's nothing that can't be stored in a file.
The more complex the tables get, the less future proof, and the less maintainable, but the higher the performance and readability.
For example, a file has no documentation of its own, but the schema of a database could be all that is required to understand a sufficiently well-designed database.
That's how games don't even appear to have databases.
They are so complex, for the sake of performance, they have forgotten they are merely a data transform.
This sliding scale of complexity affects scalability too, which is why some people have moved towards NoSQL databases, and document store types of data storage.
These systems are more like a filesystem where the documents are accessed by name, and have fewer limits on how they are structured.
This has been good for horizontal scalability, as it's simpler to add more hardware when you don't have to keep your data consistent across multiple tables that might be on different machines.
There may come a day when memory is so tightly tied to the closest physical CPU, or when memory chips themselves get more processing power, or running 100 SoCs inside your desktop rig is more effective than a single monolithic CPU, that moving to document store at the high-level could be beneficial inside your app, but for now, there do not seem to be any benefits in that processing model for tasks on local hardware.

We're not going to go into the details of the lowest level of how we utilise large data primitives such as meshes, textures, sounds and such.
For now, think of these raw assets (sounds, textures, vertex buffers, etc.) as primitives, much like the integers, floating point numbers, strings and boolean values we shall be working with.
We do this because the relational model calls for atomicity when working with data.
What is and is not atomic has been debated without an absolute answer becoming clear, but for the intents of developing software intended for human consumption, the granularity can be rooted in considering the data from the perspective of human perception.
There are existing APIs that present strings in various ways depending on how they are used, for example the difference between human-readable strings (usually UTF-8) and ASCII strings for debugging.
Adding sounds, textures, and meshes to this seems quite natural once you realise all these things are resources which if cut into smaller pieces begin to lose what it is that makes them what they are.
For example, half of a sentence is a lot less useful than a whole one, and loses integrity by disassociation.
A slice of a sentence is clearly not reusable in any meaningful way with another random slice of a different sentence.
Even subtitles are split along meaningful boundaries, and it's this idea of meaningful boundary that gives us our definition of atomicity for software developed for humans.
To this end, when working with your data, when you're normalising, try to stay at the level of nouns, the nameable pieces.
A whole song can be an atom, but so is a single tick sound of a clock.
A whole page of text is an atom, but so is the player's gamer-tag.

\subsection{Normalising Your Data}

We're going to work with a level file for a game where you hunt for keys to unlock doors in order to get to the exit room.
The level file is a sequence of script calls which create and configure a collection of different game objects which represent a playable level of the game, and the relationships between those objects.
First, we'll assume it contains rooms (some trapped, some not), with doors leading to other rooms which can be locked.
It will also contain a set of pickups, some let the player unlock doors, some affect the player's stats (like health potions and armour), and all the rooms have lovely textured meshes, as do all the pickups.
One of the rooms is marked as the exit, and one has a player start point.

In this setup script (TODO :: LISTING ) we load some resources, create some pickup prototypes, build up a few rooms, add some instances to the rooms, and then link things together.
Here we also see a standard solution to the problem of things which reference each other.
We create the rooms before we connect them to each other because before they exist we can't.
When we create entities in C++, we assume they are bound to memory, and the only efficient way to reference them is through pointers, but we cannot know where they exist in memory before we allocate them, and we cannot allocate them before filling them out with their data as the allocation and initialisation are bound to each other through the \texttt{new} mechanism.
This means we have difficulty describing relationships between objects before they exist and have to stagger the creation of content into phases of setting up and connecting things together.

To bring this setup script into a usable database-like format, or relational model, we will need to normalise it.
When putting things in a relational model of any sort, it needs to be in tables.
In the first step you take all the data and put it into a very messy, but hopefully complete, table design.
In our case we take the form of the data from the object creation script and fit it into a table.
The asset loading can be directly translated into tables, as can be seen in table (TODO :: TABLE).

Primed with this data, it's now possible for us to create the \texttt{Pickups}.
We convert the calls to \texttt{CreatePickup} into the tables in table (TODO :: TABLE).
Notice that there was a pickup which did not specify a colour tint, and this means we need to use a \texttt{NULL} to represent not giving details about that aspect of the row.
The same applies to animations.
Only keys had animations, so there needs to be \texttt{NULL} entries for all non-key rows.

Once we have taken the construction script and generated these first tables, we find the tables contain a lot of \texttt{NULL}s.
The \texttt{NULL}s in the rows replace the optional content of the objects.
If an object instance doesn't have a certain attribute then we replace those features with \texttt{NULL}s.
There are also elements which contain more than one item of data.
Having multiple doors per room is tricky to handle in this table.
How would you figure out what doors it had?
The same goes for whether the door is locked, and whether there are any pickups.
The first stage in normalising is going to be reducing the number of elements in each cell to 1, and increasing it to 1 where it's currently \texttt{NULL}.

Once we have loaded the assets and have created the pickup prototypes, we move onto creating a table for rooms.
We need to invent attributes as necessary using \texttt{NULL} everywhere that an instance doesn't have that attribute.
We convert the calls to \texttt{CreateRoom}, \texttt{AddDoor}, \texttt{SetRoomAsSpecial}, and \texttt{AddPickup}, to columns in the Rooms table.
See table (TODO :: TABLE) for one way to build up a table that represents all those setup function calls.

\section{Normalisation}

Back when SQL was first created there were only three well-defined stages of data normalisation.
There are many more now, including six numbered normal forms.
To get the most out of a database, it is important to know most of them, or at least get a feel for why they exist.
They teach you about data dependency and can hint at reinterpretations of your data layout.
For game structures, BCNF (Boyce-Codd normal form is explained later) is probably as far as you normally would need to take your methodical process.
Beyond that, you might wish to normalise your data for hot/cold access patterns, but that kind of normalisation is not part of the standard literature on database normalisation.
If you're interested in more than this book covers on the subject, a very good read, and one which introduces the phrase ``The key, the whole key, and nothing but the key." is the article \textit{A Simple Guide to Five Normal Forms in Relational Database Theory} by William Kent.

If a table is in first normal form, then every cell contains one and only one atomic value.
That is, no arrays of values, and no \texttt{NULL} entries.
First normal form also requires every row be distinct.
For those unaware of what a primary key is, we shall discuss that first.

\subsection{Primary Keys}

All tables are made up of rows and columns.
In a database, each row must be unique.
This constraint has important consequences.
When you have normalised your data, it becomes clear why duplicate rows don't make sense, but for now, from a computer programming point of view, consider tables to be more like sets, where the whole row is the set value.
This is very close to reality, as sets are also not ordered, and a database table is not ordered either.
There is always some differentiation between rows, even if a database management system (DBMS) has to rely on hidden row \texttt{ID} values.
It is better to not rely on this as databases work more efficiently when the way in which they are used matches their design.
All tables need a key.
The key is often used to order the sorting of the table in physical media, to help optimise queries.
For this reason, the key needs to be unique, but as small as possible.
You can think of the key as the key in a map or dictionary.
Because of the uniqueness rule, every table has an implicit key because the table can use the combination of all the columns at once to identify each row uniquely.
That is, the key, or the unique lookup, which is the primary key for a table, can be defined as the totality of the whole row.
If the row is unique, then the primary key is unique.
Normally, we try to avoid using the whole row as the primary key, but sometimes, it's actually our only choice.
We will come across examples of that later.

For example, in the mesh table, the combination of \texttt{meshID} and filename is guaranteed to be unique.
However, currently it's only guaranteed to be unique because we have presumed that the \texttt{meshID} is unique.
If it was the same mesh, loaded from the same file, it could still have a different \texttt{meshID}.
The same can be said for the \texttt{textureID} and filename in the textures table.
From the table [ TODO : TABLE IN LATEX? ] it's possible to see how we could use the type, mesh, texture, tint and animation to uniquely define each Pickup prototype.

Now consider rooms.
If you use all the columns other than the \texttt{RoomID} of the room table, you will find the combination can be used to uniquely define the room.
If you consider an alternative, where a row had the same combination of values making up the room, it would in fact be describing the same room.
From this, it can be claimed that the \texttt{RoomID} is being used as an alias for the rest of the data.
We have stuck the \texttt{RoomID} in the table, but where did it come from?
To start with, it came from the setup script.
The script had a \texttt{RoomID}, but we didn't need it at that stage.
We needed it for the destination of the doors.
In another situation, where nothing connected logically to the room, we would not need a \texttt{RoomID} as we would not need an alias to it.

A primary key must be unique.
\texttt{RoomID} is an example of a primary key because it uniquely describes the room.
It is an alias in this sense as it contains no data in and of itself, but merely acts as a handle.
In some cases the primary key is information too, which again, we will meet later.

As a bit of an aside, the idea that a row in a database is also the key can be a core concept worth spending time thinking about.
If a database table is a set, when you insert a record, you're actually just asking that one particular combination of data is being recorded as existing.
It is as if a database table is a very sparse set from an extremely large domain of possible values.
This can be useful because you may notice that under some circumstances, the set of possible values isn't very large, and your table can be more easily defined as a bit set.
As an example, consider a table which lists the players in an MMO that are online right now.
For an MMO that shards its servers, there can be limits in the early thousands for the number of unique players on each server.
In that case, it may be easier to store the currently online players as a bit set.
If there are at most 10,000 players online, and only 1000 players online at any one time, then the bitset representation would take up 1.25kb of memory, whereas storing the online players as a list of \texttt{ID}s, would require at least 2kb of data if their \texttt{ID}s were shrunk to shorts, or 4kb if they had 32bit \texttt{ID}s to keep them unique across multiple servers.
The other benefit in this case is the performance of queries into the data.
To quickly access the \texttt{ID} in the list, you need it to remain sorted.
The best case then is $ \mathcal{O}(\log{}n)$. In the bitset variant, it's $ \mathcal{O}(1)$

Going back to the asset table, an important and useful detail when we talk about the \texttt{meshID} and \texttt{mesh filename} is that even though there could be two different \texttt{meshID}s pointing at the same file, most programmers would intuitively understand that a single \texttt{meshID} was unlikely to point at two different mesh files.
Because of this asymmetry, you can deduce, the column that seems more likely to be unique will also be the column you can use as the primary key.
We'll choose the \texttt{meshID} as it is easier to manipulate and is unlikely to have more than one meaning or usage, but remember, we could have chosen the filename and gone without the \texttt{meshID} altogether.

If we settle on \texttt{TextureID}, \texttt{PickupID}, and \texttt{RoomID} as the primary keys for those tables, we can then look at continuing on to first normal form.
We're using t1, m2, r3, etc. to show typesafe ID values, but in reality, these can all be simple integers.
The idea here is to remain readable, but it also shows that each type can have unique \texttt{ID}s for that type, but have common \texttt{ID}s with another.
For example, a room may have an integer \texttt{ID} value of 0, but so may a texture.
It can be beneficial to have \texttt{ID}s which are unique across types, as that can help debugging, and using the top few bits in that case can be helpful.
If you're unlikely to have more than a million entities per class of entity, then you have enough bits to handle over a thousand distinct classes.

\subsubsection{$1st$ Normal Form}

First normal form can be described as making sure the tables are not sparse.
We require that there be no \texttt{NULL} pointers and that there be no arrays of data in each element of data.
This can be performed as a process of moving the repeats and all the optional content to other tables.
Anywhere there is a \texttt{NULL}, it implies optional content.
Our first fix is going to be the \texttt{Pickups} table, it has optional \texttt{ColourTint} and \texttt{Animation} elements.
We invent a new table \texttt{PickupTint}, and use the primary key of the \texttt{Pickup} as the primary key of the new table.
We also invent a new table \texttt{PickupAnim}.
Table (TODO :: TABLE) shows the result of the transformation, and note we no longer have any \texttt{NULL} entries.

\subsubsection{2nd Normal Form}
\subsubsection{3rd Normal Form}
\subsubsection{Boyce-Cold Normal Form}
\subsubsection{Domain Key/Knowledge}
\subsubsection{Reflections}

What we see here as we normalise our data is a tendency to split data by dependency.
Looking at many third party engines and APIs, you can see some parallels with the results of these normalisations.
It's unlikely that the people involved in the design and evolution of these engines took their data and applied database normalisation techniques, but sometimes the separations between object and components of objects can be obvious enough that you don't need a formal technique in order to realise some positive structural changes.

In some games, the entity object is not just an object that can be anything, but is instead a specific subset of the types of entity involved in the game.
For example, in one game there might be a class for the player character, and one for each major type of enemy character, and another for vehicles.
The player may have different attributes to other entities, such as lacking AI controls, or having player controls, or having regenerating health, or having ammo.
This object-oriented approach puts a line, invisible to the user, but intrusive to the developer, between classes of object and their instances.
It is intrusive because when classes touch, they have to adapt to each other.
When they don't reside in the same hierarchy, they have to work through abstraction layers to message each other.
The amount of code required to bridge these gaps can be small, but they always introduce complexity.

When developing software, this usually manifests as time spent writing out templated code that can operate on multiple classes rather than refactoring the classes involved into more discrete components.
This could be considered wasted time as the likelihood of other operations needing to operate on all the objects is greater than zero, and the effort to refactor into components is usually similar to the effort to create a working templated operation.

Without classes to define boundaries, the table-based approach levels the playing field for data to be manipulated together.
In all cases on our journey through normalising the level data, we have made it so changes to the design require fewer changes to the data, and made it so data changes are less likely to cause the state to become inconsistent.
In many cases, it would seem we have added complexity when it wasn't necessary, and that's up to experimentation and experience to help you decide how far to go.

\subsection{Operations}

When you use objects, you call methods on them, so how do you unlock a door in this table-based approach?
Actions are always going to be insert, delete, or updates.
These were clearly specified in Edgar F. Codd's works, and they are all you need to manipulate a relational model.

In a real database, finding what mesh to load, or whether a door is locked would normally require a join between tables.
A real database would also attempt to optimise the join by changing the sequence of operations until it had made the smallest possible expected workload.
We can do better than that because we can take absolute charge of how we look at and request data from our tables.
To find out if a door is locked, we don't need to join tables, we know we can look up into the locked doors table directly.
Just because the data is laid out like a database, doesn't mean we have to use a query language to access it.

When it comes to operations that change state, it's best to try to stick to the kind of operation you would normally find in a DBMS, as doing unexpected operations brings unexpected state complexity.
For example, imagine you have a table of doors that are open, and a table of doors that are closed.
Moving a door from one table might be considered wasteful, so you may consider changing the representation to a single table, but with all closed doors at one end, and all open at the other.
By having both tables represented as a single table, and having the isClosed attribute defined implicitly by a cut-off point in the array, such as in listing [TODO :: LISTING], leads to the table being somewhat ordered.
This type of memory optimisation comes at a price.
Introducing order into a table makes the whole table inherently less parallelisable to operations, so beware the additional complexity introduced by making changes like this, and document them well.

\begin{lstlisting}{language=c++}
      typedef std::pair<int, int> Door;
      typedef std::vector<Door> DoorVector;
      
      AddClosedDoor(Door d)
      {
          gDoors.push_back();
      }
      AddOpenDoor(Door d)
      {
          gDoors.insert(
                gDoors.begin() + gDoors_firstClosedDoor, d);
          gDoors_firstClosedDoors += 1;
      }
\end{lstlisting}

Unlocking a door can be a delete.
A door is locked because there is an entry in the \texttt{LockedDoors} table that matches the \texttt{Door} you are interested in.
Unlocking a door is a delete if door matches, and you have the right key.

The player inventory would be a table with just \texttt{PickupID}s.
This is the idea that ``the primary key is also the data'' mentioned much earlier.
If the player enters a room and picks up a \texttt{Pickup}, then the entry matching the room is deleted while the inventory is updated to include the new \texttt{PickupID}.

Databases have the concept of triggers, whereupon operations on a table can cause cascades of further operations.
In the case of picking up a key, we would want a trigger on insert into the inventory that joined the new \texttt{PickupID} with the \texttt{LockedDoors} table.
For each matching row there, delete it, and now the door is unlocked.

\subsection{Summing Up}

At this point we can see it is perfectly reasonable to store any highly complex data structures in a database format, even game data with its high interconnectedness and rapid design changing criteria.

Games have lots of state, and the relational model provides a strong structure to hold both static information, and mutable state. The strong structure leads to similar solutions to similar problems in practise, and similar solutions have similar processing. You can expect algorithms and techniques to be more reusable while working with tables, as the data layout is less surprising.

If you're looking for a way to convert your interconnected complicated objects into a simpler flatter memory layout, you could do worse than approach the conversion with normalisation in mind.

A database approach to data storage has some other useful side-effects. It provides an easier route to allowing old executables to run off new data, and it allows new executables to more easily run with old data. This can be vital when working with other people who might need to run an earlier or later version. We saw that sometimes adding new features required nothing more than adding a new table, or a new column to an existing table. That's a non-intrusive modification if you are using a database style of storage, but a significant change if you're adding a new member to a class.

\subsection{Stream Processing}

Now we realise that all the game data and game runtime can be implemented in a database-like approach, we can also see that all game data can be implemented as streams. Our persistent storage is a database, our runtime data is in the same format as it was on disk, what do we benefit from this? Databases can be thought of as collections of rows, or collections of columns, but it's also possible to think about the tables as sets. The set is the set of all possible permutations of the attributes.

For most applications, using a bitset to represent a table would be wasteful, as the set size quickly grows out of scope of any hardware, but it can be interesting to note what this means from a processing point of view. Processing a set, transforming it into another set, can be thought of as traversing the set and producing the output set, but the interesting attribute of a set is that it is unordered. An unordered list can be trivially parallel processed. There are massive benefits to be had by taking advantage of this trivialisation of parallelism wherever possible, and we normally cannot get near this because of the data layout of the object-oriented approaches.

Coming at this from another angle, graphics cards vendors have been pushing in this direction for many years, and we now need to think in this way for game logic too. We can process lots of data quickly as long as we utilise stream processing or set processing as much as possible and use random access processing as little as possible. Stream processing in this case means to process data without writing to variables external to the process. This means not allowing things like global accumulators, or accessing global memory not set as a source for the process. This ensures the processes or transforms are trivially parallelisable.

When you prepare a primitive render for a graphics card, you set up constants such as the transform matrix, the texture binding, any lighting values, or which shader you want to run. When you come to run the shader, each vertex and pixel may have its own scratchpad of local variables, but they never write to globals or refer to a global scratchpad. The concept of shared memory in general purpose GPU code, such as CUDA and OpenCL, allows the use of a kind of managed cache. None of the GPGPU techniques offer access to global memory, and thus maintain a clear separation of domains and continue to guarantee no side-effects caused by any kernels being run outside of their own sandboxed shared memory. By enforcing this lack of side-effects, we can guarantee trivial parallelism because the order of operations are assured to be irrelevant. If a shader was allowed to write to globals, there would be locking, or it would become an inherently serial operation. Neither of these are good for massive core count devices like graphics cards, so that has been a self imposed limit and an important factor in their design. Adding shared memory to the mix starts to inject some potential locking into the process, and hence is explicitly only used when writing compute shaders.

Doing all processing this way, without globals / global scratchpads, gives you the rigidity of intention to highly parallelise your processing and make it easier to think about the system, inspect it, debug it, and extend it or interrupt it to hook in new features. If you know the order doesn't matter, it's very easy to rerun any tests or transforms that have caused bad state.

\subsection{Why Does Database Technology Matter?}

As mentioned at the start of the chapter, the relational model is currently a very good fit for developing non-sparse data layouts that are manipulable with very little complicated state management required once the tables have been designed. However, the only constant is change. That which is current, regularly becomes the old way, and for widely scaled systems, the relational model no longer provides all features required.

After the emergence of NoSQL solutions for handling even larger workloads, and various large companies' work on creating solutions to distribute computing power, there have been advances in techniques to process enormous data-sets. There have been advances in how to keep databases current, distributed, and consistent (within tolerance). Databases now regularly include NULL entries, to the point where there are far more NULL entries than there are values, and these highly sparse databases need a different solution for processing. Many large calculations and processes now run via a technique called map-reduce, and distributing workloads has become commonplace enough that people have to be reminded they don't always need a cluster to add up some numbers.

What's become clear over the last decade is that most of the high-level data processing techniques which are proving to be useful are a combination of hardware-aware data manipulation layers being used by functional programming style high-level algorithms. As the hardware in your PC becomes more and more like the internet itself, these techniques will begin to dominate on personal hardware, whether it be personal computers, phones, or whatever the next generation brings. Data-oriented design was inspired by a realisation that the hardware had moved on to the point where the techniques we used to use to defend against latency from CPU to hard drive, now apply to memory. In the future, if we raise processing power by the utilisation of hoards of isolated unreliable computation units, then the techniques for distributing computing across servers that we're developing in this era, will apply to the desktops of the next.

\section{Existential Processing}

If you saw there weren't any apples in stock, would you still haggle over their price?

Existential processing attempts to provide a way to remove unnecessary querying about whether or not to process your data.
In most software, there are checks for NULL and queries to make sure the objects are in a valid state before work is started.
What if you could always guarantee your pointers were not null?
What if you were able to trust that your objects were in a valid state, and should always be processed?

In this chapter, a dynamic runtime polymorphism technique is shown that can work with the data-oriented design methodology.
It is not the only way to implement data-oriented design friendly runtime polymorphism, but was the first solution discovered by the author, and fits well with other game development technologies, such as components and compute shaders.

\subsection{Complexity}

When studying software engineering you may find references to cyclomatic complexity or conditional complexity.
This is a complexity metric providing a numeric representation of the complexity of programs and is used in analysing large-scale software projects.
Cyclomatic complexity concerns itself only with flow control.
The formula, summarised for our purposes, is one (1) plus the number of conditionals present in the system being analysed.
That means for any system it starts at one, and for each if, while, for, and do-while, we add one.
We also add one per path in a switch statement excluding the default case if present.

Under the hood, if we consider how a virtual call works, that is, a lookup in a function pointer table followed by a branch into the class method, we can see that a virtual call is effectively just as complex as a switch statement.
Counting the flow control statements is more difficult in a virtual call because to know the complexity value, you have to know the number of possible methods that can fulfil the request.
In the case of a virtual call, you have to count the number of overrides to a base virtual call.
If the base is pure-virtual, then you may subtract one from the complexity.
However, if you don't have access to all the code that is running, which can be possible in the case of dynamically loaded libraries, then the number of different potential code paths increases by an unknown amount.
This hidden or obscured complexity is necessary to allow third party libraries to interface with the core process, but requires a level of trust that implies no single part of the process is ever going to be thoroughly tested.

This kind of complexity is commonly called control flow complexity.
There is another form of complexity inherent in software, and that is the complexity of state.
In the paper \textit{Out of the Tar Pit}, it's concluded that the aspect of software which causes the most complexity is state.
The paper continues and presents a solution which attempts to minimise what it calls accidental state, that is, state which is required by the software to do its job, but not directly required by the problem being solved.
The solution also attempts to abolish any state introduced merely to support a programming style.

We use flow control to change state, and state changes what is executed in our programs.
In most cases flow control is put in for one of two reasons :
to solve the problem presented (which is equivalent to the essential state in \textit{Out of the Tar Pit}), and to help with the implementation of the solution (which is equivalent to the accidental state).

Essential control is when we need to implement the design, a gameplay feature which has to happen when some conditions are met, such as jumping when the jump button is pressed or autosaving at a save checkpoint when the savedata is dirty, or a timer has run out.

Accidental control is non-essential to the program from the point of view of the person using it, but could be foundation work, making it critical for successful program creation.
This type of control complexity is itself generally split into two forms.
The first form is structural, such as to support a programming paradigm, to provide performance improvements, or to drive an algorithm.
The second form is defensive programming or developer helpers such as reference counting or garbage collection.
These techniques increase complexity where functions operating on the data aren't sure the data exists, or is making sure bounds are observed.
In practice, you will find this kind of control complexity when using containers and other structures, control flow is going to be in the form of bounds checks and ensuring data has not gone out of scope.
Garbage collection adds complexity.
In many languages, there are few guarantees about how and when it will happen.
This also means it can be hard to reason about object lifetimes.
Because of a tendency to ignore memory allocations early in development when working with these languages, it can be very hard to fix memory leaks closer to shipping dates.
Garbage collection in unmanaged languages is easier to handle, as reference counts can more easily be interrogated, but also due to the fact that unmanaged languages generally allocate less often in the first place.

\subsection{Debugging}

What classes of issues do we suffer with high complexity programs?
Analysing the complexity of a system helps us understand how difficult it is to test, and in turn, how hard it is to debug.
Some issues can be classified as being in an unexpected state, and then having no way forward.
Others can be classified as having bad state, and then exhibiting unexpected behaviour due to reacting to this invalid data.
Yet others can be classified as performance problems, not just correctness, and these issues, though somewhat disregarded by a large amount of academic literature, are costly in practice and usually come from complex dependencies of state.

For example, the complexity caused by performance techniques such as caching, are issues of complexity of state.
The CPU cache is in a state, and not being aware of it, and not working with the expected state in mind, leads to issues of poor or inconsistent performance.

Much of the time, the difficulty we have in debugging comes from not fully observing all the flow control points, assuming one route has been taken when it hasn't.
When programs do what they are told, and not what we mean, they will have entered into a state we had not expected or prepared for.

With runtime polymorphism using virtual calls, the likelihood of that happening can dramatically increase as we cannot be sure we know all the different ways the code can branch until we either litter the code with logging, or step through in a debugger to see where it goes at run-time.

\subsection{Why Use An If}

In real-world cases of game development, the most common use of an explicit flow control statement would appear to be in the non-essential set.
Where defensive programming is being practiced, many of the flow control statements are just to stop crashes.
There are fail-safes for out of bounds accesses, protection from pointers being NULL, and defenses against other exceptional cases that would bring the program to a halt.
It's pleasing to note, GitHub contains plenty of high quality C++ source-code that bucks this trend, preferring to work with reference types, or with value types where possible.
In game development, another common form of flow control is looping.
Though these are numerous, most compilers can spot them, and have good optimisations for these and do a very good job of removing condition checks that aren't necessary.
The final inessential but common flow control comes from polymorphic calls, which can be helpful in implementing some of the gameplay logic, but mostly are there to entertain the do-more-with-less-code development model partially enforced in the object-oriented approach to writing games.

Essential game design originating flow control doesn't appear very often in profiles as causes of branching, as all the supporting code is run far more frequently.
This can lead to an underappreciation of the effect each conditional has on the performance of the software.
Code that does use a conditional to implement AI or handle character movement, or decide on when to load a level, will be calling down into systems which are full of loops and tree traversals, or bounds checks on arrays they are accessing in order to return the data upon which the game is going to produce the boolean value to finally drive the side of the if to which it will fall through.
That is, when the rest of your code-base is slow, it's hard to validate writing fast code for any one task.
It's hard to tell what additional costs you're adding on.

If we decide the elimination of control flow is a goal worthy of consideration, then we must begin to understand what control flow operations we can eliminate.
If we begin our attempt to eliminate control flow by looking at defensive programming, we can try to keep our working set of data as a collections of arrays.
This way we can guarantee none of our data will be NULL.
That one step alone may eliminate many of our flow control statements.
It won't get rid of loops, but as long as they are loops over data running a pure functional style transform, then there are no side-effects to worry about, and it will be easier to reason about.

The inherent flow control in a virtual call is avoidable, as it is a fact that many programs were written in a non-object-oriented style.
Without virtuals, we can rely on switch statements.
Without those, we can rely on function pointer tables.
Without those, we can have a long sequence of ifs.
There are many ways to implement runtime polymorphism.
It is also possible to maintain that if you don't have an explicit type, you don't need to switch on it, so if you can eradicate the object-oriented approach to solving the problem, those flow control statements go away completely.

When we get to the control flow in gameplay logic, we find there is no simple way to eradicate it.
This is not a terrible thing to worry about, as the gameplay logic is as close to essential complexity as we can get when it comes to game development.

Reducing the number of conditionals, and thus reducing the cyclomatic complexity on such a scale is a benefit which cannot be overlooked, but it is one that comes with a cost.
The reason we are able to get rid of the check for NULL is that we will have our data in a format that doesn't allow for NULL at all.
This inflexibility will prove to be a benefit, but it requires a new way of processing our entities.

Where once we would have an object instance for an area in a game, and we would interrogate it for exits that take us to other areas, now we look into a structure that only contains links between areas, and filter by the area we are in.
This reversal of ownership can be a massive benefit in debugging, but can sometimes appear backward when all you want to do is find out what exits are available to get out of an area.

If you've ever worked with shopping lists or to-do lists, you'll know how much more efficient you can be when you have a definite list of things to purchase or complete.
It's very easy to make a list, and adding to it is easy as well.
If you're going shopping, it's very hard to think what might be missing from your house in order to get what you need.
If you're the type that tries to plan meals, then a list is nigh on essential as you figure out ingredients and then tally up the number of tins of tomatoes, or other ingredients you need to last through all the meals you have planned.
If you have a to-do list and a calendar, you know who is coming and what needs to be done to prepare for them.
You know how many extra mouths need feeding, how much food and drink you need to buy, and how much laundry you need done to make enough beds for the visitors.

To-do lists are great because you can set an end goal and then add in subtasks that make a large and long distant goal seem more doable.
Adding in estimates can provide a little urgency that is usually missing when the deadline is so far away.
Many companies use software to support tracking of tasks, and this software often comes with features allowing the producers to determine critical paths, expected developer hours required, and sometimes even the balance of skills required to complete a project.
Not using this kind of software is often a sign that a company isn't overly concerned with efficiency, or waste.
If you're concerned about efficiency and waste in your program, lists of tasks seem like a good way to start analysing where the costs are coming from.
If you keep track of these lists by logging them, you can look at the data and see the general shape of the processing your software is performing.
Without this, it can be difficult to tell where the real bottlenecks are, as it might not be the processing that is the problem, but the requirement to process data itself which has gotten out of hand.

When your program is running, if you don't give it homogeneous lists to work with, but instead let it do whatever comes up next, it will be inefficient and have irregular or lumpy frame timings.
Inefficiency of hardware utilisation often comes from unpredictable processing.
In the case of large arrays of pointers to heterogeneous classes all being called with an update() function, you can hit high amounts of data dependency which leads to misses in both data and instruction caches.
See chapter [TODO :: LINK] for more details on why.

Slowness also comes from not being able to see how much work needs to be done, and therefore not being able to prioritise or scale the work to fit what is possible within the given time-frame.
Without a to-do list, and an ability to estimate the amount of time each task will take, it is difficult to decide the best course of action to take in order to reduce overhead while maintaining feedback to the user.

Object-oriented programming works very well when there are few patterns in the way the program runs.
When either the program is working with only a small amount of data, or when the data is incredibly heterogeneous, to the point that there are as many classes of things as there are things.

Irregular frame timings can often be blamed on not being able to act on distant goals ahead of time.
If you, as a developer, know you have to load the assets for a new island when a player ventures into the seas around it, the streaming system can be told to drag in any data necessary.
This could also be for a room and the rooms beyond.
It could be for a cave or dungeon when the player is within sight of the entrance.
We consider this kind of preemptive streaming of data to be a special case and invent systems to provide this level of forethought.
Relying on humans, or even level-designers, to link these together is prone to error.
In many cases, there are chains of dependencies that can be missed without an automated check.
The reason we cannot make systems self-aware enough to preload themselves is that we don't have a common language to describe temporal dependencies.

In many games, we stream things in with explicit triggers, but there is often no such system for many of the other game elements.
It's virtually unheard of for an AI to pathfind to some goal because there might soon be a need to head that way.
The closest would be for the developer to pre-populate a navigation map so coarse grain pathing can be completed swiftly.

There's also the problem of depth of preemptive work.
Consider the problem of a small room, built as a separate asset, a waiting room with two doors near each other, both leading to large, but different maps.
When the player gets near the door to the waiting room in map A, that little room can be preemptively streamed in.
However, in many engines, map B won't be streamed in, as the locality of map B to map A is hidden behind the logical layer of the waiting room.

It's also not commonplace to find a physics system doing look ahead to see if a collision has happened in the future in order to start doing further work.
It might be possible to do a more complex breakup simulation if it were more aware.

If you let your game generate to-do lists, shopping lists, distant goals, and allow for preventative measures by forward-thinking, then you can simplify your task as a coder into prioritising goals and effects, or writing code that generates priorities at runtime.
You can start to think about how to chain those dependencies to solve the waiting room problem.
You can begin to preempt all types of processing.

\subsection{Types of Programming}

Existential processing is related to to-do lists.
When you process every element in a homogeneous set of data, you know you are processing every element the same way.
You are running the same instructions for every element in that set.
There is no definite requirement for the output in this specification, however, it usually comes down to one of three types of operation :
a filter, a mutation, or an emission.
A mutation is a one to one manipulation of the data, it takes incoming data and some constants that are set up before the transform, and produces one and only one element for each input element.
A filter takes incoming data, again with some constants set up before the transform, and produces one element or zero elements for each input element.
An emission is a manipulation of the incoming data that can produce multiple output elements.
Just like the other two transforms, an emission can use constants, but there is no guaranteed size of the output table;
it can produce anywhere between zero and infinity elements.

A fourth, and final form, is not really a manipulation of data, but is often part of a transform pipeline, and that is the generator.
A generator takes no input data, but merely produces output based on the constants set up.
When working with compute shaders, you might come across this as a function that merely clears out an array to zero, one, or an ascending sequence.

[ TODO :: TABLE ]

These categories can help you decide what data structure you will use to store the elements in your arrays, and whether you even need a structure, or you should instead pipe data from one stage to another without it touching down on an intermediate buffer.

Every CPU can efficiently handle running processing kernels over homogeneous sets of data, that is, doing the same operation over and over again over contiguous data.
When there is no global state, no accumulator, it is proven to be parallelisable.
Examples can be given from existing technologies such as map-reduce and simple compute shaders, as to how to go about building real work applications within these restrictions.
Stateless transforms also commit no crimes that prevent them from being used within distributed processing technologies.
Erlang relies on these guarantees of being side-effect free to enable not just thread safe processing or interprocess safe processing, but distributed computing safe processing.
Stateless transforms of stateful data are highly robust and deeply parallelisable.

Within the processing of each element, that is for each datum operated on by the transform kernel, it is fair to use control flow.
Almost all compilers should be able to reduce simple local value branch instructions into a platform's preferred branch-free representation, such as a CMOV, or select function for a SIMD operation.
When considering branches inside transforms, it's best to compare to existing implementations of stream processing such as graphics card shaders or compute kernels.

In predication, flow control statements are not ignored, but they are used instead as an indicator of how to merge two results.
When the flow control is not based on a constant, a predicated if will generate code that will run both sides of the branch at the same time and discard one result based on the value of the condition.
It manages this by selecting one of the results based on the condition.
As mentioned before, in many CPUs there is an intrinsic for this, but all CPUs can use bit masking to effect this trick.

SIMD or single-instruction-multiple-data allows the parallel processing of data when the instructions are the same.
The data is different but local.
When there are no conditionals, SIMD operations are simple to implement on your transforms.
In MIMD, that is multiple instructions, multiple data, every piece of data can be operated on by a different set of instructions.
Each piece of data can take a different path.
This is the simplest and most error-prone to code for because it's how most parallel programming is currently done.
We add a thread and process some more data with a separate thread of execution.
MIMD includes multi-core general purpose CPUs.
It often allows shared memory access and all the synchronisation issues that come with it.
It is by far the easiest to get up and running, but it is also the most prone to the kind of rare fatal error caused by complexity of state.
Because the order of operations become non-deterministic, the number of different possible routes taken through the code explode super-exponentially.

\subsection{Don't Use Booleans}

When you study compression technology, one of the most important aspects you have to understand is the difference between data and information.
There are many ways to store information in systems, from literal strings that can be parsed to declare something exists, right down to something simple like a single bit flag to show that a thing might have an attribute.
Examples include the text that declares the existence of a local variable in a scripting language, or the bit field containing all the different collision types a physics mesh will respond to.
Sometimes we can store even less information than a bit by using advanced algorithms such as arithmetic encoding, or by utilising domain knowledge.
Domain knowledge normalisation applies in most game development, but it is increasingly infrequently applied, as many developers are falling foul to overzealous application of quoting premature optimisation.
As information is encoded in data, and the amount of information encoded can be amplified by domain knowledge, it's important that we begin to see that the advice offered by compression techniques is :
\textbf{what we are really encoding is probabilities}.

If we take an example, a game where the entities have health, regenerate after a while of not taking damage, can die, can shoot each other, then let's see what domain knowledge can do to reduce processing.

We assume the following domain knowledge:

\begin{itemize}
      \item
            If you have full health, then you don't need to regenerate.
      \item
            Once you have been shot, it takes some time until you begin regenerating.
      \item
            Once you are dead, you cannot regenerate.
      \item
            Once you are dead you have zero health.
\end{itemize}

[TODO :: LISTINGS]

If we have a list for the entities such as in listing [TODO :: REFERENCE LISTINGS ABOVE], then we see the normal problem of data potentially causing cache line utilisation issues.
Aside from that, we can see how you might run an update function over the list, such as in listing [TODO :: REFERENCE LISTINGS ABOVE], which will run for every entity in the game, every update.

We can make this better by looking at the flow control statement.
The function won't run if health is at max.
It won't run if the entity is dead.
The regenerate function only needs to run if it has been long enough since the last damage dealt.
All these things considered, regeneration isn't the common case.
We should try to organise the data layout for the common case.

[TODO :: LISTINGS ABOUT THE NEW CODE WITH HASHES OF DAMAGED UNITS]

Let's change the structures to those in listing [TODO :: REFERENCE LISTINGS ABOVE] and then we can run the update function over the health table rather than the entities.
This means we already know, as soon as we are in this function, that the entity is not dead, and they are hurt.

[TODO :: LISTINGS ABOUT THE UPDATE LOOP OVER DAMAGED UNITS ONLY]

We only add a new entityhealth element when an entity takes damage.
If an entity takes damage when it already has an entityhealth element, then it can update the health rather than create a new row, also updating the time damage was last dealt.
If you want to find out someone's health, then you only need to look and see if they have an entityhealth row, or if they have a row in deadEntities table.
The reason this works is, an entity has an implicit boolean hidden in the row existing in the table.
For the entityDamages table, that implicit boolean is the isHurt variable from the first function.
For the deadEntities table, the boolean of isDead is now implicit, and also implies a health value of 0, which can reduce processing for many other systems.
If you don't have to load a float and check it is less than 0, then you're saving a floating point comparison or conversion to boolean.

This eradication of booleans is nothing new, because every time you have a pointer to something you introduce a boolean of having a non-NULL value.
It's the fact that we don't want to check for NULL which pushes us towards finding a different representation for the lack of existence of an object to process.

Other similar cases include weapon reloading, oxygen levels when swimming, anything which has a value that runs out, has a maximum, or has a minimum.
Even things like driving speeds of cars.
If there are traffic, then they will spend most of their time driving at traffic speed not some speed they need to calculate.
If you have a group of people all heading in the same direction, then someone joining the group can be intercepting until they manage to, at which point they can give up their independence, and become controlled by the group.
There is more on this point in chapter [TODO :: REFERENCE CHAPTER ??].

By moving to keeping lists of attribute state, you can introduce even more performance improvements.
The first thing you can do for attributes that are linked to time is to put them in a sorted list, sorted by time of when they should be acted upon.
You could put the regeneration times in a sorted list and pop entityDamage elements until you reach one that can't be moved to the active list, then run through all the active list in one go, knowing they have some damage, aren't dead, and can regen as it's been long enough.

Another aspect is updating certain attributes at different time intervals.
Animals and plants react to their environment through different mechanisms.
There are the very fast mechanisms such as reactions to protect us from danger.
Pulling your hand away from hot things, for example.
There are the slower systems too, like the rationalising parts of the brain.
Some, apparently quick enough that we think of them as real-time, are the quick thinking and acting processes we consider to be the actions taken by our brains when we don't have time to think about things in detail, such as catching a ball or balancing a bicycle.
There is an even slower part of the brain, the part that isn't so much reading this book, but is consuming the words, and making a model of what they mean so as to digest them.
There is also the even slower systems, the ones which react to stress, chemical levels spread through the body as hormones, or just the amount of sugar you have available, or current level of hydration.
An AI which can think and react on multiple time-scales is more likely to waste fewer resources, but also much less likely to act oddly, or flip-flop between their decisions.
Committing to doing an update of every system every frame could land you in an impossible situation.
Splitting the workload into different update rates can still be regular, but offers a chance to balance the work over multiple frames.

Another use is in state management.
If an AI hears gunfire, then they can add a row to a table for when they last heard gunfire, and that can be used to determine whether they are in a heightened state of awareness.
If an AI has been involved in a transaction with the player, it is important they remember what has happened as long as the player is likely to remember it.
If the player has just sold an AI their +5 longsword, it's very important the shopkeeper AI still have it in stock if the player just pops out of the shop for a moment.
Some games don't even keep inventory between transactions, and that can become a sore point if they accidentally sell something they need and then save their progress.

From a gameplay point of view, these extra bits of information are all about how the world and player interact.
In some games, you can leave your stuff lying around forever, and it will always remain just how you left it.
It's quite a feat that all the things you have dumped in the caves of some open-world role-playing games, are still hanging around precisely where you left them hours and hours ago.

The general concept of tacking on data, or patching loaded data with dynamic additional attributes, has been around for quite a while.
Save games often encode the state of a dynamic world as a delta from the base state, and one of the first major uses was in fully dynamic environments, where a world is loaded, but can be destroyed or altered later.
Some world generators took a procedural landscape and allowed their content creators to add patches of extra information, villages, forts, outposts, or even break out landscaping tools to drastically adjust the generated data.

\subsection{Don't Use Enums Quite As Much}

Enumerations are used to define sets of states.
We could have had a state variable for the regenerating entity, one that had infullhealth, ishurt, isdead as its three states.
We could have had a team index variable for the avoidance entity enumerating all the available teams.
Instead, we used tables to provide all the information we needed, as there were only two teams.
Any enum can be emulated with a variety of tables.
All you need is one table per enumerable value.
Setting the enumeration is an insert into a table or a migration from one table to another.

When using tables to replace enums, some things become more difficult :
finding out the value of an enum in an entity is difficult as it requires checking all the tables which represent that state for the entity.
However, the main reason for getting the value is either to do an operation based on an external state or to find out if an entity is in the right state to be considered for an operation.
This is disallowed and unnecessary for the most part, as firstly, accessing external state is not valid in a pure function, and secondly, any dependent data should already be part of the table element.

If the enum is a state or type enum previously handled by a switch or virtual call, then we don't need to look up the value, instead, we change the way we think about the problem.
The solution is to run transforms taking the content of each of the switch cases or virtual methods as the operation to apply to the appropriate table, the table corresponding to the original enumeration value.

If the enum is instead used to determine whether or not an entity can be operated upon, such as for reasons of compatibility, then consider an auxiliary table to represent being in a compatible state.
If you're thinking about the case where you have an entity as the result of a query and need to know if it is in a certain state before deciding to commit some changes, consider that the compatibility you seek could have been part of the criteria for generating the output table in the first place, or a second filtering operation could be committed to create a table in the right form.

In conclusion, the reason why you would put an enum in table form, is to reduce control flow impact.
Given this, it's when we aren't using the enumerations to control instruction flow that it's fine to leave them alone.
Another possibility is when the value of the enum changes with great frequency, as moving objects from table to table has a cost too.

Examples of enumerations that make sense are keybindings, enumerations of colours, or good names for small finite sets of values.
Functions that return enums, such as collision responses (none, penetrating, through).
Any kind of enumeration which is actually a lookup into data of another form is good, where the enum is being used to rationalise the access to those larger or harder to remember data tables.
There is also a benefit to some enums in that they will help you trap unhandled cases in switches, and to some extent, they are a self-documenting feature in most languages.

\subsection{Prelude To Polymorphism}

Let's consider now how we implement polymorphism.
We know we don't have to use a virtual table pointer;
we could use an enum as a type variable.
That variable, the member of the structure that defines at runtime what that structure should be capable of and how it is meant to react.
That variable will be used to direct the choice of functions called when methods are called on the object.

When your type is defined by a member type variable, it's usual to implement virtual functions as switches based on that type, or as an array of functions.
If we want to allow for runtime loaded libraries, then we would need a system to update which functions are called.
The humble switch is unable to accommodate this, but the array of functions could be modified at runtime.

We have a solution, but it's not elegant, or efficient.
The data is still in charge of the instructions, and we suffer the same instruction cache misses and branch mispredictions as whenever a virtual function is unexpected.
However, when we don't really use enums, but instead tables that represent each possible value of an enum, it is still possible to keep compatible with dynamic library loading the same as with pointer based polymorphism, but we also gain the efficiency of a data-flow processing approach to processing heterogeneous types.

For each class, instead of a class declaration, we have a factory that produces the correct selection of table insert calls.
Instead of a polymorphic method call, we utilise existential processing.
Our elements in tables allow the characteristics of the class to be implicit.
Creating your classes with factories can easily be extended by runtime loaded libraries.
Registering a new factory should be simple as long as there is a data-driven factory method.
The processing of the tables and their update() functions would also be added to the main loop.

\subsection{Dynamic Runtime Polymorphism}

If you create your classes by composition, and you allow the state to change by inserting and removing from tables, then you also allow yourself access to dynamic runtime polymorphism.
This is a feature normally only available when dynamically responding via a switch.

Polymorphism is the ability for an instance in a program to react to a common entry point in different ways due only to the nature of the instance.
In C++, compile-time polymorphism can be implemented through templates and overloading.
Runtime polymorphism is the ability for a class to provide a different implementation for a common base operation with the class type unknown at compile-time.
C++ handles this through virtual tables, calling the right function at runtime based on the type hidden in the virtual table pointer at the start of the memory pointed to by the this pointer.
Dynamic runtime polymorphism is when a class can react to a common call signature in different ways based on its type, but its type can change at runtime.
C++ doesn't implement this explicitly, but if a class allows the use of an internal state variable or variables, it can provide differing reactions based on the state as well as the core language runtime virtual table lookup.
Other languages which define their classes more fluidly, such as Python, allow each instance to update how it responds to messages, but most of these languages have very poor general performance as the dispatch mechanism has been built on top of dynamic lookup.

[TODO :: LISTINGS OF POLYMORPHISM USING C++ VIRTUAL TABLES]

Consider the code in listing [TODO :: REFER LISTINGS ABOVE], where we expect the runtime method lookup to solve the problem of not knowing the type but wanting the size.
Allowing the objects to change shape during their lifetime requires some compromise.
One way is to keep a type variable inside the class such as in listing [TODO :: REFER LISTINGS BELOW], where the object acts as a container for the type variable, rather than as an instance of a specific shape.

[TODO :: LISTINGS OF POLYMORPHISM IMPLEMENTED USING ENUMS]

A better way is to have a conversion function to handle each case.
In listing [TODO :: REFER LISTINGS BELOW] we see how that can be achieved.

[TODO :: LISTINGS OF POLYMORPHISM USING CONVERSION]

Though this works, all the pointers to the old class are now invalid.
Using handles would mitigate these worries, but add another layer of indirection in most cases, dragging down performance even further.

If you use existential processing techniques, your classes are defined by the tables they belong to.
Then you can switch between tables at runtime.
This allows you to change behaviour without any tricks, without the complexity of managing a union to carry all the different data around for all the states you need.
If you compose your class from different attributes and abilities then need to change them post creation, you can.
If you're updating tables, the fact that the pointer address of an entity has changed will mean little to you.
It's normal for an entity to move around memory in table-based processing, so there are fewer surprises.
Looking at it from a hardware point of view, in order to implement this form of polymorphism you need a little extra space for the reference to the entity in each of the class attributes or abilities, but you don't need a virtual table pointer to find which function to call.
You can run through all entities of the same type increasing cache effectiveness, even though it provides a safe way to change type at runtime.

Via the nature of having classes defined implicitly by the tables they belong to, there is an opportunity to register a single entity with more than one table.
This means that not only can a class be dynamically runtime polymorphic, but it can also be multi-faceted in the sense that it can be more than one class at a time.
A single entity might react in two different ways to the same trigger call because it might be appropriate for the current state of that class.

This kind of multidimensional classing doesn't come up much in traditional gameplay code, but in rendering, there are usually a few different axes of variation such as the material, what blend mode, what kind of skinning or other vertex adjustments are going to take place on a given instance.
Maybe we don't see this flexibility in gameplay code because it's not available through the natural tools of the language.
It could be that we do see it, but it's what some people call entity component systems.

\subsection{Event Handling}

When you wanted to listen for events in a system in the old days, you'd attach yourself to an interrupt.
Sometimes you might get to poke at code that still does this, but it's normally reserved for old or microcontroller scale hardware.
The idea was simple, the processor wasn't really fast enough to poll all the possible sources of information and do something about the data, but it was fast enough to be told about events and process the information as and when it arrived.
Event handling in games has often been like this, register yourself as interested in an event, then get told about it when it happens.
The publish and subscribe model has been around for decades, but there's no standard interface built for it in some languages and too many standards in others.
As it often requires some knowledge from the problem domain to choose the most effective implementation.

Some systems want to be told about every event in the system and decide for themselves, such as Windows event handling.
Some systems subscribe to very particular events but want to react to them as soon as they happen, such as handlers for the BIOS events like the keyboard interrupt.
The events could be very important and dispatched directly by the action of posting the event, such as with callbacks.
The events could be lazy, stuck in a queue somewhere waiting to be dispatched at some later point.
The problem they are trying to solve will define the best approach.

Using your existence in a table as the registration technique makes this simpler than before and lets you register and de-register with great pace.
Subscription becomes an insert, and unsubscribing a delete.
It's possible to have global tables for subscribing to global events.
It would also be possible to have named tables.
Named tables would allow a subscriber to subscribe to events before the publisher exists.

When it comes to firing off events, you have a choice.
You can choose to fire off the transform immediately, or queue up new events until the whole transform is complete, then dispatch them all in one go.
As the model becomes simpler and more usable, the opportunity for more common use leads us to new ways of implementing code traditionally done via polling.

For example :
unless a player character is within the distance to activate a door, the event handler for the player's action button needn't be attached to anything door related.
When the character comes within range, the character registers into the \texttt{has\_pressed\_action} event table with the \texttt{open\_door\_(X)} event result.
This reduces the amount of time the CPU wastes figuring out what thing the player was trying to activate, and also helps provide state information such as on-screen displays saying pressing Green will Open the door.

If we allow for all tables to have triggers like those found in DBMSs, then it may be possible to register interest in changes to input mappings, and react.
Hooking into low-level tables such as an insert into a \texttt{has\_pressed\_action} table would allow user interfaces to know to change their on-screen display to show the new prompt.

This coding style is somewhat reminiscent of aspect-oriented programming where it is easy to allow for cross-cutting concerns in the code.
In aspect-oriented programming, the core code for any activities is kept clean, and any side effects or vetoes of actions are handled by other concerns hooking into the activity from outside.
This keeps the core code clean at the expense of not knowing what is really going to be called when you write a line of code.
How using registration tables differs is in where the reactions come from and how they are determined.
Debugging can become significantly simpler as the barriers between cause and effect normally implicit in aspect-oriented programming are significantly diminished or removed, and the hard to adjust nature of object-oriented decision making can be softened to allow your code to become more dynamic without the normally associated cost of data-driven control flow.

\section{Component Based Objects}

A component-oriented design is a good start for high-level data-oriented design.
Developing with components can put you in the right frame of mind to avoid linking together concepts needlessly.
Objects built this way can more easily be processed by type, instead of by instance, which can lead to them being easier to profile.
Entity systems built around them are often found in game development as a way to provide data-driven functionality packs for entities, allowing for designer control over what would normally be in the realm of a programmer.
Not only are component based entities better for rapid design changes, but they also stymie the chances of getting bogged down into monolithic objects, as most game designers would demand more components with new features over extending the scope of existing components.
This is because most new designs need iterating on, and extending an existing component by code to introduce design changes wouldn't allow game designers to switch back and forth trying out different things as easily.
It's usually more flexible to add another component as an extension or as an alternative.

A problem that comes up with talking about component-oriented development is how many different types of entity component systems there are.
To help clear the ambiguity, we shall describe some different ways in which component-oriented designs work.

The first kind of component-oriented approach most people use is a compound object.
There are a few engines that use them this way, and most of them use the power of their scripting language to help them achieve a flexible, and designer friendly way to edit and create objects out of components.
For example, Unity's GameObject is a base entity type which can include components by adding them to that particular instance's list of components.
They are all built onto the core entity object, and they refer to each other through it.
This approach means every entity still tends to update via iteration over root instances, not iteration over systems.

Common dialogue around creating compound objects frequently refers to using components to make up an object directly by including them as members of the object.
Though this is better than a monolithic class, it is not yet a fully component based approach.
This technique uses components to make the object more readable, and potentially more reusable and robust to change.
These systems are extensible enough to support large ecosystems of components shareable between projects.
The Unity Asset Store proves the worth of components from the point of view of rapid development.

When you introduce component based entities, you have an opportunity to turn the idea of how you define an object on its head.
The normal approach to defining an object in object-oriented design is to name it, then fill out the details as and when they become necessary.
For example, your car object is defined as a Car, if not extending Vehicle, then at least including some data about what physics and meshes are needed, with construction arguments for wheels and body shell model assets etc, possibly changing class dependent on whether it's an AI or player car.
In component-oriented design, objects aren't so rigidly defined, and don't so much become defined after they are named, as much as a definition is selected or compiled, and then tagged with a name if necessary.
For example, instancing a physics component with four-wheel physics, instancing a renderable for each part (wheels, shell, suspension) adding an AI or player component to control the inputs for the physics component, all adds up to something which we can tag as a Car, or leave as is and it becomes something implicit rather than explicit and immutable.

A truly component based object is nothing more than the sum of its parts.
This means the definition of a component based object is also nothing more than an inventory with some construction arguments.
This object or definition agnostic approach makes refactoring and redesigning a much simpler exercise.
Unity's ECS provides such a solution.
In the ECS, entities are intangible and implicit, and the components are first class citizens.

\subsection{Components In The Wild}

Component based approaches to development have been tried and tested.
Many high-profile studios have used component driven entity systems to great success, and this was in part due to their developer's unspoken understanding that objects aren't a good place to store all your data and traits.
For some, it was the opportunity to present the complexity of what makes up an entity through simpler pieces, so designers and modders would be able to reason about how their changes fit within the game framework.
For some, it was about giving power over to performance, where components are more easily moved to a structure-of-arrays approach to processing.

Gas Powered Games' Dungeon Siege Architecture is probably the earliest published document about a game company using a component based approach.
If you get a chance, you should read the article [TODO :: LINK TO ARTICLE] to see where things really kicked off.
The article explains that using components means the entity type doesn't need to have the ability to do anything.
Instead, all the attributes and functionality come from the components of which the entity is made up of.

The list of reasons to move to a manager driven, component based approach are numerous, and we shall attempt to cover at least a few.
We will talk about the benefits of clear update sequences.
We will mention how components can make it easier to debug.
We will talk about the problem of objects applying meaning to data, causing coupling, and therefore with the dissolution of the object as the central entity, how the tyranny of the instance is mitigated.

In this section, we'll show how we can take an existing class and rewrite it in a component based fashion.
We're going to tackle a fairly typical complex object, the Player class.
Normally these classes get messy and out of hand quite quickly.
We're going to assume it's a Player class designed for a generic 3rd person action game, and take a typically messy class as our starting point.
We shall use listing [TODO :: REFER TO LISTINGS SHOWING BASE OOP CLASS TO BE DISSECTED] as a reference example of one such class.

[TODO :: LISTINGS OF AN OOP CLASS TO BE DISSECTED]

This example class includes many of the types of things found in games, where the codebase has grown organically.
It's common for the Player class to have lots of helper functions to make writing game code easier.
Helper functions typically consider the Player as an instance in itself, from data in save through to rendering on screen.
It's not unusual for the Player class to touch nearly every aspect of a game, as the human player is the target of the code in the first place, the Player class is going to reference nearly everything too.

AI characters will have similarly gnarly looking classes if they are generalised rather than specialised.
Specialising AI was more commonplace when games needed to fit in smaller machines, but now, because the Player class has to interact with many of them over the course of the game, they tend to be unified into one type just like the player, if not the same as the player, to help simplify the code that allows them to interact.
As of writing, the way in which AI is differentiated is mostly by data, with behaviour trees taking the main stage for driving how AI thinks about its world.
Behaviour trees are another concept subject to various interpretations, so some forms are data-oriented design friendly, and others are not.

\subsection{Away From Hierarchies}

A recurring theme in articles and post-mortems from people moving from object-oriented hierarchies of gameplay classes to a component based approach is the transitional states of turning their classes into containers of smaller objects, an approach often called composition.
This transitional form takes an existing class and finds the boundaries between concepts internal to the class and attempts to refactor them out into new classes which can be owned or pointed to by the original class.
From our monolithic player class, we can see there are lots of things that are not directly related, but that does not mean they are not linked together.

Object-oriented hierarchies are is-a relationships, and components and composition oriented designs are traditionally thought of as has-a relationships.
Moving from one to the other can be thought of as delegating responsibility or moving away from being locked into what you are, but having a looser role and keeping the specialisation until further down the tree.
Composition clears up most of the common cases of diamond inheritance issues, as capabilities of the classes are added by accretion as much as they are added by overriding.

The first move we need to make will be to take related pieces of our monolithic class and move them into their own classes, along the lines of composing, changing the class from owning all the data and the actions that modify the data into having instances which contain data and delegating actions down into those specialised structures where possible.
We move the data out into separate structures so they can be more easily combined into new classes later.
We will initially only separate by categories we perceive as being the boundaries between systems.
For example, we separate rendering from controller input, from gameplay details such as inventory, and we split out animation from all.

Taking a look at the results of splitting the player class up, such as in listing [TODO :: REFER LISTINGS SHOWING THE MODIFIED OOP CLASS], it's possible to make some initial assessments of how this may turn out.
We can see how a first pass of building a class out of smaller classes can help organise the data into distinct, purpose oriented collections, but we can also see the reason why a class ends up being a tangled mess.
When you think about the needs of each of the pieces, what their data requirements are, the coupling can become evident.
The rendering functions need access to the player's position as well as the model, and the gameplay functions such as Shoot(Vec target) need access to the inventory as well as setting animations and dealing damage.
Taking damage will need access to the animations and health.
Things are already seeming more difficult to handle than expected, but what's really happening here is that it's becoming clear that code needs to cut across different pieces of data.
With just this first pass, we can start to see that functionality and data don't belong together.

[TODO :: LISTINGS SHOWING THE MODIFIED OOP CLASS]

In this first step, we made the player class a container for the components.
Currently, the player has the components, and the player class has to be instantiated to make a player exist.
To allow for the cleanest separation into components in the most reusable way, it's worth attempting to move components into being managed by managers, and not handled or updated by their entities.
In doing this, there will also be a benefit of cache locality when we're iterating over multiple entities doing related tasks when we move them away from their owners.

This is where it gets a bit philosophical.
Each system has an idea of the data it needs in order to function, and even though they will overlap, they will not share all data.
Consider what it is that a serialisation system needs to know about a character.
It is unlikely to care about the current state of the animation system, but it will care about inventory.
The rendering system will care about position and animation, but won't care about the current amount of ammo.
The UI rendering code won't even care about where the player is, but will care about inventory and their health and damage.
This difference of interest is at the heart of why putting all the data in one class isn't a good long-term solution.

The functionality of a class, or an object, comes from how the internal state is interpreted, and how the changes to state over time are interpreted too.
The relationship between facts is part of the problem domain and could be called meaning, but the facts are only raw data.
This separation of fact from meaning is not possible with an object-oriented approach, which is why every time a fact acquires a new meaning, the meaning has to be implemented as part of the class containing the fact.
Dissolving the class, extracting the facts and keeping them as separate components, has given us the chance to move away from classes that instill permanent meaning at the expense of occasionally having to look up facts via less direct methods.
Rather than store all the possibly associated data by meaning, we choose to only add meaning when necessary.
We add meaning when it is part of the immediate problem we are trying to solve.

\subsection{Toward Managers}

[TODO :: LISITNGS SHOWING MANAGERS + CLASSES]

After splitting your classes up into components, you might find your classes look more awkward now they are accessing variables hidden away in new structures.
But it's not your classes that should be looking up variables, but instead transforms on the classes.
A common operation such as rendering requires the position and the model information, but it also requires access to the renderer.
Such object boundary crossing access is seen as a compromise during game development, but here it can be seen as the method by which we move away from a class-centric approach to a data-oriented approach.
We will aim at transforming our data into render requests which affect the graphics pipeline without referring to data unimportant to the renderer.

Referring to listing [TODO :: REFER LISTINGS ABOVE SHOWING MANAGERS], we move to no longer having a player update, but instead an update for each component that makes up the player.
This way, everyone entity's physics is updated before it is rendered, or could be updated while the rendering is happening on another thread.
All entity's controls (whether they be player or AI) can be updated before they are animated.
Having the managers control when the code is executed is a large part of the leap towards fully parallelisable code.
This is where performance can be gained with more confidence that it's not negatively impacting other areas.
Analysing which components need updating every frame, and which can be updated less frequently leads to optimisations that unlock components from each other.

In many component systems that allow scripting languages to define the actions taken by components or their entities, performance can fall foul of the same inefficiencies present in an object-oriented program design.
Notably, the dependency inversion practice of calling Tick or Update functions will often have to be sandboxed in some way which will lead to error checking and other safety measures wrapping the internal call.
There is a good example of this being an issue with the older versions of Unity, where their component based approach allowed every instance to have its own script which would have its own call from the core of Unity on every frame.
The main cost appeared to be transitioning in and out of the scripting language, crossing the boundary between the C++ at the core, and the script that described the behaviour of the component.
In his article 10,000 Update() calls [TODO :: REFERENCE TEN K UPDATE ARTICLE], Valentin Simonov provided information on why the move to managers makes so much sense, giving details on what is costing the most when utilising dependency inversion to drive your general code update strategies.
The main cost was in moving between the different areas of code, but even without having to straddle the language barrier, managers make sense as they ensure updates to components happen in sync.

What happens when we let more than just the player use these arrays?
Normally we'd have some separate logic for handling player fire until we refactored the weapons to be generic weapons with NPCs using the same code for weapons probably by having a new weapon class that can be pointed to by the player or an NPC, but instead what we have here is a way to split off the weapon firing code in such a way as to allow the player and the NPC to share firing code without inventing a new class to hold the firing.
 In fact, what we've done is split the firing up into the different tasks it really contains.

Tasks are good for parallel processing, and with component based objects, we open up the opportunity to move most of our previously class oriented processes out, and into more generic tasks that can be dished out to whatever CPU or co-processor can handle them.

\end{document}
