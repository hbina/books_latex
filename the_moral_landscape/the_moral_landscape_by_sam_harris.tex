\documentclass[a4paper,14pt]{extbook}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.5in]{geometry}

\renewcommand{\baselinestretch}{1.5}

\title{The Moral Landscape : How Science Can Determine Human Values}
\author{Sam Harris}

\begin{document}

\maketitle
\tableofcontents

\newpage
\section{Introduction}

\subsection{The Moral Landscape}

The people of Albania have a venerable tradition of vendetta called \textit{Kanun} :
if a man commits a murder, his victim's family can kill any one of his male relatives in reprisal.
If a boy has the misfortune of being the son or brother of a murderer, he must spend his days and nights in hiding, forgoing a proper education, adequate health care, and the pleasures of a normal life.
Untold numbers of Albanian men and boys live as prisoners of their homes even now.
Can we say that the Albanians are morally wrong to have structured their society in this way?
Is their tradition of blood feud a form of evil?
Are their values inferior to our own?

Most people imagine that science cannot pose, much less answer, questions of this sort.
How could we ever say, as a matter of scientific fact, that one way of life is better, or moral, than another?
Whose definition of ``better'' or ``moral'' would we use?
While many scientists now study the evolution of morality, as well as its underlying neurobiology, the purpose of their research is merely to describe how human beings think and behave.
No one expects science to tell us how we \textit{ought} to think and behave.
Controversies about human values are controversies about which science officially has no opinion.

I will argue, however, that questions about values --- about meaning, morality, and life's larger purpose --- are really questions about the well-being of conscious creatures.
Values, therefore, translate into facts that can be scientifically understood: regarding positive and negative social emotions, retributive impulses, the effects of specific laws and social institutions on human relationships, the neurophysiology of happiness and suffering, etc.
The most important of these facts are bound to transcend culture --- just as facts about physical and mental health do.
Cancer in the highlands of New Guinea is still cancer;
cholera is still cholera;
schizophrenia is still schizophrenia;
and so, too, I will argue, compassion is still compassion, and well-being is still well-being.
And if these are important cultural differences in how people flourish --- if, for instance, there are incompatible but equivalent ways to raise happy, intelligent, and creative children --- these differences are also facts that must depend upon the organization of the human brain.
In principle, therefore, we can account for the ways in which culture defines us within the context of neuroscience and psychology.
The more we understand ourselves at the level of the brain, the more we will see that there are right and wrong answers to questions of human values.

Of course, we will have to confront some ancient disagreements about the status of moral truth:
people who draw their worldview from religion generally believe that moral truths exists, but only because God has woven it into the very fabric of reality;
while those who lack such faith tend to think that notions of ``good'' and ``evil'' must be the products of evolutionary pressure and cultural invention.
On the first account, to speak of ``moral truth'' is, of necessity, to invoke God;
on the second, it is merely to give voice to one's apish urges, cultural biases, and philosophical confusion.
My purpose is to persuade you that both sides in this debate are wrong.
The goal of this book is to begin a conversation about how moral truth can be understood in the context of science.

While the argument I make in this book is bound to be controversial, it rests on a very simple premise :
human well-being entirely depends in the events in the world and on states of human brain.
Consequently, there must be scientific truths to be known about it.
A more detailed understanding of these truths will force us to draw clear distinctions between different ways of living in society with one another, judging some to be better or worse, more or less true to the facts, and more or less ethical.
Clearly, such insights could help us improve the quality of human life --- and this is where academic debate ends and choices affecting the lives of millions of people begin.

I am not suggesting that we are guaranteed to resolve every moral controversy through science.
Differences of opinion will remain --- but opinions will be increasingly constraints by facts.
And it is important to realize that our inability to answer a question say nothing whether the question itself has an answer.
Exactly how many people were bitten by mosquitoes in the last sixty seconds?
How many of these people will contract malaria?
How many will die as a result?
Given the technical challenges involved, no team of scientists could possibly respond to such questions.
And yet we know that they admit of simple numerical answers.
Does our inability to gather the relevant data oblige us to respect all opinions equally?
Of course not,
In the same way, the fact that we not be able to resolve specific moral dilemmas does not suggest that all competing responses to them are equally valid.
In my experience, mistaking \textit{no answers in practice} for \textit{fno answers in principle} is a great source of moral confusion.

There are, for instance, twenty-one U. S. states that still allow corporal punishment in their schools.
These are places where it is actually legal for a teacher to beat a child with a wooden board hard enough to raise large bruises and even to break the skin.
Hundreds of thousands of children are subjected to this violence each year, almost exclusively in the South.
Needless to say, the rationale for this behavior is explicitly religious :
for the Creator of the Universe Himself has told us not to spare the rod, lest we spoil the Child (Proverbs 13:24, 20:30 and 23:13-14).
However, if we are actually concerned about human well-being, and would treat children in such a way as to promote it, we might wonder whether it is generally wise to subject little boys and girls to pain, terror, and public humiliation as a means of encouraging their cognitive and emotional development.
Is there any doubt that this question \textit{has} an answer?
Is there any doubt that it matters that we get it right?
In fact, all the research indicates that corporal punishment is a disastrous practice, leading to more violence and social pathology --- and, perversely, to greater support for corporal punishment.

But the deeper point is that there simply must be answers to questions of this kind, whether we know them or not.
And these are not areas where we can afford to simply respect the ``traditions'' of others and agree to disagree.
Why will science increasingly decide such questions?
Because the discrepant answers people give to them --- along with the consequences that follow in terms of human relationships, states of mind, acts of violence, entanglements with the law, etc. --- translate into differences in our brains, in the brains of others, and in the world at large.
I hope to show that when talking about values, we are actually talking about an interdependent worlds of facts.

There are facts to be understood about how thoughts and intentions arise in the human brain;
there are facts to be learned about how these mental states translate into behavior;
there are further facts to be known about how these behaviors influence the world and the experience of other conscious beings.
We will see that facts of this sort exhaust what we can reasonably mean by terms like ``good'' and ``evil''.
They will also increasingly fall within the purview of science and run far deeper than a person's religious affiliation.
Just as there is no such things as Christian physics or Muslim algebra, we will see that there is no such things as Christian or Muslim morality.
Indeed, I will argue that morality should be considered an underdeveloped branch of science.

Since the publication of my first book, \textit{The End of Faith}, I have had a privileged view of the ``culture wars'' --- both in the United States, between secular liberals and Christian conservatives, and in Europe, between largely irreligious societies and their growing Muslim populations.
Having received tens of thousands of letters and emails from people at every pint on the continuum between faith and doubt, I can say with some confidence that a shared belief in the limitations of reason lies at the bottom these cultural divides.
Both sides believe that reason is powerless to answer the most important questions in human life.
And how a person perceives the gulf between facts and values seems to influence his views on almost every issue of social importance --- from the fighting of wars to the education of children.

This rupture in our thinking has different consequences at each end of the political spectrum :
religious conservatives tend to believe that there are right answers to questions of meaning and morality, but only because the God of Abraham deems it so.
They concede that ordinary facts can be discovered through rational inquiry, but they believe that values must come from a voice in a whirlwind.
Scriptural literalism, intolerance of diversity, mistrust of science, disregard for the real causes of human and animal suffering --- too often, this is how the division between facts and values expresses itself on the religious right.

Secular liberals, on the other hand, tend to imagine that no objectives answers to moral questions exist.
While John Stuart Mill might conform to \textit{our} cultural ideal of goodness better than Osama Bin Laden does, most secularists suspect that Mill's ideas about right and wrong reach no closer to the Truth.
Multiculturalism, moral relativism, political correctness, tolerance even of \textit{intolerance} --- these are familiar consequences of separating facts and values on the left.

It should concern us that these two orientations are not equally empowering.
Increasingly, secular democracies are left supine before the unreasoning zeal of old-time religion.
The juxtaposition of conservative dogmatism and liberal doubt accounts for the decade that has been lost in the United States to a ban on federal funding for embryonic stem-cell research;
it explains the years of political distraction we have suffered, and will continue to suffer, over issues like abortion and gay marriage;
it lies at the bottom of current efforts to pass anti-blasphemy laws at the United Nations (which would make it illegal for the citizens of member states to criticize religion);
it has hobbled the West in its generational war against radical Islam;
and it may yet refashion the societies of Europe into a new Caliphate.
Knowing what the Creator of the Universe believes about right and wrong inspires religious conservatives to enforce this vision in the public sphere at almost any cost; not knowing what is right—or that anything can ever be truly right—often leads secular liberals to surrender their intellectual standards and political freedoms with both hands.

The scientific community is predominantly secular and liberal --- and the concessions that scientists have made to religious dogmatism have been breathtaking.
As we will see, the problem reaches as high as the National Academies of Science and the National Institutes of Health.
Even the journal \textit{Nature}, the most influential scientific publication on Earth, has been unable to reliably police the boundary between reasoned discourse and pious fiction.
I recently reviewed every appearance of the term ``religion'' in the journal going back ten years and found that \textit{Nature}'s editors have generally accepted Stephen J. Gould's doomed notion of ``non-overlapping magisteria'' --- the idea that science and religion, properly construed, cannot be in conflict because they constitute different domains of expertise.
As one editorial put it, problems arise only when these disciplines ``stray onto each other's territories and stir up trouble. ''
The underlying claim is that while science is the best authority on the workings of the physical universe, religion is the best authority on meaning, values, morality, and the good life.
I hope to persuade you that this is not only untrue, it could not possibly be true.
Meaning, values, morality, and the good life must relate to facts about the well-being of conscious creatures --- and, in our case, must lawfully depend upon events in the world and upon states of the human brain.
Rational, open-ended, honest inquiry has always been the true source of insight into such processes.
Faith, if it is ever right about anything, is right by accident.

The scientific community's reluctance to take a stand on moral issues has come at a price.
It has made science appear divorced, in principle, from the most important questions of human life.
From the point of view of popular culture, science often seems like little more than a hatchery for technology.
While most educated people will concede that the scientific method has delivered centuries of fresh embarrassment to religion on matters of fact, it is now an article of almost unquestioned certainty, both inside and outside scientific circles, that science has nothing to say about what constitutes a good life.
Religious thinkers in all faiths, and on both ends of the political spectrum, are united on precisely this point;
the defense one most often hears for belief in God is not that there is compelling evidence for His existence, but that faith in Him is the only reliable source of meaning and moral guidance.
Mutually incompatible religious traditions now take refuge behind the same non sequitur.

It seems inevitable, however, that science will gradually encompass life's deepest questions --- and this is guaranteed to provoke a backlash.
How we respond to the resulting collision of worldviews will influence the progress of science, of course, but it may also determine whether we succeed in building a global civilization based on shared values.
The question of how human beings should live in the twenty-first century has many competing answers --- and most of them are surely wrong.
Only a rational understanding of human well-being will allow billions of us to coexist peacefully, converging on the same social, political, economic, and environmental goals.
A science of human flourishing may seem a long way off, but to achieve it, we must first acknowledge that the intellectual terrain actually exists.

Throughout this book I make reference to a hypothetical space that I call ``the moral landscape'' --- a space of real and potential outcomes whose peaks correspond to the heights of potential well-being and whose valleys represent the deepest possible suffering.
Different ways of thinking and behaving --- different cultural practices, ethical codes, modes of government, etc. --- will translate into movements across this landscape and, therefore, into different degrees of human flourishing.
I'm not suggesting that we will necessarily discover one right answer to every moral question or a single best way for human beings to live.
Some questions may admit of many answers, each more or less equivalent.
However, the existence of multiple peaks on the moral landscape does not make them any less real or worthy of discovery.
Nor would it make the difference between being on a peak and being stuck deep in a valley any less clear or consequential.

To see that multiple answers to moral questions need not pose a problem for us, consider how we currently think about food :
no one would argue that there must be one right food to eat.
And yet there is still an objective difference between healthy food and poison.
There are exceptions --- some people will die if they eat peanuts, for instance --- but we can account for these within the context of a rational discussion about chemistry, biology, and human health.
The world's profusion of foods never tempts us to say that there are no facts to be known about human nutrition or that all culinary styles must be equally healthy in principle.

Movement across the moral landscape can be analyzed on many levels --- ranging from biochemistry to economics --- but where human beings are concerned, change will necessarily depend upon states and capacities of the human brain.
While I fully support the notion of ``consilience'' in science --- and, therefore, view the boundaries between scientific specialties as primarily a function of university architecture and limitations on how much any one person can learn in a lifetime --- the primacy of neuroscience and the other sciences of mind on questions of human experience cannot be denied.
Human experience shows every sign of being determined by, and realized in, states of the human brain.

Many people seem to think that a universal conception of morality requires that we find moral principles that admit of no exceptions.
If, for instance, it is truly wrong to lie, it must \textit{always} be wrong to lie --- and if one can find a single exception, any notion of moral truth must be abandoned.
But the existence of moral truth --- that is, the connection between how we think and behave and our well-being --- does not require that we define morality in terms of unvarying moral precepts.
Morality could be a lot like chess :
there are surely principles that generally apply, but they might admit of important exceptions.
If you want to play good chess, a principle like ``Don't lose your Queen'' is almost always worth following.
But it admits of exceptions :
sometimes sacrificing your Queen is a brilliant thing to do;
occasionally, it is the \textit{only} thing you can do.
It remains a fact, however, that from any position in a game of chess there will be a range of objectively good moves and objectively bad ones.
If there are objective truths to be known about human well-being --- if kindness, for instance, is generally more conducive to happiness than cruelty is --- then science should one day be able to make very precise claims about which of our behaviors and uses of attention are morally good, which are neutral, and which are worth abandoning.

While it is too early to say that we have a full understanding of how human beings flourish, a piecemeal account is emerging.
Consider, for instance, the connection between early childhood experience, emotional bonding, and a person's ability to form healthy relationships later in life.
We know, of course, that emotional neglect and abuse are not good for us, psychologically or socially.
We also know that the effects of early childhood experience must be realized in the brain.
Research on rodents suggests that parental care, social attachment, and stress regulation are governed, in part, by the hormones vasopressin and oxytocin, because they influence activity in the brain's reward system.
When asking why early childhood neglect is harmful to our psychological and social development, it seems reasonable to think that it might result from a disturbance in this same system.

While it would be unethical to deprive young children of normal care for the purposes of experiment, society inadvertently performs such experiments every day.
To study the effects of emotional deprivation in early childhood, one group of researchers measured the blood concentrations of oxytocin and vasopressin in two populations :
children raised in traditional homes and children who spent their first years in an orphanage.
As you might expect, children raised by the State generally do not receive normal levels of nurturing.
They also tend to have social and emotional difficulties later in life.
As predicted, these children failed to show a normal surge of oxytocin and vasopressin in response to physical contact with their adoptive mothers.
The relevant neuroscience is in its infancy, but we know that our emotions, social interactions, and moral intuitions mutually influence one another.
We grow attuned to our fellow human beings through these systems, creating culture in the process.
Culture becomes a mechanism for further social, emotional, and moral development.
There is simply no doubt that the human brain is the nexus of these influences.
Cultural norms influence our thinking and behavior by altering the structure and function of our brains.
Do you feel that sons are more desirable than daughters?
Is obedience to parental authority more important than honest inquiry?
Would you cease to love your child if you learned that he or she was gay?
The ways parents view such questions, and the subsequent effects in the lives of their children, must translate into facts about their brains.

My goal is to convince you that human knowledge and human values can no longer be kept apart.
The world of measurement and the world of meaning must eventually be reconciled.
And science and religion --- being antithetical ways of thinking about the same reality -- will never come to terms.
As with all matters of fact, differences of opinion on moral questions merely reveal the incompleteness of our knowledge;
they do not oblige us to respect a diversity of views indefinitely.

\subsection{Facts and Values}

The eighteenth-century Scottish philosopher David Hume famously argued that no description of the way the world is (facts) can tell us how we ought to behave (morality).
Following Hume, the philosopher G. E. Moore declared that any attempt to locate moral truths in the natural world was to commit a ``naturalistic fallacy.''
Moore argued that goodness could not be equated with any property of human experience (e.g., pleasure, happiness, evolutionary fitness) because it would always be appropriate to ask whether the property on offer was itself \textit{good}.
If, for instance, we were to say that goodness is synonymous with whatever gives people pleasure, it would still be possible to worry whether a specific instance of pleasure is actually \textit{good}.
This is known as Moore's ``open question argument.''
And while I think this verbal trap is easily avoided when we focus on human well-being, most scientists and public intellectuals appear to have fallen into it.
Other influential philosophers, including Karl Popper, have echoed Hume and Moore on this point, and the effect has been to create a firewall between facts and values throughout our intellectual discourse.

While psychologists and neuroscientists now routinely study human happiness, positive emotions, and moral reasoning, they rarely draw conclusions about how human beings ought to think or behave in light of their findings.
In fact, it seems to be generally considered intellectually disreputable, even vaguely authoritarian, for a scientist to suggest that his or her work offers some guidance about how people should live.
The philosopher and psychologist Jerry Fodor crystallizes the view :

Science is about facts, not norms;
it might tell us how we are, but it couldn't tell us what is wrong with how we are.
There couldn't be a science of the human condition.

While it is rarely stated this clearly, this faith in the intrinsic limits of reason is now the received opinion in intellectual circles.

Despite the reticence of most scientists on the subject of good and evil, the scientific study of morality and human happiness is well underway.
This research is bound to bring science into conflict with religious orthodoxy and popular opinion --- just as our growing understanding of evolution has --- because the divide between facts and values is illusory in at least three senses :

\begin{enumerate}

      \item
            Whatever can be known about maximizing the well-being of conscious creatures --- which is, I will argue, the only thing we can reasonably value --- must at some point translate into facts about brains and their interaction with the world at large;
      \item
            The very idea of ``objective'' knowledge (i.e., knowledge acquired through honest and reasoning) has values built into it, as every effort we make to discuss facts depends upon principles that we must first value (i.e. logical consistency, reliance on evidence, parsimony, etc.);
      \item
            Beliefs about facts and beliefs about values seem to arise from similar processes at the level of the brain:
            it appears that we have a common system for judging truth and falsity in both domains.

\end{enumerate}


I will discuss each of these points in greater detail below.
Both in terms of what there is to know about the world and the brain mechanisms that allow us to know it, we will see that a clear boundary between facts and values simply does not exist.

Many readers might wonder how can we base our values on something as difficult to define as ``well-being''?
It seems to me, however, that the concept of well-being is like the concept of physical health :
it resists precise definition, and yet it is indispensable.
In fact, the meanings of both terms seem likely to remain perpetually open to revision as we make progress in science.
Today, a person can consider himself physically healthy if he is free of detectable disease, able to exercise, and destined to live into his eighties without suffering obvious decrepitude.
But this standard may change.
If the biogerontologist Aubrey de Grey is correct in viewing aging as an engineering problem that admits of a full solution, being able to walk a mile on your hundredth birthday will not always constitute ``health.''
There may come a time when not being able to run a marathon at age five hundred will be considered a profound disability.
Such a radical transformation of our view of human health would not suggest that current notions of health and sickness are arbitrary, merely subjective, or culturally constructed.
Indeed, the difference between a healthy person and a dead one is about as clear and consequential a distinction as we ever make in science.
The differences between the heights of human fulfillment and the depths of human misery are no less clear, even if new frontiers await us in both directions.

If we define ``good'' as that which supports well-being, as I will argue we must, the regress initiated by Moore's ``open question argument'' really does stop.
While I agree with Moore that it is reasonable to wonder whether maximizing pleasure in any given instance is ``good,'' it makes no sense at all to ask whether maximizing well-being is ``good.''
It seems clear that what we are really asking when we wonder whether a certain state of pleasure is ``good,'' is whether it is conducive to, or obstructive of, some deeper form of well-being.
This question is perfectly coherent;
it surely has an answer (whether or not we are in a position to answer it);
and yet, it keeps notions of goodness anchored to the experience of sentient beings.

Defining goodness in this way does not resolve all questions of value;
it merely directs our attention to what values actually are --- the set of attitudes, choices, and behaviors that potentially affect our well-being, as well as that of other conscious minds.
While this leaves the question of what constitutes well-being genuinely open, there is every reason to think that this question has a finite range of answers.
Given that change in the well-being of conscious creatures is bound to be a product of natural laws, we must expect that this space of possibilities --- the moral landscape --- will increasingly be illuminated by science.

It is important to emphasize that a scientific account of human values --- i.e., one that places them squarely within the web of influences that link states of the world and states of the human brain --- is not the same as an evolutionary account.
Most of what constitutes human well-being at this moment escapes any narrow Darwinian calculus.
While the possibilities of human experience must be realized in the brains that evolution has built for us, our brains were not designed with a view to our ultimate fulfillment.
Evolution could never have foreseen the wisdom or necessity of creating stable democracies, mitigating climate change, saving other species from extinction, containing the spread of nuclear weapons, or of doing much else that is now crucial to our happiness in this century.

As the psychologist Steven Pinker has observed, if conforming to the dictates of evolution were the foundation of subjective well-being, most men would discover no higher calling in life than to make daily contributions to their local sperm bank.
After all, from the perspective of a man's genes, there could be nothing more fulfilling than spawning thousands of children without incurring any associated costs or responsibilities.
But our minds do not merely conform to the logic of natural selection.
In fact, anyone who wears eyeglasses or uses sunscreen has confessed his disinclination to live the life that his genes have made for him.
While we have inherited a multitude of yearnings that probably helped our ancestors survive and reproduce in small bands of hunter-gatherers, much of our inner life is frankly incompatible with our finding happiness in today's world.
The temptation to start each day with several glazed donuts and to end it with an extramarital affair might be difficult for some people to resist, for reasons that are easily understood in evolutionary terms, but there are surely better ways to maximize one's long-term well-being.
I hope it is clear that the view of ``good'' and ``bad'' I am advocating, while fully constrained by our current biology (as well as by its future possibilities), cannot be directly reduced to instinctual drives and evolutionary imperatives.
As with mathematics, science, art, and almost everything else that interests us, our modern concerns about meaning and morality have flown the perch built by evolution.

\subsection{The Importance of Belief}

The human brain is an engine of belief.
Our minds continually consume, produce, and attempt to integrate ideas about ourselves and the world that purport to be true :
\textit{Iran is developing nuclear weapons;}
\textit{the seasonal flu can be spread through casual contact;}
\textit{I actually look better with gray hair.}
What must we do to believe such statements?
What, in other words, must a brain do to accept such propositions as true?
This question marks the intersection of many fields :
psychology, neuroscience, philosophy, economics, political science, and even jurisprudence.

Belief also bridges the gap between facts and values.
We form beliefs about facts :
and belief in this sense constitutes most of what we know about the world --- through science, history, journalism, etc.
But we also form beliefs about values :
judgments about morality, meaning, personal goals, and life's larger purpose.
While they might differ in certain respects, beliefs in these two domains share very important features.
Both types of belief make tacit claims about right and wrong :
claims not merely about how we think and behave, but about how we should think and behave.
Factual beliefs like ``water is two parts hydrogen and one part oxygen'' and ethical beliefs like ``cruelty is wrong'' are not expressions of mere preference.
To really believe either proposition is also to believe that you have accepted it for legitimate reasons.
It is, therefore, to believe that you are in compliance with certain norms --- that you are sane, rational, not lying to yourself, not confused, not overly biased, etc.
When we believe that something is factually true or morally good, we also believe that another person, similarly placed, should share our belief.
This seems unlikely to change.
In chapter 3, we will see that both the logical and neurological properties of belief further suggest that the divide between facts and values is illusory.

\subsection{The Bad Life and the Good Life}

For my argument about the moral landscape to hold, I think one need only grant two points :

\begin{enumerate}
      \item Some people have better lives than others, and
      \item These differences relate, in some lawful and not entirely arbitrary ways, to states of the human brain and to states of the world.
\end{enumerate}

To make these premises less abstract, consider two generic lives that lie somewhere near the extremes on this continuum :

\subsubsection{The Bad Life}

You are a young widow who has lived her entire life in the midst of civil war.
Today, your seven-year-old daughter was raped and dismembered before your eyes.
Worse still, the perpetrator was your fourteen-year-old son, who was goaded to this evil at the point of a machete by a press gang of drug-addicted soldiers.
You are now running barefoot through the jungle with killers in pursuit.
While this is the worst day of your life, it is not entirely out of character with the other days of your life :
since the moment you were born, your world has been a theater of cruelty and violence.
You have never learned to read, taken a hot shower, or traveled beyond the green hell of the jungle.
Even the luckiest people you have known have experienced little more than an occasional respite from chronic hunger, fear, apathy, and confusion.
Unfortunately, you've been very unlucky, even by these bleak standards.
Your life has been one long emergency, now it is nearly over.

\subsubsection{The Good Life}

You are married to the most loving, intelligent, and charismatic person you have ever met.
Both of you have careers that are intellectually stimulating, and financially rewarding.
For decades, your wealth and social connections have allowed you to devote yourself to activities that bring you immense personal satisfaction.
One of your greatest sources of happiness has been to find creative ways to help people who have not had your good fortune in life.
In fact, you have just won a billion-dollar grant to benefit children in the developing world.
If asked, you would say that you could not imagine how your time on Earth could be better spent.
Due to a combination of good genes and optimal circumstances, you and your closest friends and family will live very long, healthy lives, untouched by crimes, sudden bereavements, and other misfortunes.

The examples I have picked, while generic, are nonetheless real --- in that they represent lives that some human beings are likely to be leading at this moment.
While there are surely ways in which this spectrum of suffering and happiness might be extended, I think these cases indicate the general range of experience that is accessible, in principle, to most of us.
I also think it is indisputable that most of what we do with our lives is predicated on there being nothing more important, at least for ourselves and for those closest to us, than the difference between the Bad Life and the Good Life.

Let me simply concede that if you don't see a distinction between these two lives that is worth valuing (premise 1 above), there may be nothing I can say that will attract you to my view of the moral landscape.
Likewise, if you admit that these lives are different, and that one is surely better than the other, but you believe these differences have no lawful relationship to human behavior, societal conditions, or states of the brain (premise 2), then you will also fail to see the point of my argument.
While I don't see how either premise 1 or 2 can be reasonably doubted, my experience discussing these issues suggests that I should address such skepticism, however far-fetched it may seem.

There are actually people who claim to be unimpressed by the difference between the Bad Life and the Good Life.
I have even met people who will go so far as to deny that any difference exists.
While they will acknowledge that we habitually speak and act as if there were a continuum of experience that can be described by words like ``misery,'' ``terror,'' ``agony,'' ``madness,'' etc., on one end and ``well-being,'' ``happiness,'' ``peace,'' ``bliss,'' etc., on the other, when the conversation turns to philosophical and scientific matters, such people will say learned things like, ``but, of course, that is just how we play our particular language game. It doesn't mean there is a difference in reality.''
One hopes that these people take life's difficulties in stride.
They also use words like ``love'' and ``happiness,'' from time to time, but we should wonder what these terms could signify that does not entail a preference for the Good Life over the Bad Life.
Anyone who claims to see no difference between these two states of being (and their concomitant worlds), should be just as likely to consign himself and those he ``loves'' to one or the other at random and call the result ``happiness.''

Ask yourself, if the difference between the Bad Life and the Good Life doesn't matter to a person, what could possibly matter to him?
Is it conceivable that something might matter more than this difference, expressed on the widest possible scale?
What would we think of a person who said, ``Well, I could have delivered all seven billion of us into the Good Life, but I had other priorities.''
Would it be \textit{possible} to have other priorities?
Wouldn't any real priority be best served amid the freedom and opportunity afforded by the Good Life?
Even if you happen to be a masochist who fancies an occasional taunting with a machete, wouldn't this desire be best satisfied in the context of the Good Life?

Imagine someone who spends all his energy trying to move as many people as possible toward the Bad Life, while another person is equally committed to undoing this damage and moving people in the opposite direction:
Is it conceivable that you or anyone you know could overlook the differences between these two projects?
Is there any possibility of confusing them or their underlying motivations?
And won't there necessarily be objective conditions for these differences?
If, for instance, one's goal were to place a whole population securely in the Good Life, wouldn't there be more and less effective ways of doing this?
How would forcing boys to rape and murder their female relatives fit into the picture?

I do not mean to belabor the point, but the point is crucial --- and there is a pervasive assumption among educated people that either such differences don't exist, or that they are too variable, complex, or culturally idiosyncratic to admit of general value judgments.
However, the moment one grants there is a difference between the Bad Life and the Good Life that lawfully relates to states of the human brain, to human behavior, and to states of the world, one has admitted that there are right and wrong answers to questions of morality.
To make sure this point is nailed down, permit me to consider a few more objections :

\textit{What if, seen in some larger context, the Bad Life is actually better than the Good Life --- e.g., what if all those child soldiers will be happier in some afterlife, because they have been purified of sin or have learned to call God by the right name, while the people in the Good Life will get tortured in some physical hell for eternity?}

If the universe is really organized this way, much of what I believe will stand corrected on the Day of Judgment.
However, my basic claim about the connection between facts and values would remain unchallenged.
The rewards and punishments of an afterlife would simply alter the temporal characteristics of the moral landscape.
If the Bad Life is actually better over the long run than the Good Life --- because it wins you endless happiness, while the Good Life represents a mere dollop of pleasure presaging an eternity of suffering --- then the Bad Life would surely be better than the Good Life.
If this were the way the universe worked, we would be morally obligated to engineer an appropriately pious Bad Life for as many people as possible.
Under such a scheme, there would still be right and wrong answers to questions of morality, and these would still be assessed according to the experience of conscious beings.
The only thing left to be decided is how reasonable it is to worry that the universe might be structured in so bizarre a way.
It is not reasonable at all, I think --- but that is a different discussion.

\textit{What if certain people would actually prefer the Bad Life to the Good Life?
      Perhaps there are psychopaths and sadists who can expect to thrive in the context of the Bad Life and would enjoy nothing more than killing other people with machetes.}

Worries like this merely raise the question of how we should value dissenting opinions.
Jeffrey Dahmer's idea of a life well lived was to kill young men, have sex with their corpses, dismember them, and keep their body parts as souvenirs.
We will confront the problem of psychopathy in greater detail in chapter 3.
For the moment, it seems sufficient to notice that in any domain of knowledge, we are free to say that certain opinions do not count.
In fact, we must say this for knowledge or expertise to count at all.
Why should it be any different on the subject of human well-being?

Anyone who doesn't see that the Good Life is preferable to the Bad Life is unlikely to have anything to contribute to a discussion about human well-being.
Must we really argue that beneficence, trust, creativity, etc., enjoyed in the context of a prosperous civil society are better than the horrors of civil war endured in a steaming jungle filled with aggressive insects carrying dangerous pathogens?
I don't think so.
In the next chapter, I will argue that anyone who would seriously maintain that the opposite is the case --- or even that it \textit{might} be the case --- is either misusing words or not taking the time to consider the details.

If we were to discover a new tribe in the Amazon tomorrow, there is not a scientist alive who would assume \textit{a priori} that these people must enjoy optimal physical health and material prosperity.
Rather, we would ask questions about this tribe's average lifespan, daily calorie intake, the percentage of women dying in childbirth, the prevalence of infectious disease, the presence of material culture, etc.
Such questions would have answers, and they would likely reveal that life in the Stone Age entails a few compromises.
And yet news that these jolly people enjoy sacrificing their firstborn children to imaginary gods would prompt many (even most) anthropologists to say that this tribe was in possession of an alternate moral code every bit as valid and impervious to refutation as our own.
However, the moment one draws the link between morality and well-being, one sees that this is tantamount to saying that the members of this tribe must be as fulfilled, psychologically and socially, as any people on Earth.
The disparity between how we think about physical health and mental/societal health reveals a bizarre double standard :
one that is predicated on our not knowing --- or, rather, on our \textit{pretending} not to know --- anything at all about human well-being.

Of course, some anthropologists have refused to follow their colleagues over the cliff.
Robert Edgerton performed a book-length exorcism on the myth of the ``noble savage,'' detailing the ways in which the most influential anthropologists of the 1920s and 1930s --- such as Franz Boas, Margaret Mead, and Ruth Benedict --- systematically exaggerated the harmony of folk societies and ignored their all too frequent barbarism or reflexively attributed it to the malign influence of colonialists, traders, missionaries, and the like.
Edgerton details how this romance with mere difference set the course for the entire field.
Thereafter, to compare societies in moral terms was deemed impossible.
Rather, it was believed that one could only hope to understand and accept a culture on its own terms.
Such cultural relativism became so entrenched that by 1939 one prominent Harvard anthropologist wrote that this suspension of judgment was ``probably the most meaningful contribution which anthropological studies have made to general knowledge.''
Let's hope not.
In any case, it is a contribution from which we are still struggling to awaken.

Many social scientists incorrectly believe that all long-standing human practices must be evolutionarily adaptive :
for how else could they persist?
Thus, even the most bizarre and unproductive behaviors --- female genital excision, blood feuds, infanticide, the torture of animals, scarification, foot binding, cannibalism, ceremonial rape, human sacrifice, dangerous male initiations, restricting the diet of pregnant and lactating mothers, slavery, potlatch, the killing of the elderly, sati, irrational dietary and agricultural taboos attended by chronic hunger and malnourishment, the use of heavy metals to treat illness, etc. --- have been rationalized, or even idealized, in the fire-lit scribblings of one or another dazzled ethnographer.
But the mere endurance of a belief system or custom does not suggest that it is adaptive, much less wise.
It merely suggests that it hasn't led directly to a society's collapse or killed its practitioners outright.

The obvious difference between genes and \textit{memes} (e.g., beliefs, ideas, cultural practices) is also important to keep in view.
The latter are \textit{communicated};
they do not travel with the gametes of their human hosts.
The survival of memes, therefore, is not dependent on their conferring some actual benefit (reproductive or otherwise) on individuals or groups.
It is quite possible for people to traffic in ideas and other cultural products that diminish their well-being for centuries on end.

Clearly, people can adopt a form of life that needlessly undermines their physical health --- as the average lifespan in many primitive societies is scarcely a third of what it has been in the developed world since the middle of the twentieth century.
Why isn't it equally obvious that an ignorant and isolated people might undermine their psychological well-being or that their social institutions could become engines of pointless cruelty, despair, and superstition?
Why is it even slightly controversial to imagine that some tribe or society could harbor beliefs about reality that are not only false but demonstrably harmful?

Every society that has ever existed has had to channel and subdue certain aspects of human nature --- envy, territorial violence, avarice, deceit, laziness, cheating, etc. --- through social mechanisms and institutions.
It would be a miracle if all societies --- irrespective of size, geographical location, their place in history, or the genomes of their members --- had done this equally well.
And yet the prevailing bias of cultural relativism assumes that such a miracle has occurred not just once, but always.
Let's take a moment to get our bearings.
From a factual point of view, is it possible for a person to believe the wrong things?
Yes.
It is possible for a person to value the wrong things (that is, to believe the wrong things about human well-being)?
I am arguing that the answer to this question is an equally emphatic ``yes'' and, therefore, that science should increasingly inform our values.
Is it possible that certain people are incapable of wanting what they should want?
Of course --- just as there will always be people who are unable to grasp specific facts or believe certain true propositions.
As with every other description of a mental capacity or incapacity, these are ultimately statements about the human brain.

\subsection{Can Suffering Be Good?}

It seems clear that ascending the slopes of the moral landscape may sometimes require suffering.
It may also require negative social emotions, like guild and indignation.
Again, the analogy with physical health seems useful :
we must occasionally experience some unpleasantness --- medication, surgery, etc. --- in order to avoid greater suffering or death.
This principle seems to apply throughout our lives.
Merely learning to read or to play a new sport can produce feelings of deep frustration.
And yet there is little question that acquiring such skills generally improves our lives.
Even periods of depression may lead to better life decisions and to creative insights.
This seems to be the way our minds work.
So be it.

Of course, this principle also applies to civilization as a whole.
Merely making necessary improvements to a city's infrastructure greatly inconveniences millions of people.
And unintended effects are always possible.
For instance, the most dangerous road on Earth now appears to be a two-lane highway between Kabul and Jalalabad.
When it was unpaved, cratered, and strewn with boulders, it was comparatively safe.
But once some helpful Western contractors improved it, the driving skills of the local Afghans were finally liberated from the laws of physics.
Many now have a habit of passing slow-moving trucks on blind curves, only to find themselves suddenly granted a lethally unimpeded view of a thousand-foot gorge.
Are there lessons to be learned from such missteps in the name of progress?
Of course.
But they do not negate the reality of progress.
Again, the difference between the Good Life and the Bad Life could not be clearer :
the question, for both individuals and groups, is how can we most reliably move in one direction and avoid moving in the other?

\subsubsection{The Problem of Religion}

Anyone who wants to understand the world should be open to new facts and new arguments, even on subjects where his or her views are very well established.
Similarly, anyone truly interested in morality --- in the principles of behavior that allow people to flourish --- should be open to new evidence and new arguments that bear upon questions of happiness and suffering.
Clearly, the chief enemy of open conversation is dogmatism in all its forms.
Dogmatism is a well-recognized obstacle to scientific reasoning;
and yet, because scientists have been reluctant even to imagine that they might have something prescriptive to say about values, dogmatism is still granted remarkable scope on questions of both truth and goodness under the banner of religion.

In the fall of 2006, I participated in a three-day conference at the Salk Institute entitled Beyond Belief : Science, Religion, Reason, and Survival.
This event was organized by Roger Bingham and conducted as a town-hall meeting before an audience of invited guests.
Speakers included Steven Weinberg, Harold Kroto, Richard Dawkins, and many other scientists and philosophers who have been, and remain, energetic opponents of religious dogmatism and superstition.
It was a room full of highly intelligent, scientifically literate people --- molecular biologists, anthropologists, physicists, and engineers --- and yet, to my amazement, three days were insufficient to force agreement on the simple question of whether there is any conflict at all between religion and science.
Imagine a meeting of mountaineers unable to agree about whether their sport ever entails walking uphill, and you will get a sense of how bizarre our deliberations began to seem.

While at Salk, I witnessed scientists giving voice to some of the most dishonest religious apologies I have ever heard.
It is one thing to be told that the pope is a peerless champion of reason and that his opposition to embryonic stem-cell research is both morally principled and completely uncontaminated by religious dogmatism;
it is quite another to be told this by a Stanford physician who sits on the President's Council on Bioethics.
Over the course of the conference, I had the pleasure of hearing that Hitler, Stalin, and Mao were examples of secular reason run amok, that the Islamic doctrines of martyrdom and jihad are not the cause of Islamic terrorism, that people can never be argued out of their beliefs because we live in an irrational world, that science has made no important contributions to our ethical lives (and cannot), and that it is not the job of scientists to undermine ancient mythologies and, thereby, “take away people's hope” --- all from atheist scientists who, while insisting on their own skeptical hardheadedness, were equally adamant that there was something feckless and foolhardy, even indecent, about criticizing religious belief.
There were several moments during our panel discussions that brought to mind the final scene of Invasion of the Body Snatchers :
people who looked like scientists, had published as scientists, and would soon be returning to their labs, nevertheless gave voice to the alien hiss of religious obscurantism at the slightest prodding.
I had previously imagined that the front lines in our culture wars were to be found at the entrance to a megachurch.
I now realized that we have considerable work to do in a nearer trench.

I have made the case elsewhere that religion and science are in a zero-sum conflict with respect to facts.
Here, I have begun to argue that the division between facts and values is intellectually unsustainable, especially from the perspective of neuroscience.
Consequently, it should come as no surprise that I see very little room for compromise between faith and reason on questions of morality.
While religion is not the primary focus of this book, any discussion about the relationship between facts and values, the nature of belief, and the role of science in public discourse must continually labor under the burden of religious opinion.
I will, therefore, examine the conflict between religion and science in greater depth in chapter 4.

But there is no mystery why many scientists feel that they must pretend that religion and science are compatible.
We have recently emerged --- some of us leaping, some shuffling, others crawling --- out of many dark centuries of religious bewilderment and persecution, into an age when mainstream science is still occasionally treated with overt hostility by the general public and even by governments.
While few scientists living in the West now fear torture or death at the hands of religious fanatics, many will voice concerns about losing their funding if they give offense to religion, particularly in the United States.
It also seems that, given the relative poverty of science, wealthy organizations like the Templeton Foundation (whose endowment currently stands at \$1.5 billion) have managed to convince some scientists and science journalists that it is wise to split the difference between intellectual integrity and the fantasies of a prior age.

Because there are no easy remedies for social inequality, many scientists and public intellectuals also believe that the great masses of humanity are best kept sedated by pious delusions.
Many assert that, while they can get along just fine without an imaginary friend, most human beings will always need to believe in God.
In my experience, people holding this opinion never seem to notice how condescending, unimaginative, and pessimistic a view it is of the rest of humanity --- and of generations to come.

There are social, economic, environmental, and geopolitical costs to this strategy of benign neglect --- ranging from personal hypocrisy to public policies that needlessly undermine the health and safety of millions.
Nevertheless, many scientists seem to worry that subjecting people's religious beliefs to criticism will start a war of ideas that science cannot win.
I believe that they are wrong.
More important, I am confident that we will eventually have no choice in the matter.
Zero-sum conflicts have a way of becoming explicit.
Here is our situation :
if the basic claims of religion are true, the scientific worldview is so blinkered and susceptible to supernatural modification as to be rendered nearly ridiculous;
if the basic claims of religion are false, most people are profoundly confused about the nature of reality, confounded by irrational hopes and fears, and tending to waste precious time and attention --- often with tragic results.
Is this really a dichotomy about which science can claim to be neutral?
The deference and condescension of most scientists on these subjects is part of a larger problem in public discourse :
people tend not to speak honestly about the nature of belief, about the invidious gulf between science and religion as modes of thought, or about the real sources of moral progress.
Whatever is true about us, ethically and spiritually, is discoverable in the present and can be talked about in terms that are not an outright affront to our growing understanding of the world.
It makes no sense at all to have the most important features of our lives anchored to divisive claims about the unique sanctity of ancient books or to rumors of ancient miracles.
There is simply no question that how we speak about human values --- and how we study or fail to study the relevant phenomena at the level of the brain --- will profoundly influence our collective future.

\newpage
\section{Chapter 1 : Moral Truths}

Many people believe that something in the last few centuries of intellectual progress prevents us from speaking in terms of ``moral truth'' and, therefore, from making cross-cultural moral judgments --- or moral judgments at all.
Having discussed this subject in a variety of public forums, I have heard from literally thousands of highly educated men and women that morality is a myth, that statements about human values are without truth conditions (and are, therefore, nonsensical), and that concepts like well-being and misery are so poorly defined, or so susceptible to personal whim and cultural influence, that it is impossible to know anything about them.
Many of these people also claim that a scientific foundation for morality would serve no purpose in any case.
They think we can combat human evil all the while knowing that our notions of ``good'' and ``evil'' are completely unwarranted.
It is always amusing when these same people then hesitate to condemn specific instances of patently abominable behavior.
I don't think one has fully enjoyed the life of the mind until one has seen a celebrated scholar defend the ``contextual'' legitimacy of the burqa, or of female genital mutilation, a mere thirty seconds after announcing that moral relativism does nothing to diminish a person's commitment to making the world a better place.

And so it is obvious that before we can make any progress toward a science of morality, we will have to clear some philosophical brush.
In this chapter, I attempt to do this within the limits of what I imagine to be most readers' tolerance for such projects.
Those who leave this section with their doubts intact are encouraged to consult the endnotes.

First, I want to be very clear about my general thesis :
I am not suggesting that science can give us an evolutionary or neurobiological account of what people do in the name of ``morality.''
Nor am I merely saying that science can help us get what we want out of life.
These would be quite banal claims to make --- unless one happens to doubt the truth of evolution, the mind's dependency on the brain, or the general utility of science.
Rather I am arguing that science can, in principle, help us understand what we \textit{should} do and \textit{should} want --- and, therefore, what \textit{other people} should do and should want in order to live the best lives possible.
My claim is that there are right and wrong answers to moral questions, just as there are right and wrong answers to questions of physics, and such answers may one day fall within reach of the maturing sciences of mind.

Once we see that a concern for well-being (defined as deeply and as inclusively as possible) is the only intelligible basis for morality and values, we will see that there must be a science of morality, whether or not we ever succeed in developing it :
because the well-being of conscious creatures depends upon how the universe is, altogether.
Given that changes in the physical universe and in our experience of it can be understood, science should increasingly enable us to answer specific moral questions.
For instance, would it be better to spend our next billion dollars eradicating racism or malaria?
Which is generally more harmful to our personal relationships, ``white'' lies or gossip?
Such questions may seem impossible to get a hold of at this moment, but they may not stay that way forever.
As we come to understand how human beings can best collaborate and thrive in this world, science can help us find a path leading away from the lowest depths of misery and toward the heights of happiness for the greatest number of people.
Of course, there will be practical impediments to evaluating the consequences of certain actions, and different paths through life may be morally equivalent (i.e., there may be many peaks on the moral landscape), but I am arguing that there are no obstacles, in principle, to our speaking about \textit{moral truth}.

It seems to me, however, that most educated, secular people (and this includes most scientists, academics, and journalists) believe that there is no such thing as moral truth --- only moral preference, moral opinion, and emotional reactions that we mistake for genuine knowledge of right and wrong.
While we can understand how human beings think and behave in the name of ``morality,'' it is widely imagined that there are no right answers to moral questions for science to discover.

Some people maintain this view by defining ``science'' in exceedingly narrow terms, as though it were synonymous with mathematical modeling or immediate access to experimental data.
However, this is to mistake science for a few of its tools.
Science simply represents our best effort to understand what is going on in this universe, and the boundary between it and the rest of rational thought cannot always be drawn.
There are many tools one must get in hand to think scientifically --- ideas about cause and effect, respect for evidence and logical coherence, a dash of curiosity and intellectual honesty, the inclination to make falsifiable predictions, etc. --- and these must be put to use long before one starts worrying about mathematical models or specific data.
Many people are also confused about what it means to speak with scientific ``objectivity'' about the human condition.
As the philosopher John Searle once pointed out, there are two very different senses of the terms ``objective'' and ``subjective.''
The first sense relates to how we know (i.e., epistemology), the second to what there is to know (i.e., ontology).
When we say that we are reasoning or speaking ``objectively,'' we generally mean that we are free of obvious bias, open to counterarguments, cognizant of the relevant facts, and so on.
This is to make a claim about how we are thinking.
In this sense, there is no impediment to our studying subjective (i.e., first-person) facts ``objectively.''

For instance, it is true to say that I am experiencing tinnitus (ringing in my ear) at this moment.
This is a subjective fact about me, but in stating this fact, I am being entirely objective :
I am not lying;
I am not exaggerating for effect;
I am not expressing a mere preference or personal bias.
I am simply stating a fact about what I am hearing at this moment.
I have also been to an otologist and had the associated hearing loss in my right ear confirmed.
No doubt, my experience of tinnitus must have an objective (third-person) cause that could be discovered (likely, damage to my cochlea).
There is simply no question that I can speak about my tinnitus in the spirit of scientific objectivity --- and, indeed, the sciences of mind are largely predicated on our being able to correlate first-person reports of subjective experience with third-person states of the brain.
This is the only way to study a phenomenon like depression :
the underlying brain states must be distinguished with reference to a person's subjective experience.

However, many people seem to think that because moral facts relate to our experience (and are, therefore, ontologically ``subjective''), all talk of morality must be ``subjective'' in the epistemological sense (i.e., biased, merely personal, etc.).
This is simply untrue.
I hope it is clear that when I speak about ``objective'' moral truths, or about the ``objective'' causes of human well-being, I am not denying the necessarily subjective (i.e., experiential) component of the facts under discussion.
I am certainly not claiming that moral truths exist independent of the experience of conscious beings --- like the Platonic Form of the Good --- or that certain actions are intrinsically wrong.
I am simply saying that, given that there are facts --- real facts --- to be known about how conscious creatures can experience the worst possible misery and the greatest possible well-being, it is objectively true to say that there are right and wrong answers to moral questions, whether or not we can always answer these questions in practice.

And, as I have said, people consistently fail to distinguish between there being answers in practice and answers in principle to specific questions about the nature of reality.
When thinking about the application of science to questions of human well-being, it is crucial that we not lose sight of this distinction.
After all, there are countless phenomena that are subjectively real, which we can discuss objectively (i.e., honestly and rationally), but which remain impossible to describe with precision.
Consider the complete set of ``birthday wishes'' corresponding to every conscious hope that people have entertained silently while blowing out candles on birthday cakes.
Will we ever be able to retrieve these unspoken thoughts?
Of course not.
Many of us would be hard-pressed to recall even one of our own birthday wishes.
Does this mean that these wishes never existed or that we can't make true or false statements about them?
What if I were to say that every one of these wishes was phrased in Latin, focused on improvements in solar panel technology, and produced by the activity of exactly 10,000 neurons in each person's brain?
Is this a vacuous assertion?
No, it is quite precise and surely wrong.
But only a lunatic could believe such a thing about his fellow human beings.
Clearly, we can make true or false claims about human (and animal) subjectivity, and we can often evaluate these claims without having access to the facts in question.
This is a perfectly reasonable, scientific, and often necessary thing to do.
And yet many scientists will say that moral truths do not exist, simply because certain facts about human experience cannot be readily known, or may never be known.
As I hope to show, this misunderstanding has created tremendous confusion about the relationship between human knowledge and human values.

Another thing that makes the idea of moral truth difficult to discuss is that people often employ a double standard when thinking about consensus :
most people take scientific consensus to mean that scientific truths exist, and they consider scientific controversy to be merely a sign that further work remains to be done;
and yet many of these same people believe that moral controversy proves that there can be no such thing as moral truth, while moral consensus shows only that human beings often harbor the same biases.
Clearly, this double standard rigs the game against a universal conception of morality.

The deeper issue, however, is that truth has nothing, in principle, to do with consensus :
one person can be right, and everyone else can be wrong.
Consensus is a guide to discovering what is going on in the world, but that is all that it is.
Its presence or absence in no way constrains what may or may not be true.
There are surely physical, chemical, and biological facts about which we are ignorant or mistaken.
In speaking of ``moral truth,'' I am saying that there must be facts regarding human and animal well-being about which we can also be ignorant or mistaken.
In both cases, science --- and rational thought generally --- is the tool we can use to uncover these facts.

And here is where the real controversy begins, for many people strongly object to my claim that morality and values relate to facts about the well-being of conscious creatures.
My critics seem to think that consciousness holds no special place where values are concerned, or that any state of consciousness stands the same chance of being valued as any other.
The most common objection to my argument is some version of the following :

But you haven't said \textit{why} the well-being of conscious beings ought to matter to us.
If someone wants to torture all conscious beings to the point of madness, what is to say that he isn't just as ``moral'' as you are?

While I do not think anyone sincerely believes that this kind of moral skepticism makes sense, there is no shortage of people who will press this point with a ferocity that often passes for sincerity.
Let us begin with the fact of consciousness :
I think we can know, through reason alone, that consciousness is the only intelligible domain of value.
What is the alternative?
I invite you to try to think of a source of value that has absolutely nothing to do with the (actual or potential) experience of conscious beings.
Take a moment to think about what this would entail :
whatever this alternative is, it cannot affect the experience of any creature (in this life or in any other).
Put this thing in a box, and what you have in that box is --- it would seem, by \textit{definition} --- the least interesting thing in the universe.

So how much time should we spend worrying about such a transcendent source of value?
I think the time I will spend typing this sentence is already too much.
All other notions of value will bear some relationship to the actual or potential experience of conscious beings.
So my claim that consciousness is the basis of human values and morality is not an arbitrary starting point.
Now that we have consciousness on the table, my further claim is that the concept of ``well-being'' captures all that we can intelligibly value.
And ``morality'' --- whatever people's associations with this term happen to be --- \textit{really} relates to the intentions and behaviors that affect the well-being of conscious creatures.

On this point, religious conceptions of moral law are often put forward as counterexamples :
for when asked why it is important to follow God's law, many people will cannily say, ``for its own sake.''
Of course, it is possible to say this, but this seems neither an honest nor a coherent claim.
What if a more powerful God would punish us for eternity for following Yahweh's law?
Would it then make sense to follow Yahweh's law ``for its own sake''?
The inescapable fact is that religious people are as eager to find happiness and to avoid misery as anyone else :
many of them just happen to believe that the most important changes in conscious experience occur after death (i.e., in heaven or in hell).
And while Judaism is sometimes held up as an exception --- because it tends not to focus on the afterlife --- the Hebrew Bible makes it absolutely clear that Jews should follow Yahweh's law \textit{out of concern for the negative consequences of not following it}.
People who do not believe in God or an afterlife, and yet still think it important to subscribe to a religious tradition, only believe this because living this way seems to make some positive contribution to their well-being or to the well-being of others.

Religious notions of morality, therefore, are not exceptions to our common concern for well-being.
And all other philosophical efforts to describe morality in terms of duty, fairness, justice, or some other principle that is not explicitly tied to the well-being of conscious creatures, draw upon some conception of well-being in the end.

The doubts that immediately erupt on this point invariably depend upon bizarre and restrictive notions of what the term ``well-being'' might mean.
I think there is little doubt that most of what matters to the average person --- like fairness, justice, compassion, and a general awareness of terrestrial reality --- will be integral to our creating a thriving global civilization and, therefore, to the greater well-being of humanity.
And, as I have said, there may be many different ways for individuals and communities to thrive --- many peaks on the moral landscape --- so if there is real diversity in how people can be deeply fulfilled in this life, such diversity can be accounted for and honored in the context of science.
The concept of ``well-being,'' like the concept of ``health,'' is truly open for revision and discovery.
Just how fulfilled is it possible for us to be, personally and collectively?
What are the conditions --- ranging from changes in the genome to changes in economic systems --- that will produce such happiness?
We simply do not know.

But what if certain people insist that their ``values'' or ``morality'' have nothing to do with well-being?
Or, more realistically, what if their conception of well-being is so idiosyncratic and circumscribed as to be hostile, in principle, to the well-being of all others?
For instance, what if a man like Jeffrey Dahmer says, ``The only peaks on the moral landscape that interest me are ones where I get to murder young men and have sex with their corpses.''
This possibility --- the prospect of radically different moral commitments --- is at the heart of many people's doubts about moral truth.

Again, we should observe the double standard in place regarding the significance of consensus :
those who do not share our scientific goals have no influence on scientific discourse whatsoever;
but, for some reason, people who do not share our moral goals render us incapable of even speaking about moral truth.
It is, perhaps, worth remembering that there are trained ``scientists'' who are Biblical Creationists, and their ``scientific'' thinking is purposed toward interpreting the data of science to fit the Book of Genesis.
Such people claim to be doing ``science,'' of course, but real scientists are free, and indeed obligated, to point out that they are misusing the term.
Similarly, there are people who claim to be highly concerned about ``morality'' and ``human values,'' but when we see that their beliefs cause tremendous misery, nothing need prevent us from saying that they are misusing the term ``morality'' or that their values are distorted.
How have we convinced ourselves that, on the most important questions in human life, all views must count equally?

Consider the Catholic Church :
an organization which advertises itself as the greatest force for good and as the only true bulwark against evil in the universe.
Even among non-Catholics, its doctrines are widely associated with the concepts of ``morality'' and ``human values.''
However, the Vatican is an organization that excommunicates women for attempting to become priests but does not excommunicate male priests for raping children.
It excommunicates doctors who perform abortions to save a mother's life --- even if the mother is a \textit{nine-year-old girl raped by her stepfather and pregnant with twins} --- but it did not excommunicate a single member of the Third Reich for committing genocide.
Are we really obliged to consider such a diabolical inversion of priorities to be evidence of an alternative ``moral'' framework?
No.

It seems clear that the Catholic Church is as misguided in speaking about the ``moral'' peril of contraception, for instance, as it would be in speaking about the ``physics'' of Transubstantiation.
In both domains, it is true to say that the Church is grotesquely confused about which things in this world are worth paying attention to.
However, many people will continue to insist that we cannot speak about moral truth, or anchor morality to a deeper concern for well-being, because concepts like ``morality'' and ``well-being'' must be defined with reference to specific goals and other criteria --- and nothing prevents people from disagreeing about these definitions.
I might claim that morality is really about maximizing well-being and that well-being entails a wide range of psychological virtues and wholesome pleasures, but someone else will be free to say that morality depends upon worshipping the gods of the Aztecs and that well-being, if it matters at all, entails always having a terrified person locked in one's basement, waiting to be sacrificed.

Of course, goals and conceptual definitions matter.
But this holds for all phenomena and for every method we might use to study them.
My father, for instance, has been dead for twenty-five years.
What do I mean by ``dead''?
Do I mean ``dead'' with reference to specific goals?
Well, if you must, yes --- goals like respiration, energy metabolism, responsiveness to stimuli, etc.
The definition of ``life'' remains, to this day, difficult to pin down.
Does this mean we can't study life scientifically?
No.
The science of biology thrives despite such ambiguities.
Again, the concept of ``health'' is looser still :
it, too, must be defined with reference to specific goals --- not suffering chronic pain, not always vomiting, etc. --- and these goals are continually changing.
Our notion of ``health'' may one day be defined by goals that we cannot currently entertain with a straight face (like the goal of spontaneously regenerating a lost limb).
Does this mean we can't study health scientifically?

I wonder if there is anyone on Earth who would be tempted to attack the philosophical underpinnings of medicine with questions like: ``What about all the people who don't share your goal of avoiding disease and early death?
Who is to say that living a long life free of pain and debilitating illness is `healthy'?
What makes you think that you could convince a person suffering from fatal gangrene that he is not as healthy as you are?''
And yet these are precisely the kinds of objections I face when I speak about morality in terms of human and animal well-being.
Is it possible to voice such doubts in human speech?
Yes.
But that doesn't mean we should take them seriously.
One of my critics put the concern this way :
``Morals are relative to the time and place in which they appear.
If you do not already accept well-being as a value, then there seems to be no argument for why one \textit{should} promote well-being.''
As proof of this assertion, he observed that I would be unable to convince the Taliban that they value the wrong things.
By this standard, however, the truths of science are also ``relative to the time and place in which they appear,'' and there is no way to convince someone who does not value empirical evidence that he should value it.
Despite 150 years of working at it, we still can't convince a majority of Americans that evolution is a fact.
Does this mean biology isn't a proper science?

Everyone has an intuitive ``physics,'' but much of our intuitive physics is wrong (with respect to the goal of describing the behavior of matter).
Only physicists have a deep understanding of the laws that govern the behavior of matter in our universe.
I am arguing that everyone also has an intuitive ``morality,'' but much of our intuitive morality is clearly wrong (with respect to the goal of maximizing personal and collective well-being).
And only genuine moral experts would have a deep understanding of the causes and conditions of human and animal well-being.
Yes, we must have a goal to define what counts as ``right'' or ``wrong'' when speaking about physics or morality, but this criterion visits us equally in both domains.
And yes, I think it is quite clear that members of the Taliban are seeking well-being in this world (as well as hoping for it in the next).
But their religious beliefs have led them to create a culture that is almost perfectly hostile to human flourishing.
Whatever they \textit{think} they want out of life --- like keeping all women and girls subjugated and illiterate --- they simply do not understand how much better life would be for them if they had different priorities.

Science cannot tell us why, \textit{scientifically}, we should value health.
But once we admit that health is the proper concern of medicine, we can then study and promote it through science.
Medicine can resolve specific questions about human health --- and it can do this even while the very definition of ``health'' continues to change.
Indeed, the science of medicine can make marvelous progress without knowing how much its own progress will alter our conception of health in the future.

I think our concern for well-being is even less in need of justification than our concern for health is --- as health is merely one of its many facets.
And once we begin thinking seriously about human well-being, we will find that science can resolve specific questions about morality and human values, even while our conception of ``well-being'' evolves.

It is essential to see that the demand for \textit{radical} justification leveled by the moral skeptic could not be met by any branch of science.
Science is defined with reference to the goal of understanding the processes at work in the universe.
Can we justify this goal scientifically?
Of course not.
Does this make science itself unscientific?
If so, we appear to have pulled ourselves \textit{down} by our bootstraps.

It would be impossible to prove that our definition of science is correct, because our standards of proof will be built into any proof we would offer.
What evidence could prove that we should value evidence?
What logic could demonstrate the importance of logic?
We might observe that standard science is better at predicting the behavior of matter than Creationist ``science'' is.
But what could we say to a ``scientist'' whose only goal is to authenticate the Word of God?
Here, we seem to reach an impasse.
And yet, no one thinks that the failure of standard science to silence all possible dissent has any significance whatsoever;
why should we demand more of a science of morality?
Many moral skeptics piously cite Hume's is/ought distinction as though it were well known to be the last word on the subject of morality until the end of the world.
They insist that notions of what we ought to do or value can be justified only in terms of other ``oughts,'' never in terms of facts about the way the world is.
After all, in a world of physics and chemistry, how could things like moral obligations or values really exist?
How could it be objectively true, for instance, that we ought to be kind to children?

But this notion of ``ought'' is an artificial and needlessly confusing way to think about moral choice.
In fact, it seems to be another dismal product of Abrahamic religion --- which, strangely enough, now constrains the thinking of even atheists.
If this notion of ``ought'' means anything we can possibly care about, it must translate into a concern about the actual or potential experience of conscious beings (either in this life or in some other).
For instance, to say that we ought to treat children with kindness seems identical to saying that everyone will tend to be better off if we do.
The person who claims that he does not want to be better off is either wrong about what he does, in fact, want (i.e., he doesn't know what he's missing), or he is lying, or he is not making sense.
The person who insists that he is committed to treating children with kindness for reasons that have nothing to do with anyone's well-being is also not making sense.
It is worth noting in this context that the God of Abraham never told us to treat children with kindness, but He did tell us to kill them for talking back to us (Exodus 21:15, Leviticus 20:9, Deuteronomy 21:18–21, Mark 7:9–13, and Matthew 15:4–7).
And yet everyone finds this ``moral'' imperative perfectly insane.
Which is to say that no one --- not even fundamentalist Christians and orthodox Jews --- can so fully ignore the link between morality and human well-being as to be truly bound by God's law.

\subsection{The Worst Possible Misery for Everyone}

I have argued that values only exist relative to actual and potential changes in the well-being of conscious creatures.
However, as I have said, many people seem to have strange associations with the concept of ``well-being'' --- imagining that it must be at odds with principles like justice, autonomy, fairness, scientific curiosity, etc., when it simply isn't.
They also worry that the concept of “well-being” is poorly defined.
Again, I have indicated why I do not think this is a problem (just as it's not a problem with concepts like ``life'' and ``health'').
However, it is also useful to notice that a universal morality can be defined with reference to the negative end of the spectrum of conscious experience : I
refer to this extreme as ``the worst possible misery for everyone.''

Even if each conscious being has a unique nadir on the moral landscape, we can still conceive of a state of the universe in which everyone suffers as much as he or she (or it) possibly can.
If you think we cannot say this would be ``bad,'' then I don't know what you could mean by the word ``bad'' (and I don't think you know what you mean by it either).
Once we conceive of ``the worst possible misery for everyone,'' then we can talk about taking incremental steps toward this abyss :
What could it mean for life on Earth to get worse for all human beings simultaneously?
Notice that this need have nothing to do with people enforcing their culturally conditioned moral precepts.
Perhaps a neurotoxic dust could fall to Earth from space and make everyone extremely uncomfortable.
All we need imagine is a scenario in which everyone loses a little, or a lot, without there being compensatory gains (i.e., no one learns any important lessons, no one profits from others' losses, etc.).
It seems uncontroversial to say that a change that leaves everyone worse off, by any rational standard, can be reasonably called ``bad,'' if this word is to have any meaning at all.

We simply must stand somewhere. I am arguing that, in the moral sphere, it is safe to begin with the premise that it is good to avoid behaving in such a way as to produce the worst possible misery for everyone.
I am not claiming that most of us personally care about the experience of all conscious beings;
I am saying that a universe in which all conscious beings suffer the worst possible misery is worse than a universe in which they experience well-being.
This is all we need to speak about ``moral truth'' in the context of science.
Once we admit that the extremes of absolute misery and absolute flourishing --- whatever these states amount to for each particular being in the end --- are different and dependent on facts about the universe, then we have admitted that there are right and wrong answers to questions of morality.

Granted, genuine ethical difficulties arise when we ask questions like, ``How much should I care about other people's children?
How much should I be willing to sacrifice, or demand that my own children sacrifice, in order to help other people in need?''
We are not, by nature, impartial --- and much of our moral reasoning must be applied to situations in which there is tension between our concern for ourselves, or for those closest to us, and our sense that it would be better to be more committed to helping others.
And yet ``better'' must still refer, in this context, to positive changes in the experience of sentient creatures.

Imagine if there were only two people living on Earth :
we can call them Adam and Eve.
Clearly, we can ask how these two people might maximize their well-being.
Are there wrong answers to this question?
Of course.
(Wrong answer number 1: smash each other in the face with a large rock.)
And while there are ways for their personal interests to be in conflict, most solutions to the problem of how two people can thrive on Earth will not be zero-sum.
Surely the best solutions will not be zero-sum.
Yes, both of these people could be blind to the deeper possibilities of collaboration :
each might attempt to kill and eat the other, for instance.
Would they be wrong to behave this way?
Yes, if by ``wrong'' we mean that they would be forsaking far deeper and more durable sources of satisfaction.
It seems uncontroversial to say that a man and woman alone on Earth would be better off if they recognized their common interests --- like getting food, building shelter, and defending themselves against larger predators.
If Adam and Eve were industrious enough, they might realize the benefits of exploring the world, begetting future generations of humanity, and creating technology, art, and medicine.
Are there good and bad paths to take across this landscape of possibilities?
Of course. In fact, there are, by definition, paths that lead to the worst misery and paths that lead to the greatest fulfillment possible for these two people --- given the structure of their respective brains, the immediate facts of their environment, and the laws of Nature.
The underlying facts here are the facts of physics, chemistry, and biology as they bear on the experience of the only two people in existence.
Unless the human mind is fully separable from the principles of physics, chemistry, and biology, any fact about Adam and Eve's subjective experience (morally salient or not) is a fact about (part of) the universe.

In talking about the causes of Adam and Eve's first-person experience, we are talking about an extraordinarily complex interplay between brain states and environmental stimuli.
However complex these processes are, it is clearly possible to understand them to a greater or lesser degree (i.e., there are right and wrong answers to questions about Adam's and Eve's well-being).
Even if there are a thousand different ways for these two people to thrive, there will be many ways for them not to thrive --- and the differences between luxuriating on a peak of well-being and languishing in a valley of internecine horror will translate into facts that can be scientifically understood.
Why would the difference between right and wrong answers suddenly disappear once we add 6.7 billion more people to this experiment?

Grounding our values in a continuum of conscious states --- one that has the worst possible misery for everyone at its depths and differing degrees of well-being at all other points --- seems like the only legitimate context in which to conceive of values and moral norms.
Of course, anyone who has an alternative set of moral axioms is free to put them forward, just as they are free to define ``science'' any way they want.
But some definitions will be useless, or worse --- and many current definitions of ``morality'' are so bad that we can know, far in advance of any breakthrough in the sciences of mind, that they have no place in a serious conversation about how we should live in this world.
The Knights of the Ku Klux Klan have nothing meaningful to say about particle physics, cell physiology, epidemiology, linguistics, economic policy, etc.
How is their ignorance any less obvious on the subject of human well-being?

The moment we admit that consciousness is the context in which any discussion of values makes sense, we must admit that there are facts to be known about how the experience of conscious creatures can change.
Human and animal well-being are natural phenomena.
As such, they can be studied, in principle, with the tools of science and spoken about with greater or lesser precision.
Do pigs suffer more than cows do when being led to slaughter?
Would humanity suffer more or less, on balance, if the United States unilaterally gave up all its nuclear weapons?
Questions like these are very difficult to answer.
But this does not mean that they don't have answers.

The fact that it could be difficult or impossible to know exactly how to maximize human well-being does not mean that there are no right or wrong ways to do this --- nor does it mean that we cannot exclude certain answers as obviously bad.
For instance, there is often a tension between the autonomy of the individual and the common good, and many moral problems turn on just how to prioritize these competing values.
However, autonomy brings obvious benefit to people and is, therefore, an important component of the common good.
The fact that it might be difficult to decide exactly how to balance individual rights against collective interests, or that there might be a thousand equivalent ways of doing this, does not mean that there aren't objectively terrible ways of doing this.
The difficulty of getting precise answers to certain moral questions does not mean that we must hesitate to condemn the morality of the Taliban --- not just personally, but from the point of view of science.
The moment we admit that we know anything about human well-being scientifically, we must admit that certain individuals or cultures can be absolutely wrong about it.

\subsection{Moral Blindness in the Name of ``Tolerance''}

There are very practical concerns that follow from the glib idea that anyone is free to value anything --- the most consequential being that it is precisely what allows highly educated, secular, and otherwise well-intentioned people to pause thoughtfully, and often interminably, before condemning practices like compulsory veiling, genital excision, bride burning, forced marriage, and other cheerful products of alternative ``morality'' found elsewhere in the world.
Fanciers of Hume's is/ought distinction never seem to realize what the stakes are, and they do not see how abject failures of compassion are enabled by this intellectual ``tolerance'' of moral difference.
While much of the debate on these issues must be had in academic terms, this is not merely an academic debate.
There are girls getting their faces burned off with acid at this moment for daring to learn to read, or for not consenting to marry men they have never met, or even for the ``crime'' of getting raped.
The amazing thing is that some Western intellectuals won't even blink when asked to defend these practices on philosophical grounds.
I once spoke at an academic conference on themes similar to those discussed here.
Near the end of my lecture, I made what I thought would be a quite incontestable assertion :
We already have good reason to believe that certain cultures are less suited to maximizing well-being than others.
I cited the ruthless misogyny and religious bamboozlement of the Taliban as an example of a worldview that seems less than perfectly conducive to human flourishing.

As it turns out, to denigrate the Taliban at a scientific meeting is to court controversy.
At the conclusion of my talk, I fell into debate with another invited speaker, who seemed, at first glance, to be very well positioned to reason effectively about the implications of science for our understanding of morality.
In fact, this person has since been appointed to the President's Commission for the Study of Bioethical Issues and is now one of only thirteen people who will advise President Obama on ``issues that may emerge from advances in biomedicine and related areas of science and technology'' in order to ensure that ``scientific research, health care delivery, and technological innovation are conducted in an ethically responsible manner.''
Here is a snippet of our conversation, more or less verbatim :

She:
What makes you think that science will ever be able to say that forcing women to wear burqas is wrong?

Me:
Because I think that right and wrong are a matter of increasing or decreasing well-being --- and it is obvious that forcing half the population to live in cloth bags, and beating or killing them if they refuse, is not a good strategy for maximizing human well-being.

She:
But that's only your opinion.

Me:
Okay \dots Let's make it even simpler.
What if we found a culture that ritually blinded every third child by literally plucking out his or her eyes at birth, would you then agree that we had found a culture that was needlessly diminishing human well-being?

She:
It would depend on why they were doing it.

Me [slowly returning my eyebrows from the back of my head]:
Let's say they were doing it on the basis of religious superstition.
In their scripture, God says, ``Every third must walk in darkness.''

She:
Then you could never say that they were wrong.

Such opinions are not uncommon in the Ivory Tower.
I was talking to a woman (it's hard not to feel that her gender makes her views all the more disconcerting) who had just delivered an entirely lucid lecture on some of the moral implications of recent advances in neuroscience.
She was concerned that our intelligence services might one day use neuroimaging technology for the purposes of lie detection, which she considered a likely violation of cognitive liberty.
She was especially exercised over rumors that our government might have exposed captured terrorists to aerosols containing the hormone oxytocin in an effort to make them more cooperative.
Though she did not say it, I suspect that she would even have opposed subjecting these prisoners to the smell of freshly baked bread, which has been shown to have a similar effect.
While listening to her talk, as yet unaware of her liberal views on compulsory veiling and ritual enucleation, I thought her slightly overcautious, but a basically sane and eloquent authority on scientific ethics.
I confess that once we did speak, and I peered into the terrible gulf that separated us on these issues, I found that I could not utter another word to her.
In fact, our conversation ended with my blindly enacting two neurological cliches :
my jaw quite literally dropped open, and I spun on my heels before walking away.

While human beings have different moral codes, each competing view presumes its own universality.
This seems to be true even of moral relativism.
While few philosophers have ever answered to the name of ``moral relativist,'' it is by no means uncommon to find local eruptions of this view whenever scientists and other academics encounter moral diversity.
Forcing women and girls to wear burqas may be wrong in Boston or Palo Alto, so the argument will run, but we cannot say that it is wrong for Muslims in Kabul.
To demand that the proud denizens of an ancient culture conform to our view of gender equality would be culturally imperialistic and philosophically naive.
This is a surprisingly common view, especially among anthropologists.

Moral relativism, however, tends to be self-contradictory.
Relativists may say that moral truths exist only relative to a specific cultural framework --- but this claim about the status of moral truth purports to be true across all possible frameworks.
In practice, relativism almost always amounts to the claim that we should be tolerant of moral difference because no moral truth can supersede any other.
And yet this commitment to tolerance is not put forward as simply one relative preference among others deemed equally valid.
Rather, tolerance is held to be more in line with the (universal) truth about morality than intolerance is.
The contradiction here is unsurprising.
Given how deeply disposed we are to make universal moral claims, I think one can reasonably doubt whether any consistent moral relativist has ever existed.

Moral relativism is clearly an attempt to pay intellectual reparations for the crimes of Western colonialism, ethnocentrism, and racism.
This is, I think, the only charitable thing to be said about it.
I hope it is clear that I am not defending the idiosyncrasies of the West as any more enlightened, in principle, than those of any other culture.
Rather, I am arguing that the most basic facts about human flourishing must transcend culture, just as most other facts do.
And if there are facts that are truly a matter of cultural construction --- if, for instance, learning a specific language or tattooing your face fundamentally alters the possibilities of human experience --- well, then these facts also arise from (neurophysiological) processes that transcend culture.

In his wonderful book The \textit{Blank Slate}, Steven Pinker includes a quotation from the anthropologist Donald Symons that captures the problem of multiculturalism especially well :

If only one person in the world held down a terrified, struggling, screaming little girl, cut off her genitals with a septic blade, and sewed her back up, leaving only a tiny hole for urine and menstrual flow, the only question would be how severely that person should be punished, and whether the death penalty would be a sufficiently severe sanction.
But when millions of people do this, instead of the enormity being magnified millions-fold, suddenly it becomes ``culture,'' and thereby magically becomes less, rather than more, horrible, and is even defended by some Western ``moral thinkers,'' including feminists.

It is precisely such instances of learned confusion (one is tempted to say ``learned psychopathy'') that lend credence to the claim that a universal morality requires the support of faith-based religion.
The categorical distinction between facts and values has opened a sinkhole beneath secular liberalism --- leading to moral relativism and masochistic depths of political correctness.
Think of the champions of ``tolerance'' who reflexively blamed Salman Rushdie for his fatwa, or Ayaan Hirsi Ali for her ongoing security concerns, or the Danish cartoonists for their ``controversy,'' and you will understand what happens when educated liberals think there is no universal foundation for human values.
Among conservatives in the West, the same skepticism about the power of reason leads, more often than not, directly to the feet of Jesus Christ, Savior of the Universe.
The purpose of this book is to help cut a third path through this wilderness.

\subsection{Moral Science}

Charges of ``scientism'' cannot be long in coming.
No doubt, there are still some people who will reject any description of human nature that was not first communicated in iambic pentameter.
Many readers may also fear that the case I am making is vaguely, or even explicitly, utopian.
It isn't, as should become clear in due course.

However, other doubts about the authority of science are even more fundamental.
There are academics who have built entire careers on the allegation that the foundations of science are rotten with bias --- sexist, racist, imperialist, Northern, etc.
Sandra Harding, a feminist philosopher of science, is probably the most famous proponent of this view.
On her account, these prejudices have driven science into an epistemological cul-de-sac called ``weak objectivity.''
To remedy this dire situation, Harding recommends that scientists immediately give ``feminist'' and ``multicultural'' epistemologies their due.

First, let's be careful not to confuse this quite crazy claim for its sane cousin :
There is no question that scientists have occasionally demonstrated sexist and racist biases.
The composition of some branches of science is still disproportionately white and male (though some are now disproportionately female), and one can reasonably wonder whether bias is the cause.
There are also legitimate questions to be asked about the direction and application of science :
in medicine, for instance, it seems clear that women's health issues have been sometimes neglected because the prototypical human being has been considered male.
One can also argue that the contributions of women and minority groups to science have occasionally been ignored or undervalued :
the case of Rosalind Franklin standing in the shadows of Crick and Watson might be an example of this.
But none of these facts, alone or in combination, or however multiplied, remotely suggests that our notions of scientific objectivity are vitiated by racism or sexism.

Is there really such a thing as a feminist or multicultural epistemology?
Harding's case is not helped when she finally divulges that there is not just one feminist epistemology, but many.
On this view, why was Hitler's notion of ``Jewish physics'' (or Stalin's idea of ``capitalist biology'') anything less than a thrilling insight into the richness of epistemology?
Should we now consider the possibility of not only Jewish physics, but of Jewish women's physics?
How could such a balkanization of science be a step toward ``strong objectivity''?
And if political inclusiveness is our primary concern, where could such efforts to broaden our conception of scientific truth possibly end?
Physicists tend to have an unusual aptitude for complex mathematics, and anyone who doesn't cannot expect to make much of a contribution to the field.
Why not remedy this situation as well?
Why not create an epistemology for physicists who failed calculus?
Why not be bolder still and establish a branch of physics for people suffering from debilitating brain injuries?
Who could reasonably expect that such efforts at inclusiveness would increase our understanding of a phenomenon like gravity?
As Steven Weinberg once said regarding similar doubts about the objectivity of science, ``You have to be very learned to be that wrong.''
Indeed, one does --- and many are.

There is no denying, however, that the effort to reduce all human values to biology can produce howlers.
For instance, when the entomologist E. O. Wilson (in collaboration with the philosopher Michael Ruse) wrote that ``morality, or more strictly our belief in morality, is merely an adaptation put in place to further our reproductive ends,'' the philosopher Daniel Dennett rightly dismissed it as ``nonsense.''
The fact that our moral intuitions probably conferred some adaptive advantage upon our ancestors does not mean that the present purpose of morality is successful reproduction, or that ``our belief in morality'' is just a useful delusion.
(Is the purpose of astronomy successful reproduction?
What about the practice of contraception?
Is that all about reproduction, too?)
Nor does it mean that our notion of ``morality'' cannot grow deeper and more refined as our understanding of ourselves develops.

Many universal features of human life need not have been selected for at all;
they may simply be, as Dennett says, ``good tricks'' communicated by culture or ``forced moves'' that naturally emerge out of the regularities in our world.
As Dennett says, it is doubtful that there is a gene for knowing that you should throw a spear ``pointy end first.''
And it is, likewise, doubtful that our ancestors had to spend much time imparting this knowledge to each successive generation.
We have good reason to believe that much of what we do in the name of ``morality'' --- decrying sexual infidelity, punishing cheaters, valuing cooperation, etc. --- is borne of unconscious processes that were shaped by natural selection.

But this does not mean that evolution designed us to lead deeply fulfilling lives.
Again, in talking about a science of morality, I am not referring to an evolutionary account of all the cognitive and emotional processes that govern what people do when they say they are being ``moral'';
I am referring to the totality of scientific facts that govern the range of conscious experiences that are possible for us.
To say that there are truths about morality and human values is simply to say that there are facts about well-being that await our discovery --- regardless of our evolutionary history.
While such facts necessarily relate to the experience of conscious beings, they cannot be the mere invention of any person or culture.

It seems to me, therefore, that there are at least three projects that we should not confuse:

\begin{enumerate}
      \item We can explain why people tend to follow certain patterns of thought and behavior (many of them demonstrably silly and harmful) in the name of ``morality.''
      \item We can think more clearly about the nature of moral truth and determine which patterns of thought and behavior we should follow in the name of ``morality.''
      \item We can convince people who are committed to silly and harmful patterns of thought and behavior in the name of ``morality'' to break these commitments and to live better lives.
\end{enumerate}

These are distinct and independently worthy endeavors.
Most scientists who study morality in evolutionary, psychological, or neurobiological terms are exclusively devoted to the first project :
their goal is to describe and understand how people think and behave in light of morally salient emotions like anger, disgust, empathy, love, guilt, humiliation, etc.
This research is fascinating, of course, but it is not my focus.
And while our common evolutionary origins and resultant physiological similarity to one another suggest that human well-being will admit of general principles that can be scientifically understood, I consider this first project all but irrelevant to projects 2 and 3.
In the past, I have found myself in conflict with some of the leaders in this field because many of them, like the psychologist Jonathan Haidt, believe that this first project represents the only legitimate point of contact between science and morality.

I happen to believe that the third project --- changing people's ethical commitments --- is the most important task facing humanity in the twenty-first century.
Nearly every other important goal --- from combating climate change, to fighting terrorism, to curing cancer, to saving the whales --- falls within its purview.
Of course, moral persuasion is a difficult business, but it strikes me as especially difficult if we haven't figured out in what sense moral truths exist.
Hence, my main focus is on project 2.
To see the difference between these three projects, it is best to consider specific examples :
we can, for instance, give a plausible evolutionary account of why human societies have tended to treat women as the property of men (1);
it is, however, quite another thing to give a scientific account of whether, why, and to what degree human societies change for the better when they outgrow this tendency (2);
it is yet another thing altogether to decide how best to change people's attitudes at this moment in history and to empower women on a global scale (3).
It is easy to see why the study of the evolutionary origins of ``morality'' might lead to the conclusion that morality has nothing at all to do with Truth.
If morality is simply an adaptive means of organizing human social behavior and mitigating conflict, there would be no reason to think that our current sense of right and wrong would reflect any deeper understanding about the nature of reality.
Hence, a narrow focus explaining why people think and behave as they do can lead a person to find the idea of ``moral truth'' literally unintelligible.
But notice that the first two projects give quite different accounts of how ``morality'' fits into the natural world.
In 1, ``morality'' is the collection of impulses and behaviors (along with their cultural expressions and neurobiological underpinnings) that have been hammered into us by evolution.
In 2, ``morality'' refers to the impulses and behaviors we can follow so as to maximize our well-being in the future.
To give a concrete example :
Imagine that a handsome stranger tries to seduce another man's wife at the gym.
When the woman politely informs her admirer that she is married, the cad persists, as though a happy marriage could be no impediment to his charms.
The woman breaks off the conversation soon thereafter, but far less abruptly than might have been compatible with the laws of physics.

I write now, in the rude glare of recent experience.
I can say that when my wife reported these events to me yesterday, they immediately struck me as morally salient.
In fact, she had not completed her third sentence before the dark fluids of moral indignation began coursing through my brain --- jealousy, embarrassment, anger, etc. --- albeit only at a trickle.
First, I was annoyed by the man's behavior --- and had I been present to witness it, I suspect that my annoyance would have been far greater.
If this Don Juan had been as dismissive of me in my presence as he was in my absence, I could imagine how such an encounter could result in physical violence.
No evolutionary psychologist would find it difficult to account for my response to this situation --- and almost all scientists who study ``morality'' would confine their attention to this set of facts :
my inner ape had swung into view, and any thoughts I might entertain about ``moral truth'' would be linguistic effluvium masking far more zoological concerns.
I am the product of an evolutionary history in which every male of the species has had to guard against squandering his resources on another man's offspring.
Had we scanned my brain and correlated my subjective feelings with changes in my neurophysiology, the scientific description of these events would be nearly complete.
So ends project 1.
But there are many different ways for an ape to respond to the fact that other apes find his wife desirable.
Had this happened in a traditional honor culture, the jealous husband might beat his wife, drag her to the gym, and force her to identify her suitor so that he could put a bullet in his brain.
In fact, in an honor society, the employees of the gym might sympathize with this project and help to organize a proper duel.
Or perhaps the husband would be satisfied to act more obliquely, killing one of his rival's relatives and initiating a classic blood feud.
In either case, assuming he didn't get himself killed in the process, he might then murder his wife for emphasis, leaving his children motherless.
There are many communities on Earth where men commonly behave this way, and hundreds of millions of boys are beginning to run this ancient software on their brains even now.

However, my own mind shows some precarious traces of civilization :
one being that I view the emotion of jealously with suspicion.
What is more, I happen to love my wife and genuinely want her to be happy, and this entails a certain empathetic understanding of her point of view.
Given a moment to think about it, I can feel glad that her self-esteem received a boost from this man's attention;
I can also feel compassion for the fact that, after recently having our first child, her self-esteem needed any boost at all.
I also know that she would not want to be rude, and that this probably made her somewhat slow to extricate herself from a conversation that had taken a wrong turn.
And I am under no illusions that I am the only man on Earth whom she will find attractive, or momentarily distracting, nor do I imagine that her devotion to me should consist in this impossible narrowing of her focus.
And how do I feel about the man?
Well, I still find his behavior objectionable --- because I cannot sympathize with his effort to break up a marriage, and I know that I would not behave as he did --- but I sympathize with everything else he must have felt, because I also happen to think that my wife is beautiful, and I know what it's like to be a single ape in the jungle.
Most important, however, I value my own well-being, as well as that of my wife and daughter, and I want to live in a society that maximizes the possibility of human well-being generally.
Here begins project 2 :
Are there right and wrong answers to the question of how to maximize well-being?
How would my life have been affected if I had killed my wife in response to this episode?
We do not need a completed neuroscience to know that my happiness, as well as that of many other people, would have been profoundly diminished.
And what about the collective well-being of people in an honor society that might support such behavior?
It seems to me that members of these societies are obviously worse off.
If I am wrong about this, however, and there are ways to organize an honor culture that allow for precisely the same level of human flourishing enjoyed elsewhere --- then so be it.
This would represent another peak on the moral landscape.
Again, the existence of multiple peaks would not render the truths of morality merely subjective.

The framework of a moral landscape guarantees that many people will have flawed conceptions of morality, just as many people have flawed conceptions of physics.
Some people think ``physics'' includes (or validates) practices like astrology, voodoo, and homeopathy.
These people are, by all appearances, simply wrong about physics.
In the United States, a majority of people (57 percent) believe that preventing homosexuals from marrying is a ``moral'' imperative.
However, if this belief rests on a flawed sense of how we can maximize our well-being, such people may simply be wrong about morality.
And the fact that millions of people use the term ``morality'' as a synonym for religious dogmatism, racism, sexism, or other failures of insight and compassion should not oblige us to merely accept their terminology until the end of time.

What will it mean for us to acquire a deep, consistent, and fully scientific understanding of the human mind?
While many of the details remain unclear, the challenge is for us to begin speaking sensibly about right and wrong, and good and evil, given what we already know about our world.
Such a conversation seems bound to shape our morality and public policy in the years to come.

\newpage
\section{Chapter 2 : Good And Evil}

There may be nothing more important than human cooperation.
Whenever more pressing concerns seem to arise --- like the threat of a deadly pandemic, an asteroid impact, or some other global catastrophe --- human cooperation is the only remedy (if a remedy exists).
Cooperation is the stuff of which meaningful human lives and viable societies are made.
Consequently, few topics will be more relevant to a maturing science of human well-being.

Open a newspaper, today or any day for the rest of your life, and you will witness failures of human cooperation, great and small, announced from every corner of the world.
The results of these failures are no less tragic for being utterly commonplace :
deception, theft, violence, and their associated miseries arise in a continuous flux of misspent human energy.
When one considers the proportion of our limited time and resources that must be squandered merely to guard against theft and violence (to say nothing of addressing their effects), the problem of human cooperation seems almost the only problem worth thinking about.
``Ethics'' and ``morality'' (I use these terms interchangeably) are the names we give to our deliberate thinking on these matters.
Clearly, few subjects have greater bearing upon the question of human well-being.

As we better understand the brain, we will increasingly understand all of the forces --- kindness, reciprocity, trust, openness to argument, respect for evidence, intuitions of fairness, impulse control, the mitigation of aggression, etc. --- that allow friends and strangers to collaborate successfully on the common projects of civilization.
Understanding ourselves in this way, and using this knowledge to improve human life, will be among the most important challenges to science in the decades to come.

Many people imagine that the theory of evolution entails selfishness as a biological imperative.
This popular misconception has been very harmful to the reputation of science.
In truth, human cooperation and its attendant moral emotions are fully compatible with biological evolution.
Selection pressure at the level of ``selfish'' genes would surely incline creatures like ourselves to make sacrifices for our relatives, for the simple reason that one's relatives can be counted on to share one's genes :
while this truth might not be obvious through introspection, your brother's or sister's reproductive success is, in part, your own.
This phenomenon, known as kin selection, was not given a formal analysis until the 1960s in the work of William Hamilton, but it was at least implicit in the understanding of earlier biologists.
Legend has it that J. B. S. Haldane was once asked if he would risk his life to save a drowning brother, to which he quipped, ``No, but I would save two brothers or eight cousins.''

The work of evolutionary biologist Robert Trivers on reciprocal altruism has gone a long way toward explaining cooperation among unrelated friends and strangers.
Trivers's model incorporates many of the psychological and social factors related to altruism and reciprocity, including :
friendship, moralistic aggression (i.e., the punishment of cheaters), guilt, sympathy, and gratitude, along with a tendency to deceive others by mimicking these states.
As first suggested by Darwin, and recently elaborated by the psychologist Geoffrey Miller, sexual selection may have further encouraged the development of moral behavior.
Because moral virtue is attractive to both sexes, it might function as a kind of peacock's tail:
costly to produce and maintain, but beneficial to one's genes in the end.

Clearly, our selfish and selfless interests do not always conflict.
In fact, the well-being of others, especially those closest to us, is one of our primary (and, indeed, most selfish) interests.
While much remains to be understood about the biology of our moral impulses, kin selection, reciprocal altruism, and sexual selection explain how we have evolved to be, not merely atomized selves in thrall to our self-interest, but social selves disposed to serve a common interest with others.

Certain biological traits appear to have been shaped by, and to have further enhanced, the human capacity for cooperation.
For instance, unlike the rest of the Earth's creatures, including our fellow primates, the sclera of our eyes (the region surrounding the colored iris) is white and exposed.
This makes the direction of the human gaze very easy to detect, allowing us to notice even the subtlest shifts in one another's visual attention.
The psychologist Michael Tomasello suggests the following adaptive logic :

If I am, in effect, advertising the direction of my eyes, I must be in a social environment full of others who are not often inclined to take advantage of this to my detriment --- by, say, beating me to the food or escaping aggression before me.
Indeed, I must be in a cooperative social environment in which others following the direction of my eyes somehow benefits me.

Tomasello has found that even twelve-month old children will follow a person's gaze, while chimpanzees tend to be interested only in head movements.
He suggests that our unique sensitivity to gaze direction facilitated human cooperation and language development.

While each of us is selfish, we are not merely so.
Our own happiness requires that we extend the circle of our self-interest to others --- to family, friends, and even to perfect strangers whose pleasures and pains matter to us.
While few thinkers have placed greater focus on the role that competing self-interests play in society, even Adam Smith recognized that each of us cares deeply about the happiness of others.
He also recognized, however, that our ability to care about others has its limits and that these limits are themselves the object of our personal and collective concern :

Let us suppose that the great empire of China, with all its myriads of inhabitants, was suddenly swallowed up by an Earthquake, and let us consider how a man of humanity in Europe, who had no sort of connection with that part of the world, would be affected upon receiving intelligence of this dreadful calamity.
He would, I imagine, first of all, express very strongly his sorrow for the misfortune of that unhappy people, he would make many melancholy reflections upon the precariousness of human life, and the vanity of all the labours of man, which could thus be annihilated in a moment.
He would too, perhaps, if he was a man of speculation, enter into many reasonings concerning the effects which this disaster might produce upon the commerce of Europe, and the trade and business of the world in general.
And when all this fine philosophy was over, when all these humane sentiments had been once fairly expressed, he would pursue his business or his pleasure, take his repose or his diversion, with the same ease and tranquility, as if no such accident had happened.
The most frivolous disaster which could befall himself would occasion a more real disturbance.
If he was to lose his little finger tomorrow, he would not sleep tonight;
but, provided he never saw them, he will snore with the most profound security over the ruin of a hundred millions of his brethren, and the destruction of that immense multitude seems plainly an object less interesting to him, than this paltry misfortune of his own.
To prevent, therefore, this paltry misfortune to himself, would a man of humanity be willing to sacrifice the lives of a hundred millions of his brethren, provided he had never seen them?
Human nature startles with horror at the thought, and the world, in its greatest depravity and corruption, never produced such a villain as could be capable of entertaining it.
But what makes this difference?

Smith captures the tension between our reflexive selfishness and our broader moral intuitions about as well as anyone can here.
The truth about us is plain to see :
most of us are powerfully absorbed by selfish desires almost every moment of our lives;
our attention to our own pains and pleasures could scarcely be more acute;
only the most piercing cries of anonymous suffering capture our interest, and then fleetingly.
And yet, when we consciously reflect on what we should do, an angel of beneficence and impartiality seems to spread its wings within us :
we genuinely want fair and just societies;
we want others to have their hopes realized;
we want to leave the world better than we found it.

Questions of human well-being run deeper than any explicit code of morality.
Morality --- in terms of consciously held precepts, social contracts, notions of justice, etc. --- is a relatively recent development.
Such conventions require, at a minimum, complex language and a willingness to cooperate with strangers, and this takes us a stride or two beyond the Hobbesian ``state of nature.''
However, any biological changes that served to mitigate the internecine misery of our ancestors would fall within the scope of an analysis of morality as a guide to personal and collective well-being.
To simplify matters enormously :

\begin{enumerate}
      \item Genetic changes in the brain gave rise to social emotions, moral intuitions, and language \dots
      \item These allowed for increasingly complex cooperative behavior, the keeping of promises, concern about one's reputation, etc \dots
      \item Which became the basis for cultural norms, laws, and social institutions whose purpose has been to render this growing system of cooperation durable in the face of countervailing forces.
\end{enumerate}

Some version of this progression has occurred in our case, and each step represents an undeniable enhancement of our personal and collective well-being.
To be sure, catastrophic regressions are always possible.
We could, either by design or negligence, employ the hard-won fruits of civilization, and the emotional and social leverage wrought of millennia of biological and cultural evolution, to immiserate ourselves more fully than unaided Nature ever could.
Imagine a global North Korea, where the better part of a starving humanity serve as slaves to a lunatic with bouffant hair :
this might be worse than a world filled merely with warring australopithecines.
What would ``worse'' mean in this context?
Just what our intuitions suggest :
more painful, less satisfying, more conducive to terror and despair, and so on.
While it may never be feasible to compare such counterfactual states of the world, this does not mean that there are no experiential truths to be compared.
Once again, there is a difference between \textit{answers in practice and answers in principle}.

The moment one begins thinking about morality in terms of well-being, it becomes remarkably easy to discern a moral hierarchy across human societies.
Consider the following account of the Dobu islanders from Ruth Benedict :

Life in Dobu fosters extreme forms of animosity and malignancy which most societies have minimized by their institutions.
Dobuan institutions, on the other hand, exalt them to the highest degree.
The Dobuan lives out without repression man's worst nightmares of the ill-will of the universe, and according to his view of life virtue consists in selecting a victim upon whom he can vent the malignancy he attributes alike to human society and to the powers of nature.
All existence appears to him as a cutthroat struggle in which deadly antagonists are pitted against one another in contest for each one of the goods of life.
Suspicion and cruelty are his trusted weapons in the strife and he gives no mercy, as he asks none.
The Dobu appear to have been as blind to the possibility of true cooperation as they were to the truths of modern science.
While innumerable things would have been worthy of their attention --- the Dobu were, after all, extremely poor and mightily ignorant --- their main preoccupation seems to have been malicious sorcery.
Every Dobuan's primary interest was to cast spells on other members of the tribe in an effort to sicken or kill them and in the hopes of magically appropriating their crops.
The relevant spells were generally passed down from a maternal uncle and became every Dobuan's most important possessions.
Needless to say, those who received no such inheritance were believed to be at a terrible disadvantage.
Spells could be purchased, however, and the economic life of the Dobu was almost entirely devoted to trade in these fantastical commodities.

Certain members of the tribe were understood to have a monopoly over both the causes and cures for specific illnesses.
Such people were greatly feared and ceaselessly propitiated.
In fact, the conscious application of magic was believed necessary for the most mundane tasks.
Even the work of gravity had to be supplemented by relentless wizardry :
absent the right spell, a man's vegetables were expected to rise out of the soil and vanish under their own power.

To make matters worse, the Dobu imagined that good fortune conformed to a rigid law of thermodynamics :
if one man succeeded in growing more yams than his neighbor, his surplus crop must have been pilfered through sorcery.
As all Dobu continuously endeavored to steal one another's crops by such methods, the lucky gardener is likely to have viewed his surplus in precisely these terms.
A good harvest, therefore, was tantamount to ``a confession of theft.''

This strange marriage of covetousness and magical thinking created a perfect obsession with secrecy in Dobu society.
Whatever possibility of love and real friendship remained seems to have been fully extinguished by a final doctrine :
the power of sorcery was believed to grow in proportion to one's intimacy with the intended victim.
This belief gave every Dobuan an incandescent mistrust of all others, which burned brightest on those closest.
Therefore, if a man fell seriously ill or died, his misfortune was immediately blamed on his wife, and vice versa.
The picture is of a society completely in thrall to antisocial delusions.

Did the Dobu love their friends and family as much as we love ours?
Many people seem to think that the answer to such a question must, in principle, be ``yes,'' or that the question itself is vacuous.
I think it is clear, however, that the question is well posed and easily answered.
The answer is ``no.''
Being fellow Homo sapiens, we must presume that the Dobu islanders had brains sufficiently similar to our own to invite comparison.
Is there any doubt that the selfishness and general malevolence of the Dobu would have been expressed at the level of their brains?
Only if you think the brain does nothing more than filter oxygen and glucose out of the blood.
Once we more fully understand the neurophysiology of states like love, compassion, and trust, it will be possible to spell out the differences between ourselves and people like the Dobu in greater detail.
But we need not await any breakthroughs in neuroscience to bring the general principle in view :
just as it is possible for individuals and groups to be wrong about how best to maintain their physical health, it is possible for them to be wrong about how to maximize their personal and social well-being.

I believe that we will increasingly understand good and evil, right and wrong, in scientific terms, because moral concerns translate into facts about how our thoughts and behaviors affect the well-being of conscious creatures like ourselves.
If there are facts to be known about the well-being of such creatures --- and there are --- then there must be right and wrong answers to moral questions.
Students of philosophy will notice that this commits me to some form of moral realism (viz. moral claims can really be true or false) and some form of consequentialism (viz. the rightness of an act depends on how it impacts the well-being of conscious creatures).
While moral realism and consequentialism have both come under pressure in philosophical circles, they have the virtue of corresponding to many of our intuitions about how the world works.

Here is my (consequentialist) starting point :
all questions of value (right and wrong, good and evil, etc.) depend upon the possibility of experiencing such value.
Without potential consequences at the level of experience --- happiness, suffering, joy, despair, etc. --- all talk of value is empty.
Therefore, to say that an act is morally necessary, or evil, or blameless, is to make (tacit) claims about its consequences in the lives of conscious creatures (whether actual or potential).
I am unaware of any interesting exception to this rule.
Needless to say, if one is worried about pleasing God or His angels, this assumes that such invisible entities are conscious (in some sense) and cognizant of human behavior.
It also generally assumes that it is possible to suffer their wrath or enjoy their approval, either in this world or the world to come.
Even within religion, therefore, consequences and conscious states remain the foundation of all values.

Consider the thinking of a Muslim suicide bomber who decides to obliterate himself along with a crowd of infidels :
this would appear to be a perfect repudiation of the consequentialist attitude.
And yet, when we look at the rationale for seeking martyrdom within Islam, we see that the consequences of such actions, both real and imagined, are entirely the point.
Aspiring martyrs expect to please God and experience an eternity of happiness after death.
If one fully accepts the metaphysical presuppositions of traditional Islam, martyrdom must be viewed as the ultimate attempt at career advancement.
The martyr is also the greatest of altruists :
for not only does he secure a place for himself in Paradise, he wins admittance for seventy of his closest relatives as well.
Aspiring martyrs also believe that they are furthering God's work here on Earth, with desirable consequences for the living.
We know quite a lot about how such people think --- indeed, they advertise their views and intentions ceaselessly --- and it has everything to do with their belief that God has told them, in the Qur'an and the hadith, precisely what the consequences of certain thoughts and actions will be.
Of course, it seems profoundly unlikely that our universe has been designed to reward individual primates for killing one another while believing in the divine origin of a specific book.
The fact that would be martyrs are almost surely wrong about the consequences of their behavior is precisely what renders it such an astounding and immoral misuse of human life.

Because most religions conceive of morality as a matter of being obedient to the word of God (generally for the sake of receiving a supernatural reward), their precepts often have nothing to do with maximizing well-being in this world.
Religious believers can, therefore, assert the immorality of contraception, masturbation, homosexuality, etc., without ever feeling obliged to argue that these practices actually cause suffering.
They can also pursue aims that are flagrantly immoral, in that they needlessly perpetuate human misery, while believing that these actions are morally obligatory.
This pious uncoupling of moral concern from the reality of human and animal suffering has caused tremendous harm.

Clearly, there are mental states and capacities that contribute to our general well-being (happiness, compassion, kindness, etc.) as well as mental states and incapacities that diminish it (cruelty, hatred, terror, etc.).
It is, therefore, meaningful to ask whether a specific action or way of thinking will affect a person's well-being and/or the well-being of others, and there is much that we might eventually learn about the biology of such effects.
Where a person finds himself on this continuum of possible states will be determined by many factors ---- genetic, environmental, social, cognitive, political, economic, etc. --- and while our understanding of such influences may never be complete, their effects are realized at the level of the human brain.
Our growing understanding of the brain, therefore, will have increasing relevance for any claims we make about how thoughts and actions affect the welfare of human beings.

Notice that I do not mention morality in the preceding paragraph, and perhaps I need not.
I began this book by arguing that, despite a century of timidity on the part of scientists and philosophers, morality can be linked directly to facts about the happiness and suffering of conscious creatures.
However, it is interesting to consider what would happen if we simply ignored this step and merely spoke about ``well-being.''
What would our world be like if we ceased to worry about ``right'' and ``wrong,'' or ``good'' and ``evil,'' and simply acted so as to maximize well-being, our own and that of others?
Would we lose anything important?
And if important, wouldn't it be, by definition, a matter of someone's well-being?

\subsection{Can We Ever Be "Right" About Right and Wrong?}

The philosopher and neuroscientist Joshua Greene has done some of the most influential neuroimaging research on morality.
While Greene wants to understand the brain processes that govern our moral lives, he believes that we should be skeptical of moral realism on metaphysical grounds.
For Greene, the question is not, ``How can you know for sure that your moral beliefs are true?'' but rather, ``How could it be that anyone's moral beliefs are true?''
In other words, what is it about the world that could make a moral claim true or false?
He appears to believe that the answer to this question is ``nothing.''

However, it seems to me that this question is easily answered.
Moral view A is truer than moral view B, if A entails a more accurate understanding of the connections between human thoughts/intentions/behavior and human well-being.
Does forcing women and girls to wear burqas make a net positive contribution to human well-being?
Does it produce happier boys and girls?
Does it produce more compassionate men or more contented women?
Does it make for better relationships between men and women, between boys and their mothers, or between girls and their fathers?
I would bet my life that the answer to each of these questions is ``no.''
So, I think, would many scientists.
And yet, as we have seen, most scientists have been trained to think that such judgments are mere expressions of cultural bias --- and, thus, unscientific in principle.
Very few of us seem willing to admit that such simple, moral truths increasingly fall within the scope of our scientific worldview.
Greene articulates the prevailing skepticism quite well :

Moral judgment is, for the most part, driven not by moral reasoning, but by moral intuitions of an emotional nature.
Our capacity for moral judgment is a complex evolutionary adaptation to an intensely social life.
We are, in fact, so well adapted to making moral judgments that our making them is, from our point of view, rather easy, a part of ``common sense.''
And like many of our common sense abilities, our ability to make moral judgments feels to us like a perceptual ability, an ability, in this case, to discern immediately and reliably mind-independent moral facts.
As a result, we are naturally inclined toward a mistaken belief in moral realism.
The psychological tendencies that encourage this false belief serve an important biological purpose, and that explains why we should find moral realism so attractive even though it is false.
Moral realism is, once again, a mistake we were born to make.

Greene alleges that moral realism assumes that ``there is sufficient uniformity in people's underlying moral outlooks to warrant speaking as if there is a fact of the matter about what's `right' or `wrong,' `just' or `unjust.' ''
But do we really need to assume such uniformity for there to be right answers to moral questions?
Is physical or biological realism predicated on ``sufficient uniformity in people's underlying [physical or biological] outlooks''?
Taking humanity as a whole, I am quite certain that there is a greater consensus that cruelty is wrong (a common moral precept) than the passage of time varies with velocity (special relativity) or that humans and lobsters share a common ancestor (evolution).
Should we doubt whether there is a ``fact of the matter'' with respect to these physical and biological truth claims?
Does the general ignorance about the special theory of relativity or the pervasive disinclination of Americans to accept the scientific consensus on evolution put our scientific worldview, even slightly, in question?

Greene notes that it is often difficult to get people to agree about moral truth, or to even get an individual to agree with himself in different contexts.
These tensions lead him to the following conclusion :

[M]oral theorizing fails because our intuitions do not reflect a coherent set of moral truths and were not designed by natural selection or anything else to behave as if they were \dots
If you want to make sense of your moral sense, turn to biology, psychology, and sociology --- not normative ethics.

This objection to moral realism may seem reasonable, until one notices that it can be applied, with the same leveling effect, to any domain of human knowledge.
For instance, it is just as true to say that our logical, mathematical, and physical intuitions have not been designed by natural selection to track the Truth.
Does this mean that we must cease to be realists with respect to physical reality?
We need not look far in science to find ideas and opinions that defy easy synthesis.
There are many scientific frameworks (and levels of description) that resist integration and which divide our discourse into areas of specialization, even pitting Nobel laureates in the same discipline against one another.
Does this mean that we can never hope to understand what is really going on in the world?
No.
It means the conversation must continue.

Total uniformity in the moral sphere --- either interpersonally or intrapersonally --- may be hopeless.
So what?
This is precisely the lack of closure we face in all areas of human knowledge.
Full consensus as a scientific goal only exists in the limit, at a hypothetical end of inquiry.
Why not tolerate the same open-endedness in our thinking about human well-being?

Again, this does not mean that all opinions about morality are justified.
To the contrary --- the moment we accept that there are right and wrong answers to questions of human well-being, we must admit that many people are simply wrong about morality.
The eunuchs who tended the royal family in China's Forbidden City, dynasty after dynasty, seem to have felt generally well compensated for their lives of arrested development and isolation by the influence they achieved at court --- as well as by the knowledge that their genitalia, which had been preserved in jars all the while, would be buried with them after their deaths, ensuring them rebirth as human beings.
When confronted with such an exotic point of view, a moral realist would like to say we are witnessing more than a mere difference of opinion :
we are in the presence of moral error.
It seems to me that we can be reasonably confident that it is bad for parents to sell their sons into the service of a government that intends to cut off their genitalia ``using only hot chili sauce as a local anesthetic.''
This would mean that Sun Yaoting, the emperor's last eunuch, who died in 1996 at the age of ninety-four, was wrong to harbor, as his greatest regret, ``the fall of the imperial system he had aspired to serve.''
Most scientists seem to believe that no matter how maladaptive or masochistic a person's moral commitments, it is impossible to say that he is ever mistaken about what constitutes a good life.

\subsection{Moral Paradox}

One of the problems with consequentialism in practice is that we cannot always determine whether the effects of an action will be bad or good.
In fact, it can be surprisingly difficult to decide this even in retrospect.
Dennett has dubbed this problem ``the Three Mile Island Effect.''
Was the meltdown at Three Mile Island a bad outcome or a good one?
At first glance, it surely seems bad, but it might have also put us on a path toward greater nuclear safety, thereby saving many lives.
Or it might have caused us to grow dependent on more polluting technologies, contributing to higher rates of cancer and to global climate change.
Or it might have produced a multitude of effects, some mutually reinforcing, and some mutually canceling.
If we cannot determine the net result of even such a well-analyzed event, how can we judge the likely consequences of the countless decisions we must make throughout our lives?

One difficulty we face in determining the moral valence of an event is that it often seems impossible to determine whose well-being should most concern us.
People have competing interests, mutually incompatible notions of happiness, and there are many well-known paradoxes that leap into our path the moment we begin thinking about the welfare of whole populations.
As we are about to see, population ethics is a notorious engine of paradox, and no one, to my knowledge, has come up with a way of assessing collective well-being that conserves all of our intuitions.
As the philosopher Patricia Churchland puts it, ``no one has the slightest idea how to compare the mild headache of five million against the broken legs of two, or the needs of one's own two children against the needs of a hundred unrelated brain-damaged children in Serbia.''
Such puzzles may seem of mere academic interest, until we realize that population ethics governs the most important decisions societies ever make.
What are our moral responsibilities in times of war, when diseases spread, when millions suffer famine, or when global resources are scarce?
These are moments in which we have to assess changes in collective welfare in ways that purport to be rational and ethical.
Just how motivated should we be to act when 250,000 people die in an Earthquake on the island of Haiti?
Whether we know it or not, intuitions about the welfare of whole populations determine our thinking on these matters.
Except, that is, when we simply ignore population ethics --- as, it seems, we are psychologically disposed to do.
The work of the psychologist Paul Slovic and colleagues has uncovered some rather startling limitations on our capacity for moral reasoning when thinking about large groups of people --- or, indeed, about groups larger than one.
As Slovic observes, when human life is threatened, it seems both rational and moral for our concern to increase with the number of lives at stake.
And if we think that losing many lives might have some additional negative consequences (like the collapse of civilization), the curve of our concern should grow steeper still.
But this is not how we characteristically respond to the suffering of other human beings.

Slovic's experimental work suggests that we intuitively care most about a single, identifiable human life, less about two, and we grow more callous as the body count rises.
Slovic believes that this ``psychic numbing'' explains the widely lamented fact that we are generally more distressed by the suffering of single child (or even a single animal) than by a proper genocide.
What Slovic has termed ``genocide neglect'' --- our reliable failure to respond, both practically and emotionally, to the most horrific instances of unnecessary human suffering --- represents one of the more perplexing and consequential failures of our moral intuition.
Slovic found that when given a chance to donate money in support of needy children, subjects give most generously and feel the greatest empathy when told only about a single child's suffering.
When presented with two needy cases, their compassion wanes.
And this diabolical trend continues :
the greater the need, the less people are emotionally affected and the less they are inclined to give.

Of course, charities have long understood that putting a face on the data will connect their constituents to the reality of human suffering and increase donations.
Slovic's work has confirmed this suspicion, which is now known as the ``identifiable victim effect.''
Amazingly, however, adding information about the scope of a problem to these personal appeals proves to be counterproductive.
Slovic has shown that setting the story of a single needy person in the context of wider human need reliably diminishes altruism.

The fact that people seem to be reliably less concerned when faced with an increase in human suffering represents an obvious violation of moral norms.
The important point, however, is that we immediately recognize how indefensible this allocation of emotional and material resources is once it is brought to our attention.
What makes these experimental findings so striking is that they are patently inconsistent :
if you care about what happens to one little girl, and you care about what happens to her brother, you must, at the very least, care as much about their combined fate.
Your concern should be (in some sense) cumulative.
When your violation of this principle is revealed, you will feel that you have committed a moral error.
This explains why results of this kind can only be obtained between subjects (where one group is asked to donate to help one child and another group is asked to support two);
we can be sure that if we presented both questions to each participant in the study, the effect would disappear (unless subjects could be prevented from noticing when they were violating the norms of moral reasoning).

Clearly, one of the great tasks of civilization is to create cultural mechanisms that protect us from the moment-to-moment failures of our ethical intuitions.
We must build our better selves into our laws, tax codes, and institutions.
Knowing that we are generally incapable of valuing two children more than either child alone, we must build a structure that reflects and enforces our deeper understanding of human well-being.
This is where a science of morality could be indispensable to us :
the more we understand the causes and constituents of human fulfillment, and the more we know about the experiences of our fellow human beings, the more we will be able to make intelligent decisions about which social policies to adopt.
For instance, there are an estimated 90,000 people living on the streets of Los Angeles.
Why are they homeless?
How many of these people are mentally ill?
How many are addicted to drugs or alcohol?
How many have simply fallen through the cracks in our economy?
Such questions have answers.
And each of these problems admits of a range of responses, as well as false solutions and neglect.
Are there policies we could adopt that would make it easy for every person in the United States to help alleviate the problem of homelessness in their own communities?
Is there some brilliant idea that no one has thought of that would make people want to alleviate the problem of homelessness more than they want to watch television or play video games?
Would it be possible to design a video game that could help solve the problem of homelessness in the real world?
Again, such questions open onto a world of facts, whether or not we can bring the relevant facts into view.

Clearly, morality is shaped by cultural norms to a great degree, and it can be difficult to do what one believes to be right on one's own.
A friend's four-year-old daughter recently observed the role that social support plays in making moral decisions :
``It's so sad to eat baby lambies,'' she said as she gnawed greedily on a lamb chop.
``So, why don't you stop eating them?'' her father asked.
``Why would they kill such a soft animal?
Why wouldn't they kill some other kind of animal?''
``Because,'' her father said, ``people like to eat the meat. Like you are, right now.''
His daughter reflected for a moment --- still chewing her lamb --- and then replied :
``It's not good.
But I can't stop eating them if they keeping killing them.''
And the practical difficulties for consequentialism do not end here.
When thinking about maximizing the well-being of a population, are we thinking in terms of total or average well-being?
The philosopher Derek Parfit has shown that both bases of calculation lead to troubling paradoxes.
If we are concerned only about total welfare, we should prefer a world with hundreds of billions of people whose lives are just barely worth living to a world in which 7 billion of us live in perfect ecstasy.
This is the result of Parfit's famous argument known as ``The Repugnant Conclusion.''
If, on the other hand, we are concerned about the average welfare of a population, we should prefer a world containing a single, happy inhabitant to a world of billions who are only slightly less happy;
it would even suggest that we might want to painlessly kill many of the least happy people currently alive, thereby increasing the average of human well-being.
Privileging average welfare would also lead us to prefer a world in which billions live under the misery of constant torture to a world in which only one person is tortured ever-so-slightly more.
It could also render the morality of an action dependent upon the experience of unaffected people.
As Parfit points out, if we care about the average over time, we might deem it morally wrong to have a child today whose life, while eminently worth living, would not compare favorably to the lives of the ancient Egyptians.
Parfit has even devised scenarios in which everyone alive could have a lower quality of life than they otherwise would and yet the average quality of life will have increased.
Clearly, this proves that we cannot rely on a simple summation or averaging of welfare as our only metric.
And yet, at the extremes, we can see that human welfare must aggregate in some way :
it really is better for all of us to be deeply fulfilled than it is for everyone to live in absolute agony.

Placing only consequences in our moral balance also leads to indelicate questions.
For instance, do we have a moral obligation to come to the aid of wealthy, healthy, and intelligent hostages before poor, sickly, and slow-witted ones?
After all, the former are more likely to make a positive contribution to society upon their release.
And what about remaining partial to one's friends and family?
Is it wrong for me to save the life of my only child if, in the process, I neglect to save a stranger's brood of eight?
Wrestling with such questions has convinced many people that morality does not obey the simple laws of arithmetic.
However, such puzzles merely suggest that certain moral questions could be difficult or impossible to answer in practice;
they do not suggest that morality depends upon something other than the consequences of our actions and intentions.
This is a frequent source of confusion :
consequentialism is less a method of answering moral questions than it is a claim about the status of moral truth.
Our assessment of consequences in the moral domain must proceed as it does in all others :
under the shadow of uncertainty, guided by theory, data, and honest conversation.
The fact that it may often be difficult, or even impossible, to know what the consequences of our thoughts and actions will be does not mean that there is some other basis for human values that is worth worrying about.

Such difficulties notwithstanding, it seems to me quite possible that we will one day resolve moral questions that are often thought to be unanswerable.
For instance, we might agree that having a preference for one's intimates is better (in that it increases general welfare) than being fully disinterested as to how consequences accrue.
Which is to say that there may be some forms of love and happiness that are best served by each of us being specially connected to a subset of humanity.
This certainly appears to be descriptively true of us at present.
Communal experiments that ignore parents' special attachment to their own children, for instance, do not seem to work very well.
The Israeli kibbutzim learned this the hard way :
after discovering that raising children communally made both parents and children less happy, they reinstated the nuclear family.
Most people may be happier in a world in which a natural bias toward one's own children is conserved --- presumably in the context of laws and social norms that disregard this bias.
When I take my daughter to the hospital, I am naturally more concerned about her than I am about the other children in the lobby.
I do not, however, expect the hospital staff to share my bias.
In fact, given time to reflect about it, I realize that I would not want them to.
How could such a denial of my self-interest actually be in the service of my self-interest?
Well, first, there are many more ways for a system to be biased against me than in my favor, and I know that I will benefit from a fair system far more than I will from one that can be easily corrupted.
I also happen to care about other people, and this experience of empathy deeply matters to me.
I feel better as a person valuing fairness, and I want my daughter to become a person who shares this value.
And how would I feel if the physician attending my daughter actually shared my bias for her and viewed her as far more important than the other patients under his care?
Frankly, it would give me the creeps.

But perhaps there are two possible worlds that maximize the well-being of their inhabitants to precisely the same degree :
in world X everyone is focused on the welfare of all others without bias, while in world Y everyone shows some degree of moral preference for their friends and family.
Perhaps these worlds are equally good, in that their inhabitants enjoy precisely the same level of well-being.
These could be thought of as two peaks on the moral landscape.
Perhaps there are others.
Does this pose a threat to moral realism or to consequentialism?
No, because there would still be right and wrong ways to move from our current position on the moral landscape toward one peak or the other, and movement would still be a matter of increasing well-being in the end.
To bring the discussion back to the especially low-hanging fruit of conservative Islam :
there is absolutely no reason to think that demonizing homosexuals, stoning adulterers, veiling women, soliciting the murder of artists and intellectuals, and celebrating the exploits of suicide bombers will move humanity toward a peak on the moral landscape.
This is, I think, as objective a claim as we ever make in science.
Consider the Danish cartoon controversy :
an eruption of religious insanity that still flows to this day.
Kurt Westergaard, the cartoonist who drew what was arguably the most inflammatory of these utterly benign cartoons has lived in hiding since pious Muslims first began calling for his murder in 2006.
A few weeks ago --- more than three years after the controversy first began --- a Somali man broke into Westergaard's home with an axe.
Only the construction of a specially designed ``safe room'' allowed Westergaard to escape being slaughtered for the glory of God (his five-year-old granddaughter also witnessed the attack).
Westergaard now lives with continuous police protection --- as do the other eighty-seven men in Denmark who have the misfortune of being named ``Kurt Westergaard.''
The peculiar concerns of Islam have created communities in almost every society on Earth that grow so unhinged in the face of criticism that they will reliably riot, burn embassies, and seek to kill peaceful people, over cartoons.
This is something they will not do, incidentally, in protest over the continuous atrocities committed against them by their fellow Muslims.
The reasons why such a terrifying inversion of priorities does not tend to maximize human happiness are susceptible to many levels of analysis --- ranging from biochemistry to economics.
But do we need further information in this case?
It seems to me that we already know enough about the human condition to know that killing cartoonists for blasphemy does not lead anywhere worth going on the moral landscape.
There are other results in psychology and behavioral economics that make it difficult to assess changes in human well-being.
For instance, people tend to consider losses to be far more significant than forsaken gains, even when the net result is the same.
For instance, when presented with a wager where they stand a 50 percent chance of losing \$100, most people will consider anything less than a potential gain of \$200 to be unattractive.
This bias relates to what has come to be known as ``the endowment effect'' :
people demand more money in exchange for an object that has been given to them than they would spend to acquire the object in the first place.
In psychologist Daniel Kahneman's words, ``a good is worth more when it is considered as something that could be lost or given up than when it is evaluated as a potential gain.''
This aversion to loss causes human beings to generally err on the side of maintaining the status quo.
It is also an important impediment to conflict resolution through negotiation :
for if each party values his opponent's concessions as gains and his own as losses, each is bound to perceive his sacrifice as being greater.
Loss aversion has been studied with functional magnetic resonance imaging (fMRI).
If this bias were the result of negative feelings associated with potential loss, we would expect brain regions known to govern negative emotion to be involved.
However, researchers have not found increased activity in any areas of the brain as losses increase.
Instead, those regions that represent gains show decreasing activity as the size of the potential losses increases.
In fact, these brain structures themselves exhibit a pattern of ``neural loss aversion'' :
their activity decreases at a steeper rate in the face of potential losses than they increase for potential gains.
There are clearly cases in which such biases seem to produce moral illusions --- where a person's view of right and wrong will depend on whether an outcome is described in terms of gains or losses.
Some of these illusions might not be susceptible to full correction.
As with many perceptual illusions, it may be impossible to ``see'' two circumstances as morally equivalent, even while ``knowing'' that they are.
In such cases, it may be ethical to ignore how things seem.
Or it may be that the path we take to arrive at identical outcomes really does matter to us --- and, therefore, that losses and gains will remain incommensurable.

Imagine, for instance, that you are empaneled as the member of a jury in a civil trial and asked to determine how much a hospital should pay in damages to the parents of children who received substandard care in their facility.
There are two scenarios to consider :
Couple A learned that their three-year-old daughter was inadvertently given a neurotoxin by the hospital staff.
Before being admitted, their daughter was a musical prodigy with an IQ of 195.
She has since lost all her intellectual gifts.
She can no longer play music with any facility and her IQ is now a perfectly average 100.
Couple B learned that the hospital neglected to give their three-year-old daughter, who has an IQ of 100, a perfectly safe and inexpensive genetic enhancement that would have given her remarkable musical talent and nearly doubled her IQ.
Their daughter's intelligence remains average, and she lacks any noticeable musical gifts.
The critical period for giving this enhancement has passed.
Obviously the end result under either scenario is the same.
But what if the mental suffering associated with loss is simply bound to be greater than that associated with forsaken gains?
If so, it may be appropriate to take this difference into account, even when we cannot give a rational explanation of why it is worse to lose something than not to gain it.
This is another source of difficulty in the moral domain :
unlike dilemmas in behavioral economics, it is often difficult to establish the criteria by which two outcomes can be judged equivalent.
There is probably another principle at work in this example, however :
people tend to view sins of commission more harshly than sins of omission.
It is not clear how we should account for this bias either.
But, once again, to say that there are right answers to questions of how to maximize human well-being is not to say that we will always be in a position to answer such questions.
There will be peaks and valleys on the moral landscape, and movement between them is clearly possible, whether or not we always know which way is up.

There are many other features of our subjectivity that have implications for morality.
For instance, people tend to evaluate an experience based on its peak intensity (whether positive or negative) and the quality of its final moments.
In psychology, this is known as the ``peak/end rule.''
Testing this rule in a clinical environment, one group found that patients undergoing colonoscopies (in the days when this procedure was done without anesthetic) could have their perception of suffering markedly reduced, and their likelihood of returning for a follow-up exam increased, if their physician needlessly prolonged the procedure at its lowest level of discomfort by leaving the colonoscope inserted for a few extra minutes.
The same principle seems to hold for aversive sounds and for exposure to cold.
Such findings suggest that, under certain conditions, it is compassionate to prolong a person's pain unnecessarily so as to reduce his memory of suffering later on.
Indeed, it might be unethical to do otherwise.
Needless to say, this is a profoundly counterintuitive result.
But this is precisely what is so important about science :
it allows us to investigate the world, and our place within it, in ways that get behind first appearances.
Why shouldn't we do this with morality and human values generally?


\subsection{Fairness and Hierarchy}

It is widely believed that focusing on the consequences of a person's actions is merely one of several approaches to ethics --- one that is beset by paradox and often impossible to implement.
Imagined alternatives are either highly rational, as in the work of a modern philosopher like John Rawls, or decidedly otherwise, as we see in the disparate and often contradictory precepts that issue from the world's major religions.

My reasons for dismissing revealed religion as a source of moral guidance have been spelled out elsewhere, so I will not ride this hobbyhorse here, apart from pointing out the obvious:

\begin{enumerate}
      \item There are many revealed religions available to us, and they offer mutually incompatible doctrines;
      \item The scriptures of many religions, including the most well subscribed (i.e., Christianity and Islam), countenance patently unethical practices like slavery;
      \item The faculty we use to validate religious precepts, judging the Golden Rule to be wise and the murder of apostates to be foolish, is something we bring to scripture;
            it does not, therefore, come from scripture;
      \item The reasons for believing that any of the world's religions were ``revealed'' to our ancestors (rather than merely invented by men and women who did not have the benefit of a twenty-first-century education) are either risible or nonexistent --- and the idea that each of these mutually contradictory doctrines is inerrant remains a logical impossibility.
\end{enumerate}

Here we can take refuge in Bertrand Russell's famous remark that even if we could be certain that one of the world's religions was perfectly true, given the sheer number of conflicting faiths on offer, every believer should expect damnation purely as a matter of probability.

Among the rational challenges to consequentialism, the ``contractualism'' of John Rawls has been the most influential in recent decades.
In his book \textit{A Theory of Justice} Rawls offered an approach to building a fair society that he considered an alternative to the aim of maximizing human welfare.
His primary method, for which this work is duly famous, was to ask how reasonable people would structure a society, guided by their self-interest, if they couldn't know what sort of person they would be in it.
Rawls called this novel starting point ``the original position,'' from which each person must judge the fairness of every law and social arrangement from behind a ``veil of ignorance.''
In other words, we can design any society we like as long as we do not presume to know, in advance, whether we will be black or white, male or female, young or old, healthy or sick, of high or low intelligence, beautiful or ugly, etc.

As a method for judging questions of fairness, this thought experiment is undeniably brilliant.
But is it really an alternative to thinking about the actual consequences of our behavior?
How would we feel if, after structuring our ideal society from behind a veil of ignorance, we were told by an omniscient being that we had made a few choices that, though eminently fair, would lead to the unnecessary misery of millions, while parameters that were ever-so-slightly less fair would entail no such suffering?
Could we be indifferent to this information?
The moment we conceive of justice as being \textit{fully} separable from human well-being, we are faced with the prospect of there being morally ``right'' actions and social systems that are, on balance, detrimental to the welfare of everyone affected by them.
To simply bite the bullet on this point, as Rawls seemed to do, saying ``there is no reason to think that just institutions will maximize the good'' seems a mere embrace of moral and philosophical defeat.

Some people worry that a commitment to maximizing a society's welfare could lead us to sacrifice the rights and liberties of the few wherever these losses would be offset by the greater gains of the many.
Why not have a society in which a few slaves are continually worked to death for the pleasure of the rest?
The worry is that a focus on collective welfare does not seem to respect people as ends in themselves.
And whose welfare should we care about?
The pleasure that a racist takes in abusing some minority group, for instance, seems on all fours with the pleasure a saint takes in risking his life to help a stranger.
If there are more racists than saints, it seems the racists will win, and we will be obliged to build a society that maximizes the pleasure of unjust men.

But such concerns clearly rest on an incomplete picture of human well-being.
To the degree that treating people as ends in themselves is a good way to safeguard human well-being, it is precisely what we should do.
Fairness is not merely an abstract principle --- it is a felt experience.
We all know this from the inside, of course, but neuroimaging has also shown that fairness drives reward-related activity in the brain, while accepting unfair proposals requires the regulation of negative emotion.
Taking others' interests into account, making impartial decisions (and knowing that others will make them), rendering help to the needy --- these are experiences that contribute to our psychological and social well-being.
It seems perfectly reasonable, within a consequentialist framework, for each of us to submit to a system of justice in which our immediate, selfish interests will often be superseded by considerations of fairness.
It is only reasonable, however, on the assumption that everyone will tend to be better off under such a system.
As, it seems, they will.

While each individual's search for happiness may not be compatible in every instance with our efforts to build a just society, we should not lose sight of the fact that societies do not suffer;
people do.
The only thing wrong with injustice is that it is, on some level, actually or potentially bad for people.
Injustice makes its victims demonstrably less happy, and it could be easily argued that it tends to make its perpetrators less happy than they would be if they cared about the well-being of others.
Injustice also destroys trust, making it difficult for strangers to cooperate.
Of course, here we are talking about the nature of conscious experience, and so we are, of necessity, talking about processes at work in the brains of human beings.
The neuroscience of morality and social emotions is only just beginning, but there seems no question that it will one day deliver morally relevant insights regarding the material causes of our happiness and suffering.
While there may be some surprises in store for us down this path, there is every reason to expect that kindness, compassion, fairness, and other classically ``good'' traits will be vindicated neuroscientifically --- which is to say that we will only discover further reasons to believe that they are good for us, in that they generally enhance our lives.

We have already begun to see that morality, like rationality, implies the existence of certain norms --- that is, it does not merely describe how we tend to think and behave;
it tells us how we \textit{should} think and behave.
One norm that morality and rationality share is the interchangeability of perspective.
The solution to a problem should not depend on whether you are the husband or the wife, the employer or employee, the creditor or debtor, etc.
This is why one cannot argue for the rightness of one's views on the basis of mere preference.
In the moral sphere, this requirement lies at the core of what we mean by ``fairness.''
It also reveals why it is generally not a good thing to have a different ethical code for friends and strangers.

We have all met people who behave quite differently in business than in their personal lives.
While they would never lie to their friends, they might lie without a qualm to their clients or customers.
Why is this a moral failing?
At the very least, it is vulnerable to what could be called the principle of the unpleasant surprise.
Consider what happens to such a person when he discovers that one of his customers is actually a friend :
``Oh, why didn't you say you were Jennifer's sister!
Uh ... Okay, don't buy that model;
this one is a much better deal.''
Such moments expose a rift in a person's ethics that is always unflattering.
People with two ethical codes are perpetually susceptible to embarrassments of this kind.
They are also less trustworthy --- and trust is a measure of how much a person can be relied upon to safeguard other people's well-being.
Even if you happen to be a close friend of such a person --- that is, on the right side of his ethics --- you can't trust him to interact with others you may care about
(``I didn't know she was your daughter.
Sorry about that'').

Or consider the position of a Nazi living under the Third Reich, having fully committed himself to exterminating the world's Jews, only to learn, as many did, that he was Jewish himself.
Unless some compelling argument for the moral necessity of his suicide were forthcoming, we can imagine that it would be difficult for our protagonist to square his Nazi ethics with his actual identity.
Clearly, his sense of right and wrong was predicated on a false belief about his own genealogy.
A genuine ethics should not be vulnerable to such unpleasant surprises.
This seems another way of arriving at Rawls's ``original position.''
That which is right cannot be dependent upon one's being a member of a certain tribe --- if for no other reason than one can be mistaken about the fact of one's membership.

Kant's ``categorical imperative,'' perhaps the most famous prescription in all of moral philosophy, captures some of these same concerns :

Hence there is only one categorical imperative and it is this :
``Act only according to that maxim whereby you can at the same time will that it should become a universal law.''
While Kant believed that this criterion of universal applicability was the product of pure reason, it appeals to us because it relies on basic intuitions about fairness and justification.
One cannot claim to be ``right'' about anything --- whether as a matter of reason or a matter of ethics --- unless one's views can be generalized to others.

\subsection{Is Being Good Just Too Difficult?}

Most of us spend some time over the course of our lives deciding how (or whether) to respond to the fact that other people on Earth needlessly starve to death.
Most of us also spend some time deciding which delightful foods we want to consume at home and in our favorite restaurants.
Which of these projects absorbs more of your time and material resources on a yearly basis?
If you are like most people living in the developed world, such a comparison will not recommend you for sainthood.
Can the disparity between our commitment to fulfilling our selfish desires and our commitment to alleviating the unnecessary misery and death of millions be morally justified?
Of course not.
These failures of ethical consistency are often considered a strike against consequentialism.
They shouldn't be.
Who ever said that being truly good, or even ethically consistent, must be easy?

I have no doubt that I am less good than I could be.
Which is to say, I am not living in a way that truly maximizes the well-being of others.
I am nearly as sure, however, that I am also failing to live in a way that maximizes my own well-being.
This is one of the paradoxes of human psychology :
we often fail to do what we ostensibly want to do and what is most in our self-interest to do.
We often fail to do what we most want to do --- or, at the very least, we fail to do what, at the end of the day (or year, or lifetime) we will most wish we had done.

Just think of the heroic struggles many people must endure simply to quit smoking or lose weight.
The right course of action is generally obvious : if you are smoking two packs of cigarettes a day or are fifty pounds overweight, you are surely not maximizing your well-being.
Perhaps this isn't so clear to you now, but imagine : if you could successfully stop smoking or lose weight, what are the chances that you would regret this decision a year hence?
Probably zero.
And yet, if you are like most people, you will find it extraordinarily difficult to make the simple behavioral changes required to get what you want.

Most of us are in this predicament in moral terms.
I know that helping people who are starving is far more important than most of what I do.
I also have no doubt that doing what is most important would give me more pleasure and emotional satisfaction than I get from most of what I do by way of seeking pleasure and emotional satisfaction.
But this knowledge does not change me.
I still want to do what I do for pleasure more than I want to help the starving.
I strongly believe that I would be happier if I wanted to help the starving more --- and I have no doubt that they would be happier if I spent more time and money helping them --- but these beliefs are not sufficient to change me.
I know that I would be happier and the world would be a (marginally) better place if I were different in these respects.
I am, therefore, virtually certain that I am neither as moral, nor as happy, as I could be.
I know all of these things, and I want to maximize my happiness, but I am generally not moved to do what I believe will make me happier than I now am.

At bottom, these are claims both about the architecture of my mind and about the social architecture of our world.
It is quite clear to me that given the current state of my mind --- that is, given how my actions and uses of attention affect my life --- I would be happier if I were less selfish.
This means I would be more wisely and effectively selfish if I were less selfish.
This is not a paradox.

What if I could change the architecture of my mind?
On some level, this has always been possible, as everything we devote attention to, every discipline we adopt, or piece of knowledge we acquire changes our minds.
Each of us also now has access to a swelling armamentarium of drugs that regulate mood, attention, and wakefulness.
And the possibility of far more sweeping (as well as more precise) changes to our mental capacities may be within reach.
Would it be good to make changes to our minds that affect our sense of right and wrong?
And would our ability to alter our moral sense undercut the case I am making for moral realism?
What if, for instance, I could rewire my brain so that eating ice cream was not only extremely pleasurable, but also felt like the \textit{most important thing} I could do?

Despite the ready availability of ice cream, it seems that my new disposition would present certain challenges to self-actualization.
I would gain weight.
I would ignore social obligations and intellectual pursuits.
No doubt, I would soon scandalize others with my skewed priorities.
But what if advances in neuroscience eventually allow us to change the way every brain responds to morally relevant experiences?
What if we could program the entire species to hate fairness, to admire cheating, to love cruelty, to despise compassion, etc.
Would this be morally good?
Again, the devil is in the details.
Is this really a world of equivalent and genuine well-being, where the concept of ``well-being'' is susceptible to ongoing examination and refinement as it is in our world?
If so, so be it.
What could be more important than genuine well-being?
But, given all that the concept of ``well-being'' entails in our world, it is very difficult to imagine that its properties could be entirely fungible as we move across the moral landscape.

A miniature version of this dilemma is surely on the horizon :
increasingly, we will need to consider the ethics of using medications to mitigate mental suffering.
For instance, would it be good for a person to take a drug that made her indifferent to the death of her child?
Surely not while she still had responsibilities as a parent.
But what if a mother lost her only child and was thereafter inconsolable?
How much better than inconsolable should her doctor make her feel?
How much better should she want to feel?
Would any of us want to feel perfectly happy in this circumstance?
Given a choice --- and this choice, in some form, is surely coming --- I think that most of us will want our mental states to be coupled, however loosely, to the reality of our lives.
How else could our bonds with one another be maintained?
How, for instance, can we love our children and yet be totally indifferent to their suffering and death?
I suspect we cannot.
But what will we do once our pharmacies begin stocking a genuine antidote to grief?

If we cannot always resolve such conundrums, how should we proceed?
We cannot perfectly measure or reconcile the competing needs of billions of creatures.
We often cannot effectively prioritize our own competing needs.
What we can do is try, within practical limits, to follow a path that seems likely to maximize both our own well-being and the well-being of others.
This is what it means to live wisely and ethically.
As we will see, we have already begun to discover which regions of the brain allow us to do this.
A fuller understanding of what moral life entails, however, would require a science of morality.

\subsection{Bewildered by Diversity}

The psychologist Jonathan Haidt has put forward a very influential thesis about moral judgement known as the ``social-intuitionist model''.
In a widely referenced article entitled ``The Emotional Dog and Its Rational Tail,'' Haidt summarizes our predicament this way :

[O]ur moral life is plagued by two illusions.
The first illusion can be called the ``wag-the-dog'' illusion :
We believe that our own moral judgment (the dog) is driven by our own moral reasoning (the tail).
The second illusion can be called the “wag-the-other-dog's-tail” illusion :
In a moral argument, we expect the successful rebuttal of our opponents' arguments to change our opponents' minds.
Such a belief is analogous to believing that forcing a dog's tail to wag by moving it with your hand should make the dog happy.
Haidt does not go so far as to say that reasoning never produces moral judgments;
he simply argues that this happens far less often than people think.
Haidt is pessimistic about our ever making realistic claims about right and wrong, or good and evil, because he has observed that human beings tend to make moral decisions on the basis of emotion, justify these decisions with post hoc reasoning, and stick to their guns even when their reasoning demonstrably fails.
He notes that when asked to justify their responses to specific moral (and pseudo-moral) dilemmas, people are often ``morally dumbfounded.''
His experimental subjects would ``stutter, laugh, and express surprise at their inability to find supporting reasons, yet they would not change their initial judgments \dots```
The same can be said, however, about our failures to reason effectively.
Consider the Monty Hall Problem (based on the television game show Let's Make a Deal).
Imagine that you are a contestant on a game show and presented with three closed doors :
behind one sits a new car;
the other two conceal goats.
Pick the correct door, and the car is yours.

The game proceeds this way :
Assume that you have chosen Door $\#1$.
Your host then opens Door $\#2$, revealing a goat.
He now gives you a chance to switch your bet from Door $\#1$ to the remaining Door $\#3$.
Should you switch? The correct answer is ``yes.''
But most people find this answer very perplexing, as it violates the common intuition that, with two unopened doors remaining, the odds must be 1 in 2 that the car will be behind either one of them.
If you stick with your initial choice, however, your odds of winning are actually 1 in 3.
If you switch, your odds increase to 2 in 3.
It would be fair to say that the Monty Hall problem leaves many of its victims ``logically dumbfounded.''
Even when people understand conceptually why they should switch doors, they can't shake their initial intuition that each door represents a 1/2 chance of success.
This reliable failure of human reasoning is just that --- a failure of reasoning.
It does not suggest that there is no correct answer to the Monty Hall problem.

And yet scientists like Joshua Greene and Jonathan Haidt seem to think that the very existence of moral controversy nullifies the possibility of moral truth.
In their opinion, all we can do is study what human beings do in the name of ``morality.''
Thus, if religious conservatives find the prospect of gay marriage abhorrent, and secular liberals find it perfectly acceptable, we are confronted by a mere difference of moral preference --- not a difference that relates to any deeper truths about human life.
In opposition to the liberal notion of morality as being a system of ``prescriptive judgments of justice, rights, and welfare pertaining to how people ought to relate to each other,'' Haidt asks us to ponder mysteries of the following sort :

[I]f morality is about how we treat each other, then why did so many ancient texts devote so much space to rules about menstruation, who can eat what, and who can have sex with whom?
Interesting question.
Are these the same ancient texts that view slavery as morally unproblematic?
Perhaps slavery has no moral implications after all --- otherwise, surely these ancient texts would have something of substance to say against it.
Could abolition have been the ultimate instance of liberal bias?
Or, following Haidt's logic, why not ask, ``if physics is just a system of laws that explains the structure of the universe in terms of mass and energy, why do so many ancient texts devote so much space to immaterial influences and miraculous acts of God?''
Why indeed.
Haidt appears to consider it an intellectual virtue to accept, uncritically, the moral categories of his subjects.
But where is it written that everything that people do or decide in the name of ``morality'' deserves to be considered part of its subject matter?
A majority of Americans believe that the Bible provides an accurate account of the ancient world.
Many millions of Americans also believe that a principal cause of cancer is ``repressed anger.''
Happily, we do not allow these opinions to anchor us when it comes time to have serious discussions about history and oncology.
It seems abundantly clear that many people are simply wrong about morality --- just as many people are wrong about physics, biology, history, and everything else worth understanding.
What scientific purpose is served by averting our eyes from this fact?
If morality is a system of thinking about (and maximizing) the well-being of conscious creatures like ourselves, many people's moral concerns must be immoral.
Moral skeptics like Haidt generally emphasize the intractability of moral disagreements :
The bitterness, futility, and self-righteousness of most moral arguments can now be explicated.
In a debate about abortion, politics, consensual incest, or what my friend did to your friend, both sides believe that their positions are based on reasoning about the facts and issues involved (the wag-the-dog illusion).
Both sides present what they take to be excellent arguments in support of their positions.
Both sides expect the other side to be responsive to such reasons (the wag-the-other-dog's-tail illusion).
When the other side fails to be affected by such good reasons, each side concludes that the other side must be closed minded or insincere.
In this way the culture wars over issues such as homosexuality and abortion can generate morally motivated players on both sides who believe that their opponents are not morally motivated.
But the dynamic Haidt describes will be familiar to anyone who has ever entered into a debate on any subject.
Such failures of persuasion do not suggest that both sides of every controversy are equally credible.
For instance, the above passage perfectly captures my occasional collisions with 9/11 conspiracy theorists.
A nationwide poll conducted by the Scripps Survey Research Center at Ohio University found that more than a third of Americans suspect that the federal government ``assisted in the 9/11 terrorist attacks or took no action to stop them so the United States could go to war in the Middle East'' and 16 percent believe that this proposition is ``very likely'' to be true.
Many of these people believe that the Twin Towers collapsed not because fully fueled passenger jets smashed into them but because agents of the Bush administration had secretly rigged these buildings to explode (6 percent of all respondents judged this ``very likely,'' 10 percent judged it ``somewhat likely'').
Whenever I encounter people harboring these convictions, the impasse that Haidt describes is well in place :
both sides ``present what they take to be excellent arguments in support of their positions.
Both sides expect the other side to be responsive to such reasons (the wag-the-other-dog's-tail illusion).
When the other side fails to be affected by such good reasons, each side concludes that the other side must be closed minded or insincere.''
It is undeniable, however, that if one side in this debate is right about what actually happened on September 11, 2001, the other side must be absolutely wrong.

Of course, it is now well known that our feeling of reasoning objectively is often illusory.
This does not mean, however, that we cannot learn to reason more effectively, pay greater attention to evidence, and grow more mindful of the ever-present possibility of error.
Haidt is right to notice that the brain's emotional circuitry often governs our moral intuitions, and the way in which feeling drives judgment is surely worthy of study.
But it does not follow that there are no right and wrong answers to questions of morality.
Just as people are often less than rational when claiming to be rational, they can be less than moral when claiming to be moral.

In describing the different forms of morality available to us, Haidt offers a choice between ``contractual'' and ``beehive'' approaches :
the first is said to be the province of liberals, who care mainly about harm and fairness;
the second represents the conservative (generally religious) social order, which incorporates further concerns about group loyalty, respect for authority, and religious purity.
The opposition between these two conceptions of the good life may be worth discussing, and Haidt's data on the differences between liberals and conservatives is interesting, but is his interpretation correct?
It seems possible, for instance, that his five foundations of morality are simply facets of a more general concern about harm.
What, after all, is the problem with desecrating a copy of the Qur'an?
There would be no problem but for the fact that people believe that the Qur'an is a divinely authored text.
Such people almost surely believe that some harm could come to them or to their tribe as a result of such sacrileges --- if not in this world, then in the next.
A more esoteric view might be that any person who desecrates scripture will have harmed himself directly :
a lack of reverence might be its own punishment, dimming the eyes of faith.
Whatever interpretation one favors, sacredness and respect for religious authority seem to reduce to a concern about harm just the same.
The same point can be made in the opposite direction :
even a liberal like myself, enamored as I am of thinking in terms of harm and fairness, can readily see that my vision of the good life must be safeguarded from the aggressive tribalism of others.
When I search my heart, I discover that I want to keep the barbarians beyond the city walls just as much as my conservative neighbors do, and I recognize that sacrifices of my own freedom may be warranted for this purpose.
I expect that epiphanies of this sort could well multiply in the coming years.
Just imagine, for instance, how liberals might be disposed to think about the threat of Islam after an incident of nuclear terrorism.
Liberal hankering for happiness and freedom might one day produce some very strident calls for stricter laws and tribal loyalty.
Will this mean that liberals have become religious conservatives pining for the beehive?
Or is the liberal notion of avoiding harm flexible enough to encompass the need for order and differences between in-group and out-group?
There is also the question of whether conservatism contains an extra measure of cognitive bias --- or outright hypocrisy --- as the moral convictions of social conservatives are so regularly belied by their louche behavior.
The most conservative regions of the United States tend to have the highest rates of divorce and teenage pregnancy, as well as the greatest appetite for pornography.
Of course, it could be argued that social conservatism is the consequence of so much ambient sinning.
But this seems an unlikely explanation --- especially in those cases where a high level of conservative moralism and a predilection for sin can be found in a single person.
If one wants examples of such hypocrisy, Evangelical ministers and conservative politicians seem to rarely disappoint.

When is a belief system not only false but so encouraging of falsity and needless suffering as to be worthy of our condemnation?
According to a recent poll, 36 percent of British Muslims (ages sixteen to twenty-four) think apostates should be put to death for their unbelief.
Are these people ``morally motivated,'' in Haidt's sense, or just morally confused?

And what if certain cultures are found to harbor moral codes that look terrible no matter how we jigger Haidt's five variables of harm, fairness, group loyalty, respect for authority, and spiritual purity?
What if we find a group of people who aren't especially sensitive to harm and fairness, or cognizant of the sacred, or morally astute in any other way?
Would Haidt's conception of morality then allow us to stop these benighted people from abusing their children?
Or would that be unscientific?

\subsection{The Moral Brain}

Imagine that you are having dinner in a restaurant and spot your best friend's wife seated some distance away.
As you stand to say hello, you notice that the man seated across from her is not your best friend, but a handsome stranger.
You hesitate.
Is he a colleague of hers from work?
Her brother from out of town?
Something about the scene strikes you as illicit.
While you cannot hear what they are saying, there is an unmistakable sexual chemistry between them.
You now recall that your best friend is away at a conference.
Is his wife having an affair?
What should you do?
Several regions of the brain will contribute to this impression of moral salience and to the subsequent stirrings of moral emotion.
There are many separate strands of cognition and feeling that intersect here :
sensitivity to context, reasoning about other people's beliefs, the interpretation of facial expressions and body language, suspicion, indignation, impulse control, etc.
At what point do these disparate processes constitute an instance of moral cognition?
It is difficult to say.
At a minimum, we know that we have entered moral territory once thoughts about morally relevant events (e.g., the possibility of a friend's betrayal) have been consciously entertained.
For the purposes of this discussion, we need draw the line no more precisely than this.
The brain regions involved in moral cognition span many areas of the prefrontal cortex and the temporal lobes.
The neuroscientists Jorge Moll, Ricardo de Oliveira-Souza, and colleagues have written the most comprehensive reviews of this research.
They divide human actions into four categories :

\begin{enumerate}
      \item
            Self-serving actions that do not affect others
      \item
            Self-serving actions that negatively affect others
      \item
            Actions that are beneficial to others, with a high probability of reciprocation (``reciprocal altruism'')
      \item
            Actions that are beneficial to others, with no direct personal benefits (material or reputation gains) and no expected reciprocation (``genuine altruism'').
            This includes altruistic helping as well as costly punishment of norm violators (``altruistic punishment'')
\end{enumerate}

As Moll and colleagues point out, we share behaviors 1 through 3 with other social mammals, while 4 seems to be the special province of human beings.
(We should probably add that this altruism must be intentional/conscious, so as to exclude the truly heroic self-sacrifice seen among eusocial insects like bees, ants, and termites.)
While Moll et al. admit to ignoring the reward component of genuine altruism (often called the ``warm glow'' associated with cooperation), we know from neuroimaging studies that cooperation is associated with heightened activity in the brain's reward regions.
Here, once again, the traditional opposition between selfish and selfless motivation seems to break down.
If helping others can be rewarding, rather than merely painful, it should be thought of as serving the self in another mode.

It is easy to see the role that negative and positive motivations play in the moral domain :
we feel contempt/anger for the moral transgressions of others, guilt/shame over our own moral failings, and the warm glow of reward when we find ourselves playing nicely with other people.
Without the engagement of such motivational mechanisms, moral prescriptions (purely rational notions of ``ought'') would be very unlikely to translate into actual behaviors.
The fact that motivation is a separate variable explains the conundrum briefly touched on above :
we often know what would make us happy, or what would make the world a better place, and yet we find that we are not motivated to seek these ends;
conversely, we are often motivated to behave in ways that we know we will later regret.
Clearly, moral motivation can be uncoupled from the fruits of moral reasoning.
A science of morality would, of necessity, require a deeper understanding of human motivation.
The regions of the brain that govern judgments of right and wrong include a broad network of cortical and subcortical structures.
The contribution of these areas to moral thought and behavior differs with respect to emotional tone :
lateral regions of the frontal lobes seem to govern the indignation associated with punishing transgressors, while medial frontal regions produce the feelings of reward associated with trust and reciprocation.
As we will see, there is also a distinction between personal and impersonal moral decisions.
The resulting picture is complicated :
factors like moral sensitivity, moral motivation, moral judgment, and moral reasoning rely on separable, mutually overlapping processes.

The medial prefrontal cortex (MPFC) is central to most discussions of morality and the brain.
As discussed further in chapters 3 and 4, this region is involved in emotion, reward, and judgments of self-relevance.
It also seems to register the difference between belief and disbelief.
Injuries here have been associated with a variety of deficits including poor impulse control, emotional blunting, and the attenuation of social emotions like empathy, shame, embarrassment, and guilt.
When frontal damage is limited to the MPFC, reasoning ability as well as the conceptual knowledge of moral norms are generally spared, but the ability to behave appropriately toward others tends to be disrupted.

Interestingly, patients suffering from MPFC damage are more inclined to consequentialist reasoning than normal subjects are when evaluating certain moral dilemmas --- when, for instance, the means of sacrificing one person's life to save many others is personal rather than impersonal.
Consider the following two scenarios:

\begin{enumerate}
      \item
            You are at the wheel of a runaway trolley quickly approaching a fork in the tracks.
            On the tracks extending to the left is a group of five railway workmen.
            On the tracks extending to the right is a single railway workman.

            If you do nothing the trolley will proceed to the left, causing the deaths of the five workmen.
            The only way to avoid the deaths of these workmen is to hit a switch on your dashboard that will cause the trolley to proceed to the right, causing the death of the single workman.

            Is it appropriate for you to hit the switch in order to avoid the deaths of the five workmen?
      \item
            A runaway trolley is heading down the tracks toward five workmen who will be killed if the trolley proceeds on its present course.
            You are on a footbridge over the tracks, in between the approaching trolley and the five workmen.
            Next to you on this footbridge is a stranger who happens to be very large.

            The only way to save the lives of the five workmen is to push this stranger off the bridge and onto the tracks below where his large body will stop the trolley.
            The stranger will die if you do this, but the five workmen will be saved.

            Is it appropriate for you to push the stranger onto the tracks in order to save the five workmen?

\end{enumerate}

Most people strongly support sacrificing one person to save five in the first scenario, while considering such a sacrifice morally abhorrent in the second.
This paradox has been well known in philosophical circles for years.
Joshua Greene and colleagues were the first to look at the brain's response to these dilemmas using fMRI.
They found that the personal forms of these dilemmas, like the one described in scenario two, more strongly activate brain regions associated with emotion.
Another group has since found that the disparity between people's responses to the two scenarios can be modulated, however slightly, by emotional context.
Subjects who spent a few minutes watching a pleasant video prior to confronting the footbridge dilemma were more apt to push the man to his death.

The fact that patients suffering from MPFC injuries find it easier to sacrifice the one for the many is open to differing interpretations.
Greene views this as evidence that emotional and cognitive processes often work in opposition.
There are reasons to worry, however, that mere opposition between consequentialist thinking and negative emotion does not adequately account for the data.

I suspect that a more detailed understanding of the brain processes involved in making moral judgments of this type could affect our sense of right and wrong.
And yet superficial differences between moral dilemmas may continue to play a role in our reasoning.
If losses will always cause more suffering than forsaken gains, or if pushing a person to his death is guaranteed to traumatize us in a way that throwing a switch will not, these distinctions become variables that constrain how we can move across the moral landscape toward higher states of well-being.
It seems to me, however, that a science of morality can absorb these details :
scenarios that appear, on paper, to lead to the same outcome (e.g., one life lost, five lives saved), may actually have different consequences in the real world.

\subsection{Psychopaths}

In order to understand the relationship between the mind and the brain, it is often useful to study subjects who, whether through illness or injury, lack specific mental capacities.
As luck would have it, Mother Nature has provided us with a nearly perfect dissection of conventional morality.
The resulting persons are generally referred to as ``psychopaths'' or ``sociopaths,'' and there seem to be many more of them living among us than most of us realize.
Studying their brains has yielded considerable insight into the neural basis of conventional morality.

As a personality disorder, psychopathy has been so sensationalized in the media that it is difficult to research it without feeling that one is pandering, either to oneself or to one’s audience.
However, there is no question that psychopaths exist, and many of them speak openly about the pleasure they take in terrorizing and torturing innocent people.
The extreme examples, which include serial killers and sexual sadists, seem to defy any sympathetic understanding on our parts.
Indeed, if you immerse yourself in this literature, each case begins to seem more horrible and incomprehensible than the last.
While I am reluctant to traffic in the details of these crimes, I fear that speaking in abstractions may obscure the underlying reality.
Despite a steady diet of news, which provides a daily reminder of human evil, it can be difficult to remember that certain people truly lack the capacity to care about their fellow human beings.
Consider the statement of a man who was convicted of repeatedly raping and torturing his nine-year-old stepson :

After about two years of molesting my son, and all the pornography that I had been buying, renting, swapping, I had got my hands on some ``bondage discipline'' pornography with children involved.
Some of the reading that I had done and the pictures that I had seen showed total submission.
Forcing the children to do what I wanted.

And I eventually started using some of this bondage discipline with my own son, and it had escalated to the point where I was putting a large Zip-loc bag over his head and taping it around his neck with black duct tape or black electrical tape and raping and molesting him \dots to the point where he would turn blue, pass out.
At that point I would rip the bag off his head, not for fear of hurting him, but because of the excitement.

I was extremely aroused by inflicting pain.
And when I see him pass out and change colors, that was very arousing and heightening to me, and I would rip the bag off his head and then I’d jump on his chest and masturbate in his face and make him suck my penis while he \dots started to come back awake.
While he was coughing and choking, I would rape him in the mouth.

I used this same sadistic style of plastic bag and the tape two or three times a week, and it went on for I’d say a little over a year.

I suspect that this brief glimpse of one man’s private passions will suffice to make the point.
Be assured that this is not the worst abuse a man or woman has ever inflicted upon a child just for the fun of it.
And one remarkable feature of the literature on psychopaths is the extent to which even the worst people are able to find collaborators.
For instance, the role played by violent pornography in these cases is difficult to overlook.
Child pornography alone --- which, as many have noted, is the visual record of an actual crime --- is now a global, multibillion-dollar industry, involving kidnapping, ``sex tourism,'' organized crime, and great technical sophistication in the use of the internet.
Apparently, there are enough people who are eager to see children --- and, increasingly, toddlers and infants --- raped and tortured so as to create an entire subculture.

While psychopaths are especially well represented in our prisons, many live below the threshold of overt criminality.
For every psychopath who murders a child, there are tens of thousands who are guilty of far more conventional mischief.
Robert Hare, the creator of the standard diagnostic instrument to assess psychopathy, the Psychopathy Checklist-Revised (PCL-R), estimates that while there are probably no more than a hundred serial killers in the United States at any moment, there are probably 3 million psychopaths (about 1 percent of the population).
If Hare is correct, each of us crosses paths with such people all the time.

For instance, I recently met a man who took considerable pride in having arranged his life so as to cheat on his wife with impunity.
In fact, he was also cheating on the many women with whom he was cheating --- for each believed him to be faithful.
All this gallantry involved aliases, fake businesses, and, needless to say, a blizzard of lies.
While I can’t say for certain this man was a psychopath, it was quite apparent that he lacked what most of us would consider a normal conscience.
A life of continuous deception and selfish machination seemed to cause him no discomfort whatsoever.

Psychopaths are distinguished by their extraordinary egocentricity and their total lack of concern for the suffering of others.
A list of their most frequent characteristics reads like a personal ad from hell :
they are said to be callous, manipulative, deceptive, impulsive, secretive, grandiose, thrill-seeking, sexually promiscuous, unfaithful, irresponsible, prone to both reactive and calculated aggression, and lacking in emotional depth.
They also show reduced emotional sensitivity to punishment (whether actual or anticipated).
Most important, psychopaths do not experience a normal range of anxiety and fear, and this may account for their lack of conscience.

The first neuroimaging experiment done on psychopaths found that, when compared to nonpsychopathic criminals and noncriminal controls, they exhibit significantly less activity in regions of the brain that generally respond to emotional stimuli.
While anxiety and fear are emotions that most of us would prefer to live without, they serve as anchors to social and moral norms.
Without an ability to feel anxious about one’s own transgressions, real or imagined, norms become nothing more than ``rules that others make up.''
The developmental literature also supports this interpretation :
fearful children have been shown to display greater moral understanding.
It remains an open question, therefore, just how free of anxiety we can reasonably want to be.
Again, this is something that only an empirical science of morality could decide.
And as more effective remedies for anxiety appear on the horizon, this is an issue that we will have to confront in some form.

Further neuroimaging work suggests that psychopathy is also a product of pathological arousal and reward.
People scoring high on the psychopathic personality inventory show abnormally high activity in the reward regions of their brain (in particular, the nucleus accumbens) in response to amphetamine and while anticipating monetary gains.
Hypersensitivity of this circuitry is especially linked to the impulsive-antisocial dimension of psychopathy, which leads to risky and predatory behavior.
Researchers speculate that an excessive response to anticipated reward can prevent a person from learning from the negative emotions of others.

Unlike others who suffer from mental illness or mood disorders, psychopaths generally do not feel that anything is wrong with them.
They also meet the legal definition of sanity, in that they possess an intellectual understanding of the difference between right and wrong.
However, psychopaths generally fail to distinguish between conventional and moral transgressions.

When asked ``Would it be okay to eat at your desk if the teacher gave you permission?'' vs. ``Would it be okay to hit another student in the face if the teacher gave you permission?'' normal children age thirty-nine months and above tend to see these questions as fundamentally distinct and consider the latter transgression intrinsically wrong.
In this, they appear to be guided by an awareness of potential human suffering.
Children at risk for psychopathy tend to view these questions as morally indistinguishable.
When asked to identify the mental states of other people on the basis of photographs of their eyes alone, psychopaths show no general impairment.
Their ``theory of mind'' processing (as the ability to understand the mental states of others is generally known) seems to be basically intact, with subtle deficits resulting from their simply not caring about how other people feel.
The one crucial exception, however, is that psychopaths are often unable to recognize expressions of fear and sadness in others.
And this may be the difference that makes all the difference.

Neuroscientist James Blair and colleagues suggest that psychopathy results from a failure of emotional learning due to genetic impairments of the amygdala and orbitofrontal cortex, regions vital to the processing of emotion.
The negative emotions of others, rather than parental punishment, may be what goad us to normal socialization.
Psychopathy, therefore, could result from a failure to learn from the fear and sadness of other people.

A child at risk for psychopathy, being emotionally blind to the suffering he causes, may increasingly resort to antisocial behavior in pursuit of his goals throughout adolescence and adulthood.
As Blair points out, parenting strategies that increase empathy tend to successfully mitigate antisocial behavior in healthy children;
such strategies inevitably fail with children who present with the callousness/unemotional (CU) trait that is characteristic of psychopathy.
While it may be difficult to accept, the research strongly suggests that some people cannot learn to care about others.
Perhaps we will one day develop interventions to change this.
For the purposes of this discussion, however, it seems sufficient to point out that we are beginning to understand the kinds of brain pathologies that lead to the most extreme forms of human evil.
And just as some people have obvious moral deficits, others must possess moral talent, moral expertise, and even moral genius.
As with any human ability, these gradations must be expressed at the level of the brain.

Game theory suggests that evolution probably selected for two stable orientations toward human cooperation :
tit for tat (often called ``strong reciprocity'') and permanent defection.
Tit for tat is generally what we see throughout society :
you show me some kindness, and I am eager to return the favor;
you do something rude or injurious, and the temptation to respond in kind becomes difficult to resist.
But consider how permanent defection would appear at the level of human relationships :
the defector would probably engage in continuous cheating and manipulation, sham moralistic aggression (to provoke guilt and altruism in others), and strategic mimicry of positive social emotions like sympathy (as well as of negative emotions like guilt).
This begins to sound like garden-variety psychopathy.
The existence of psychopaths, while otherwise quite mysterious, would seem to be predicted by game theory.
And yet, the psychopath who lives his entire life in a tiny village must be at a terrible disadvantage.
The stability of permanent defection as a strategy would require that a defector be able to find people to fleece who are not yet aware of his terrible reputation.
Needless to say, the growth of cities has made this way of life far more practicable than it has ever been.

\subsection{Evil}

When confronted with psychopathy at its most extreme, it is very difficult not to think in terms of good and evil.
But what if we adopt a more naturalistic view?
Consider the prospect of being locked in a cage with a wild grizzly :
why would this be a problem?
Well, clearly, wild grizzlies suffer some rather glaring cognitive and emotional deficits.
Your new roommate will not be easy to reason with or placate;
he is unlikely to recognize that you have interests analogous to his own, or that the two of you might have shared interests;
and if he could understand such things, he would probably lack the emotional resources to care.
From his point of view, you will be a distraction at best, a cowering annoyance, and something tender to probe with his teeth.
We might say that a wild bear is, like a psychopath, morally insane.
However, we are very unlikely to refer to his condition as a form of ``evil.''

Human evil is a natural phenomenon, and some level of predatory violence is innate in us.
Humans and chimpanzees tend to display the same level of hostility toward outsiders, but chimps are far more aggressive than humans are within a group (by a factor of about 200).
Therefore, we seem to have prosocial abilities that chimps lack.
And, despite appearances, human beings have grown steadily less violent.
As Jared Diamond explains :

It’s true, of course, that twentieth-century state societies, having developed potent technologies of mass killing, have broken all historical records for violent deaths.
But this is because they enjoy the advantage of having by far the largest populations of potential victims in human history;
the actual percentage of the population that died violently was on the average higher in traditional pre-state societies than it was even in Poland during the Second World War or Cambodia under Pol Pot.

We must continually remind ourselves that there is a difference between what is natural and what is actually good for us.
Cancer is perfectly natural, and yet its eradication is a primary goal of modern medicine.
Evolution may have selected for territorial violence, rape, and other patently unethical behaviors as strategies to propagate one’s genes --- but our collective well-being clearly depends on our opposing such natural tendencies.

Territorial violence might have even been necessary for the development of altruism.
The economist Samuel Bowles has argued that lethal, ``out-group'' hostility and ``in-group'' altruism are two sides of the same coin.
His computer models suggest that altruism cannot emerge without some level of conflict between groups.
If true, this is one of the many places where we must transcend evolutionary pressures through reason --- because, barring an attack from outer space, we now lack a proper ``out-group'' to inspire us to further altruism.

In fact, Bowles’s work has interesting implications for my account of the moral landscape.
Consider the following from Patricia Churchland :

Assuming our woodland ape ancestors as well as our own human ancestors engaged in out-group raids, as chimps and several South American tribes still do, can we be confident in moral condemnation of their behavior?
I see no basis in reality for such a judgment.
If, as Samuel Bowles argues, the altruism typical of modern humans plausibly co-evolved with lethal out-group competition, such a judgment will be problematic.

Of course, the purpose of my argument is to suggest a ``basis in reality'' for universal judgments of value.
However, as Churchland points out, if there was simply no other way for our ancestors to progress toward altruism without developing a penchant for out-group hostility, then so be it.
Assuming that the development of altruism represents an extraordinarily important advance in moral terms (I believe it does), this would be analogous to our ancestors descending into an unpleasant valley on the moral landscape only to make progress toward a higher peak.
But it is important to reiterate that such evolutionary constraints no longer hold.
In fact, given recent developments in biology, we are now poised to consciously engineer our further evolution.
Should we do this, and if so, in which ways?
Only a scientific understanding of the possibilities of human well-being could guide us.

\subsection{The Illusion of Free Will}

Brains allow organisms to alter their behavior and internal states in response to changes in the environment.
The evolution of these structures, tending toward increased size and complexity, has led to vast differences in how the Earth’s species live.

The human brain responds to information coming from several domains :
from the external world, from internal states of the body, and, increasingly, from a sphere of meaning --- which includes spoken and written language, social cues, cultural norms, rituals of interaction, assumptions about the rationality of others, judgments of taste and style, etc.
Generally, these domains seem unified in our experience :
You spot your best friend standing on the street corner looking strangely disheveled.
You recognize that she is crying and frantically dialing her cell phone.
Did someone assault her?
You rush to her side, feeling an acute desire to help.
Your ``self'' seems to stand at the intersection of these lines of input and output.
From this point of view, you tend to feel that you are the source of your own thoughts and actions.
You decide what to do and not to do.
You seem to be an agent acting of your own free will.
As we will see, however, this point of view cannot be reconciled with what we know about the human brain.

We are conscious of only a tiny fraction of the information that our brains process in each moment.
While we continually notice changes in our experience --- in thought, mood, perception, behavior, etc. --- we are utterly unaware of the neural events that produce these changes.
In fact, by merely glancing at your face or listening to your tone of voice, others are often more aware of your internal states and motivations than you are.
And yet most of us still feel that we are the authors of our own thoughts and actions.

All of our behavior can be traced to biological events about which we have no conscious knowledge :
this has always suggested that free will is an illusion.
For instance, the physiologist Benjamin Libet famously demonstrated that activity in the brain’s motor regions can be detected some 350 milliseconds before a person feels that he has decided to move.
Another lab recently used fMRI data to show that some ``conscious'' decisions can be predicted up to 10 seconds before they enter awareness (long before the preparatory motor activity detected by Libet).
Clearly, findings of this kind are difficult to reconcile with the sense that one is the conscious source of one’s actions.
Notice that distinction between ``higher'' and ``lower'' systems in the brain gets us nowhere :
for I no more initiate events in executive regions of my prefrontal cortex than I cause the creaturely outbursts of my limbic system.
The truth seems inescapable :
I, as the subject of my experience, cannot know what I will next think or do until a thought or intention arises;
and thoughts and intentions are caused by physical events and mental stirrings of which I am not aware.

Many scientists and philosophers realized long ago that free will could not be squared with our growing understanding of the physical world.
Nevertheless, many still deny this fact.
The biologist Martin Heisenberg recently observed that some fundamental processes in the brain, like the opening and closing of ion channels and the release of synaptic vesicles, occur at random, and cannot, therefore, be determined by environmental stimuli.
Thus, much of our behavior can be considered ``self-generated,'' and therein, he imagines, lies a basis for free will.
But ``self-generated'' in this sense means only that these events originate in the brain.
The same can be said for the brain states of a chicken.

If I were to learn that my decision to have a third cup of coffee this morning was due to a random release of neurotransmitters, how could the indeterminacy of the initiating event count as the free exercise of my will?
Such indeterminacy, if it were generally effective throughout the brain, would obliterate any semblance of human agency.
Imagine what your life would be like if all your actions, intentions, beliefs, and desires were ``self-generated'' in this way :
you would scarcely seem to have a mind at all.
You would live as one blown about by an internal wind.
Actions, intentions, beliefs, and desires are the sorts of things that can exist only in a system that is significantly constrained by patterns of behavior and the laws of stimulus-response.
In fact, the possibility of reasoning with other human beings --- or, indeed, of finding their behaviors and utterances comprehensible at all --- depends on the assumption that their thoughts and actions will obediently ride the rails of a shared reality.
In the limit, Heisenberg’s ``self-generated'' mental events would amount to utter madness.

The problem is that no account of causality leaves room for free will.
Thoughts, moods, and desires of every sort simply spring into view --- and move us, or fail to move us, for reasons that are, from a subjective point of view, perfectly inscrutable.
Why did I use the term ``inscrutable'' in the previous sentence?
I must confess that I do not know.
Was I free to do otherwise?
What could such a claim possibly mean?
Why, after all, didn’t the word ``opaque'' come to mind?
Well, it just didn’t --- and now that it vies for a place on the page, I find that I am still partial to my original choice.
Am I free with respect to this preference?
Am I free to feel that ``opaque'' is the better word, when I just do not feel that it is the better word?
Am I free to change my mind?
Of course not.
It can only change me.

It means nothing to say that a person would have done otherwise had he chosen to do otherwise, because a person’s ``choices'' merely appear in his mental stream as though sprung from the void.
In this sense, each of us is like a phenomenological glockenspiel played by an unseen hand.
From the perspective of your conscious mind, you are no more responsible for the next thing you think (and therefore do) than you are for the fact that you were born into this world.

Our belief in free will arises from our moment-to-moment ignorance of specific prior causes.
The phrase ``free will'' describes what it feels like to be identified with the content of each thought as it arises in consciousness.
Trains of thought like, ``What should I get my daughter for her birthday? I know, I’ll take her to a pet store and have her pick out some tropical fish,'' convey the apparent reality of choices, freely made.
But from a deeper perspective (speaking both subjectively and objectively), thoughts simply arise (what else could they do?) unauthored and yet author to our actions.

As Daniel Dennett has pointed out, many people confuse determinism with fatalism.
This gives rise to questions like, ``If everything is determined, why should I do anything? Why not just sit back and see what happens?''
But the fact that our choices depend on prior causes does not mean that they do not matter.
If I had not decided to write this book, it wouldn’t have written itself.
My choice to write it was unquestionably the primary cause of its coming into being.
Decisions, intentions, efforts, goals, willpower, etc., are causal states of the brain, leading to specific behaviors, and behaviors lead to outcomes in the world.
Human choice, therefore, is as important as fanciers of free will believe.
And to ``just sit back and see what happens'' is itself a choice that will produce its own consequences.
It is also extremely difficult to do :
just try staying in bed all day waiting for something to happen;
you will find yourself assailed by the impulse to get up and do something, which will require increasingly heroic efforts to resist.

Of course, there is a distinction between voluntary and involuntary actions, but it does nothing to support the common idea of free will (nor does it depend upon it).
The former are associated with felt intentions (desires, goals, expectations, etc.) while the latter are not.
All of the conventional distinctions we like to make between degrees of intent --- from the bizarre neurological complaint of alien hand syndrome to the premeditated actions of a sniper --- can be maintained :
for they simply describe what else was arising in the mind at the time an action occurred.
A voluntary action is accompanied by the felt intention to carry it out, while an involuntary action isn’t.
Where our intentions themselves come from, however, and what determines their character in every instant, remains perfectly mysterious in subjective terms.
Our sense of free will arises from a failure to appreciate this fact :
we do not know what we will intend to do until the intention itself arises.
To see this is to realize that you are not the author of your thoughts and actions in the way that people generally suppose.
This insight does not make social and political freedom any less important, however.
The freedom to do what one intends, and not to do otherwise, is no less valuable than it ever was.

\subsection{Moral Responsibility}

The question of free will is no mere curio of philosophy seminars.
The belief in free will underwrites both the religious notion of ``sin'' and our enduring commitment to retributive justice.
The Supreme Court has called free will a ``universal and persistent'' foundation for our system of law, distinct from ``a deterministic view of human conduct that is inconsistent with the underlying precepts of our criminal justice system'' ( United States v. Grayson, 1978).
Any scientific developments that threatened our notion of free will would seem to put the ethics of punishing people for their bad behavior in question.
But, of course, human goodness and human evil are the product of natural events.
The great worry is that any honest discussion of the underlying causes of human behavior seems to erode the notion of moral responsibility.
If we view people as neuronal weather patterns, how can we coherently speak about morality?
And if we remain committed to seeing people as people, some who can be reasoned with and some who cannot, it seems that we must find some notion of personal responsibility that fits the facts.
What does it really mean to take responsibility for an action?
For instance, yesterday I went to the market;
as it turns out, I was fully clothed, did not steal anything, and did not buy anchovies.
To say that I was responsible for my behavior is simply to say that what I did was sufficiently in keeping with my thoughts, intentions, beliefs, and desires to be considered an extension of them.
If, on the other hand, I had found myself standing in the market naked, intent upon stealing as many tins of anchovies as I could carry, this behavior would be totally out of character;
I would feel that I was not in my right mind, or that I was otherwise not responsible for my actions.
Judgments of responsibility, therefore, depend upon the overall complexion of one’s mind, not on the metaphysics of mental cause and effect.

Consider the following examples of human violence :

\begin{enumerate}
      \item
            A four-year-old boy was playing with his father’s gun and killed a young woman.
            The gun had been kept loaded and unsecured in a dresser drawer.

      \item
            A twelve-year-old boy, who had been the victim of continuous physical and emotional abuse, took his father’s gun and intentionally shot and killed a young woman because she was teasing him.

      \item
            A twenty-five-year-old man, who had been the victim of continuous abuse as a child, intentionally shot and killed his girlfriend because she left him for another man.

      \item
            A twenty-five-year-old man, who had been raised by wonderful parents and never abused, intentionally shot and killed a young woman he had never met ``just for the fun of it.''

      \item
            A twenty-five-year-old man, who had been raised by wonderful parents and never abused, intentionally shot and killed a young woman he had never met ``just for the fun of it.''
            An MRI of the man’s brain revealed a tumor the size of a golf ball in his medial prefrontal cortex (a region responsible for the control of emotion and behavioral impulses).

\end{enumerate}

In each case a young woman has died, and in each case her death was the result of events arising in the brain of another human being.
The degree of moral outrage we feel clearly depends on the background conditions described in each case.
We suspect that a four-year-old child cannot truly intend to kill someone and that the intentions of a twelve-year-old do not run as deep as those of an adult.
In both cases 1 and 2, we know that the brain of the killer has not fully matured and that all the responsibilities of personhood have not yet been conferred.
The history of abuse and precipitating circumstance in example 3 seem to mitigate the man’s guilt :
this was a crime of passion committed by a person who had himself suffered at the hands of others.
In 4, we have no abuse, and the motive brands the perpetrator a psychopath.
In 5, we appear to have the same psychopathic behavior and motive, but a brain tumor somehow changes the moral calculus entirely :
given its location in the MPFC, it seems to divest the killer of all responsibility.
How can we make sense of these gradations of moral blame when brains and their background influences are, in every case, and to exactly the same degree, the real cause of a woman’s death?

It seems to me that we need not have any illusions about a casual agent living within the human mind to condemn such a mind as unethical, negligent, or even evil, and therefore liable to occasion further harm.
What we condemn in another person is the intention to do harm --- and thus any condition or circumstance (e.g., accident, mental illness, youth) that makes it unlikely that a person could harbor such an intention would mitigate guilt, without any recourse to notions of free will.
Likewise, degrees of guilt could be judged, as they are now, by reference to the facts of the case :
the personality of the accused, his prior offenses, his patterns of association with others, his use of intoxicants, his confessed intentions with regard to the victim, etc.
If a person’s actions seem to have been entirely out of character, this will influence our sense of the risk he now poses to others.
If the accused appears unrepentant and anxious to kill again, we need entertain no notions of free will to consider him a danger to society.

Of course, we hold one another accountable for more than those actions that we consciously plan, because most voluntary behavior comes about without explicit planning.
But why is the conscious decision to do another person harm particularly blameworthy?
Because consciousness is, among other things, the context in which our intentions become completely available to us.
What we do subsequent to conscious planning tends to most fully reflect the global properties of our minds --- our beliefs, desires, goals, prejudices, etc.
If, after weeks of deliberation, library research, and debate with your friends, you still decide to kill the king --- well, then killing the king really reflects the sort of person you are.
Consequently, it makes sense for the rest of society to worry about you.

While viewing human beings as forces of nature does not prevent us from thinking in terms of moral responsibility, it does call the logic of retribution into question.
Clearly, we need to build prisons for people who are intent upon harming others.
But if we could incarcerate Earthquakes and hurricanes for their crimes, we would build prisons for them as well.
The men and women on death row have some combination of bad genes, bad parents, bad ideas, and bad luck --- which of these quantities, exactly, were they responsible for?
No human being stands as author to his own genes or his upbringing, and yet we have every reason to believe that these factors determine his character throughout life.
Our system of justice should reflect our understanding that each of us could have been dealt a very different hand in life.
In fact, it seems immoral not to recognize just how much luck is involved in morality itself.

Consider what would happen if we discovered a cure for human evil.
Imagine, for the sake of argument, that every relevant change in the human brain can be made cheaply, painlessly, and safely.
The cure for psychopathy can be put directly into the food supply like vitamin D.
Evil is now nothing more than a nutritional deficiency.

If we imagine that a cure for evil exists, we can see that our retributive impulse is profoundly flawed.
Consider, for instance, the prospect of withholding the cure for evil from a murderer as part of his punishment.
Would this make any moral sense at all?
What could it possibly mean to say that a person deserves to have this treatment withheld?
What if the treatment had been available prior to the person’s crime?
Would he still be responsible for his actions?
It seems far more likely that those who had been aware of his case would be indicted for negligence.
Would it make any sense at all to deny surgery to the man in example 5 as a punishment if we knew the brain tumor was the proximate cause of his violence?
Of course not.
The urge for retribution, therefore, seems to depend upon our not seeing the underlying causes of human behavior.

Despite our attachment to notions of free will, most us know that disorders of the brain can trump the best intentions of the mind.
This shift in understanding represents progress toward a deeper, more consistent, and more compassionate view of our common humanity --- and we should note that this is progress away from religious metaphysics.
It seems to me that few concepts have offered greater scope for human cruelty than the idea of an immortal soul that stands independent of all material influences, ranging from genes to economic systems.

And yet one of the fears surrounding our progress in neuroscience is that this knowledge will dehumanize us.
Could thinking about the mind as the product of the physical brain diminish our compassion for one another?
While it is reasonable to ask this question, it seems to me that, on balance, soul/body dualism has been the enemy of compassion.
For instance, the moral stigma that still surrounds disorders of mood and cognition seems largely the result of viewing the mind as distinct from the brain.
When the pancreas fails to produce insulin, there is no shame in taking synthetic insulin to compensate for its lost function.
Many people do not feel the same way about regulating mood with antidepressants (for reasons that appear quite distinct from any concern about potential side effects).
If this bias has diminished in recent years, it has been because of an increased appreciation of the brain as a physical organ.

However, the issue of retribution is a genuinely tricky one.
In a fascinating article in The New Yorker, Jared Diamond recently wrote of the high price we often pay for leaving vengeance to the state.
He compares the experience of his friend Daniel, a New Guinea highlander, who avenged the death of a paternal uncle and felt exquisite relief, to the tragic experience of his late father-in-law, who had the opportunity to kill the man who murdered his family during the Holocaust but opted instead to turn him over to the police.
After spending only a year in jail, the killer was released, and Diamond’s father-in-law spent the last sixty years of his life ``tormented by regret and guilt.''
While there is much to be said against the vendetta culture of the New Guinea Highlands, it is clear that the practice of taking vengeance answers to a common psychological need.
We are deeply disposed to perceive people as the authors of their actions, to hold them responsible for the wrongs they do us, and to feel that these debts must be repaid.
Often, the only compensation that seems appropriate requires that the perpetrator of a crime suffer or forfeit his life.
It remains to be seen how the best system of justice would steward these impulses.
Clearly, a full account of the causes of human behavior should undermine our natural response to injustice, at least to some degree.
It seems doubtful, for instance, that Diamond’s father-in-law would have suffered the same pangs of unrequited vengeance if his family had been trampled by an elephant or laid low by cholera.
Similarly, we can expect that his regret would have been significantly eased if he had learned that his family’s killer had lived a flawlessly moral life until a virus began ravaging his medial prefrontal cortex.

It may be that a sham form of retribution could still be moral, if it led people to behave far better than they otherwise would.
Whether it is useful to emphasize the punishment of certain criminals --- rather than their containment or rehabilitation --- is a question for social and psychological science.
But it seems quite clear that a retributive impulse, based upon the idea that each person is the free author of his thoughts and actions, rests on a cognitive and emotional illusion --- and perpetuates a moral one.

It is generally argued that our sense of free will presents a compelling mystery :
on the one hand, it is impossible to make sense of it in causal terms;
on the other, there is a powerful subjective sense that we are the authors of our own actions.
However, I think that this mystery is itself a symptom of our confusion.
It is not that free will is simply an illusion :
our experience is not merely delivering a distorted view of reality;
rather, we are mistaken about the nature of our experience.
We do not feel as free as we think we feel.
Our sense of our own freedom results from our not paying attention to what it is actually like to be what we are.
The moment we do pay attention, we begin to see that free will is nowhere to be found, and our subjectivity is perfectly compatible with this truth.
Thoughts and intentions simply arise in the mind.
What else could they do?
The truth about us is stranger than many suppose :
\textit{The illusion of free will is itself an illusion}.

\newpage
\section{Chapter 3 : Belief}

A candidate for the presidency of the United States once met a group of potential supporters at the home of a wealthy benefactor.
After brief introductions, he spotted a bowl of potpourri on the table beside him.
Mistaking it for a bowl of trail mix, he scooped up a fistful of this decorative debris --- which consisted of tree bark, incense, flowers, pinecones, and other inedible bits of woodland --- and delivered it greedily into his mouth.
What our hero did next went unreported (suffice it to say that he did not become the next president of the United States).
We can imagine the psychology of the scene, however :
the candidate wide-eyed in ambush, caught between the look of horror on his host’s face and the panic of his own tongue, having to quickly decide whether to swallow the vile material or disgorge it in full view of his audience.
We can see the celebrities and movie producers feigning not to notice the great man’s gaffe and taking a sudden interest in the walls, ceiling, and floorboards of the room.
Some were surely less discreet.
We can imagine their faces from the candidate’s point of view :
a pageant of ill-concealed emotion, ranging from amazement to schadenfreude.
All such responses, their personal and social significance, and their moment-to-moment physiological effects, arise from mental capacities that are distinctly human :
the recognition of another’s intentions and state of mind, the representation of the self in both physical and social space, the impulse to save face (or to help others to save it), etc.
While such mental states undoubtedly have analogs in the lives of other animals, we human beings experience them with a special poignancy.
There may be many reasons for this, but one is clearly paramount :
we alone, among all Earth’s creatures, possess the ability to think and communicate with complex language.

The work of archeologists, paleoanthropologists, geneticists, and neuroscientists --- not to mention the relative taciturnity of our primate cousins --- suggests that human language is a very recent adaptation.
Our species diverged from its common ancestor with the chimpanzees only 6.3 million years ago.
And it now seems that the split with chimps may have been less than decisive, as comparisons between the two genomes, focusing on the greater-than-expected similarity of our X chromosomes, reveal that our species diverged, interbred for a time, and then diverged for good.
Such rustic encounters notwithstanding, all human beings currently alive appear to have descended from a single population of hunter-gatherers that lived in Africa around 50,000 BCE.
These were the first members of our species to exhibit the technical and social innovations made possible by language.
Genetic evidence indicates that a band of perhaps 150 of these people left Africa and gradually populated the rest of the Earth.
Their migration would not have been without its hardships, however, as they were not alone :
Homo neanderthalensis laid claim to Europe and the Middle East, and Homo erectus occupied Asia.
Both were species of archaic humans that had developed along separate evolutionary paths after one or more prior migrations out of Africa.
Both possessed large brains, fashioned stone tools similar to those of Homo sapiens, and were well armed.
And yet over the next twenty thousand years, our ancestors gradually displaced, and may have physically eradicated, all rivals.
Given the larger brains and sturdier build of the Neanderthals, it seems reasonable to suppose that only our species had the advantage of fully symbolic, complex speech.
While there is still controversy over the biological origins of human language, as well as over its likely precursors in the communicative behavior of other animals, there is no question that syntactic language lies at the root of our ability to understand the universe, to communicate ideas, to cooperate with one another in complex societies, and to build (one hopes) a sustainable, global civilization.
But why has language made such a difference?
How has the ability to speak (and to read and write of late) given modern humans a greater purchase on the world?
What, after all, has been worth communicating these last 50,000 years?
I hope it will not seem philistine of me to suggest that our ability to create fiction has not been the driving force here.
The power of language surely results from the fact that it allows mere words to substitute for direct experience and mere thoughts to simulate possible states of the world.
Utterances like, “I saw some very scary guys in front of that cave yesterday,” would have come in quite handy 50,000 years ago.
The brain’s capacity to accept such propositions as true --- as valid guides to behavior and emotion, as predictive of future outcomes, etc. --- explains the transformative power of words.
There is a common term we use for this type of acceptance; we call it ``belief.''

\subsection{What Is ``Belief''?}

It is surprising that so little research has been done on belief, as few mental states exert so sweeping an influence over human life.
While we often make a conventional distinction between ``belief'' and ``knowledge,'' these categories are actually quite misleading.
Knowing that George Washington was the first president of the United States and believing the statement ``George Washington was the first president of the United States'' amount to the same thing.
When we distinguish between belief and knowledge in ordinary conversation, it is generally for the purpose of drawing attention to degrees of certainty :
I’m apt to say ``I know it'' when I am quite certain that one of my beliefs about the world is true;
when I’m less sure, I may say something like ``I believe it is probably true.''
Most of our knowledge about the world falls between these extremes.
The entire spectrum of such convictions --- ranging from better-than-a-coin-toss to I-would-bet-my-life-on-it --- expresses gradations of ``belief.''
It is reasonable to wonder, however, whether ``belief'' is really a single phenomenon at the level of the brain.
Our growing understanding of human memory should make us cautious :
over the last fifty years, the concept of ``memory'' has decomposed into several forms of cognition that are now known to be neurologically and evolutionarily distinct.
This should make us wonder whether a notion like ``belief'' might not also shatter into separate processes when mapped onto the brain.
In fact, belief overlaps with certain types of memory, as memory can be equivalent to a belief about the past (e.g., ``I had breakfast most days last week''), and certain beliefs are indistinguishable from what is often called ``semantic memory'' (e.g., ``The earth is the third planet from the sun'').
There is no reason to think that any of our beliefs about the world are stored as propositions, or within discrete structures, inside the brain.
Merely understanding a simple proposition often requires the unconscious activation of considerable background knowledge and an active process of hypothesis testing.
For instance, a sentence like ``The team was terribly disappointed because the second stage failed to fire,'' while easy enough to read, cannot be understood without some general concept of a rocket launch and a team of engineers.
So there is more to even basic communication than the mere decoding of words.
We must expect that a similar penumbra of associations will surround specific beliefs as well.

And yet our beliefs can be represented and expressed as discrete statements.
Imagine hearing any one of the following assertions from a trusted friend:

\begin{enumerate}
      \item
            The CDC just announced that cell phones really do cause brain cancer.
      \item
            My brother won \$100,000 in Las Vegas over the weekend.
      \item
            Your car is being towed.

\end{enumerate}

We trade in such representations of the world all the time.
The acceptance of such statements as true (or likely to be true) is the mechanism by which we acquire most of our knowledge about the world.
While it would not make any sense to search for structures in the brain that correspond to specific sentences, we may be able to understand the brain states that allow us to accept such sentences as true.
When someone says ``Your car is being towed,'' it is your acceptance of this statement as true that sends you racing out the door.
``Belief,'' therefore, can be thought of as a process taking place in the present;
it is the act of grasping, not the thing grasped.

The Oxford English Dictionary defines multiple senses of the term ``belief'' :

\begin{enumerate}
      \item
            The mental action, condition, or habit, of trusting to or confiding in a person or thing;
            trust, dependence, reliance, confidence, faith.

      \item
            Mental acceptance of a proposition, statement, or fact as true, on the ground of authority or evidence;
            assent of the mind to a statement, or to the truth of a fact beyond observation, on the testimony of another, or to a fact or truth on the evidence of consciousness;
            the mental condition involved in this assent.

      \item
            The thing believed;
            the proposition or set of propositions held true.

\end{enumerate}

Definition 2 is exactly what we are after, and 1 may apply as well.
These first two senses of the term are quite different from the data-centered meaning given in 3.
Consider the following claim :
Starbucks does not sell plutonium.
I suspect that most of us would be willing to wager a fair amount of money that this statement is generally true --- which is to say that we believe it.
However, before reading this statement, you are very unlikely to have considered the prospect that the world’s most popular coffee chain might also trade in one of the world’s most dangerous substances.
Therefore, it does not seem possible for there to have been a structure in your brain that already corresponded to this belief.
And yet you clearly harbored some representation of the world that amounts to this belief.
Many modes of information processing must lay the groundwork for us to judge the above statement as ``true.''
Most of us know, in a variety of implicit and explicit ways, that Starbucks is not a likely proliferator of nuclear material.
Several distinct capacities --- episodic memory, semantic knowledge, assumptions about human behavior and economic incentives, inductive reasoning, etc. --- conspire to make us accept the above proposition.
To say that we already believed that one cannot buy plutonium at Starbucks is to merely put a name to the summation of these processes in the present moment :
that is, ``belief,'' in this case, is the disposition to accept a proposition as true (or likely to be).

This process of acceptance often does more than express our prior commitments, however.
It can revise our view of the world in an instant.
Imagine reading the following headline in tomorrow’s New York Times :
``Most of the World’s Coffee Is Now Contaminated by Plutonium.''
Believing this statement would immediately influence your thinking on many fronts, as well as your judgment about the truth of the former proposition.
Most of our beliefs have come to us in just this form :
as statements that we accept on the assumption that their source is reliable, or because the sheer number of sources rules out any significant likelihood of error.

In fact, everything we know outside of our personal experience is the result of our having encountered specific linguistic propositions --- the sun is a star;
Julius Caesar was a Roman emperor;
broccoli is good for you --- and found no reason (or means) to doubt them.
It is ``belief'' in this form, as an act of acceptance, which I have sought to better understand in my neuroscientific research.

\subsection{Looking For Belief In The Brain}

For a physical system to be capable of complex behavior, there must be some meaningful separation between its input and output.
As far as we know, this separation has been most fully achieved in the frontal lobes of the human brain.
Our frontal lobes are what allow us to select among a vast range of responses to incoming information in light of our prior goals and present inferences.
Such ``higher-level'' control of emotion and behavior is the stuff of which human personalities are made.
Clearly, the brain’s capacity to believe or disbelieve statements of fact --- You left your wallet on the bar;
that white powder is anthrax;
your boss is in love with you --- is central to the initiation, organization, and control of our most complex behaviors.

But we are not likely to find a region of the human brain devoted solely to belief.
The brain is an evolved organ, and there does not seem to be a process in nature that allows for the creation of new structures dedicated to entirely novel modes of behavior or cognition.
Consequently, the brain’s higher-order functions had to emerge from lower-order mechanisms.
An ancient structure like the insula, for instance, helps monitor events in our gut, governing the perception of hunger and primary emotions like disgust.
But it is also involved in pain perception, empathy, pride, humiliation, trust, music appreciation, and addictive behavior.
It may also play an important role in both belief formation and moral reasoning.
Such promiscuity of function is a common feature of many regions of the brain, especially in the frontal lobes.
No region of the brain evolved in a neural vacuum or in isolation from the other mutations simultaneously occurring within the genome.
The human mind, therefore, is like a ship that has been built and rebuilt, plank by plank, on the open sea.
Changes have been made to her sails, keel, and rudder even as the waves battered every inch of her hull.
And much of our behavior and cognition, even much that now seems essential to our humanity, has not been selected for at all.
There are no aspects of brain function that evolved to hold democratic elections, to run financial institutions, or to teach our children to read.
We are, in every cell, the products of nature --- but we have also been born again and again through culture.
Much of this cultural inheritance must be realized differently in individual brains.
The way in which two people think about the stock market, or recall that Christmas is a national holiday, or solve a puzzle like the Tower of Hanoi, will almost surely differ between individuals.
This poses an obvious challenge when attempting to identify mental states with specific brain states.
Another factor that makes the strict localization of any mental state difficult is that the human brain is characterized by massive interconnectivity :
it is mostly talking to itself.
And the information it stores must also be more fine-grained than the concepts, symbols, objects, or states that we subjectively experience.
Representation results from a pattern of activity across networks of neurons and does not generally entail stable, one-to-one mappings of things/events in the world, or concepts in the mind, to discrete structures in the brain.
For instance, thinking a simple thought like Jake is married cannot be the work of any single node in a network of neurons.
It must emerge from a pattern of connections among many nodes.
None of this bodes well for one who would seek a belief ``center'' in the human brain.

As part of my doctoral research at UCLA, I studied belief, disbelief, and uncertainty with functional magnetic resonance imaging (fMRI).
To do this, we had volunteers read statements from a wide variety of categories while we scanned their brains.
After reading a proposition like, ``California is part of the United States'' or ``You have brown hair,'' participants would judge them to be ``true,'' ``false,'' or ``undecidable'' with the click of a button.
This was, to my knowledge, the first time anyone had attempted to study belief and disbelief with the tools of neuroscience.
Consequently, we had no basis to form a detailed hypothesis about which regions of the brain govern these states of mind.
It was, nevertheless, reasonable to expect that the prefrontal cortex (PFC) would be involved, given its wider role in controlling emotion and complex behavior.
The seventeenth-century philosopher Spinoza thought that merely understanding a statement entails the tacit acceptance of its being true, while disbelief requires a subsequent process of rejection.
Several psychological studies seem to support this conjecture.
Understanding a proposition may be analogous to perceiving an object in physical space :
we may accept appearances as reality until they prove otherwise.
The behavioral data acquired in our research support this hypothesis, as subjects judged statements to be ``true'' more quickly than they judged them to be ``false'' or ``undecidable.''
When we compared the mental states of belief and disbelief, we found that belief was associated with greater activity in the medial prefrontal cortex (MPFC).
This region of the frontal lobes is involved in linking factual knowledge with relevant emotional associations, in changing behavior in response to reward, and in goal-based actions.
The MPFC is also associated with ongoing reality monitoring, and injuries here can cause people to confabulate --- that is, to make patently false statements without any apparent awareness that they are not telling the truth.
Whatever its cause in the brain, confabulation seems to be a condition in which belief processing has run amok.
The MPFC has often been associated with self-representation, and one sees more activity here when subjects think about themselves than when they think about others.
The greater activity we found in the MPFC for belief compared to disbelief may reflect the greater self-relevance and/or reward value of true statements.
When we believe a proposition to be true, it is as though we have taken it in hand as part of our extended self :
we are saying, in effect, ``This is mine.
I can use this.
This fits my view of the world.''
It seems to me that such cognitive acceptance has a distinctly positive emotional valence.
We actually like the truth, and we may, in fact, dislike falsehood.
The involvement of the MPFC in belief processing suggests an anatomical link between the purely cognitive aspects of belief and emotion/reward.
Even judging the truth of emotionally neutral propositions engaged regions of the brain that are strongly connected to the limbic system, which governs our positive and negative affect.
In fact, mathematical belief (e.g., ``2 + 6 + 8 = 16'') showed a similar pattern of activity to ethical belief (e.g., ``It is good to let your children know that you love them''), and these were perhaps the most dissimilar sets of stimuli used in our experiment.
This suggests that the physiology of belief may be the same regardless of a proposition’s content.
It also suggests that the division between facts and values does not make much sense in terms of underlying brain function.
Of course, we can differentiate my argument concerning the moral landscape from my fMRI work on belief.
I have argued that there is no gulf between facts and values, because values reduce to a certain type of fact.
This is a philosophical claim, and as such, I can make it before ever venturing into the lab.
However, my research on belief suggests that the split between facts and values should look suspicious :
First, belief appears to be largely mediated by the MPFC, which seems to already constitute an anatomical bridge between reasoning and value.
Second, the MPFC appears to be similarly engaged, irrespective of a belief’s content.
This finding of content-independence challenges the fact/value distinction very directly :
for if, from the point of view of the brain, believing ``the sun is a star'' is importantly similar to believing ``cruelty is wrong,'' how can we say that scientific and ethical judgments have nothing in common?

And we can traverse the boundary between facts and values in other ways.
As we are about to see, the norms of reasoning seem to apply equally to beliefs about facts and to beliefs about values.
In both spheres, evidence of inconsistency and bias is always unflattering.
Similarities of this kind suggest that there is a deep analogy, if not identity, between the two domains.

\subsection{The Tide Of Bias}

If one wants to understand how another person thinks, it is rarely sufficient to know whether or not he believes a specific set of propositions.
Two people can hold the same belief for very different reasons, and such differences generally matter.
In the year 2003, it was one thing to believe that the United States should not invade Iraq because the ongoing war in Afghanistan was more important;
it was another to believe it because you think it is an abomination for infidels to trespass on Muslim land.
Knowing what a person believes on a specific subject is not identical to knowing how that person thinks.
Decades of psychological research suggest that unconscious processes influence belief formation, and not all of them assist us in our search for truth.
When asked to judge the probability that an event will occur, or the likelihood that one event caused another, people are frequently misled by a variety of factors, including the unconscious influence of extraneous information.
For instance, if asked to recall the last four digits of their Social Security numbers and then asked to estimate the number of doctors practicing in San Francisco, the resulting numbers will show a statistically significant relationship.
Needless to say, when the order of questions is reversed, this effect disappears.
There have been a few efforts to put a brave face on such departures from rationality, construing them as random performance errors or as a sign that experimental subjects have misunderstood the tasks presented to them --- or even as proof that research psychologists themselves have been beguiled by false norms of reasoning.
But efforts to exonerate our mental limitations have generally failed.
There are some things that we are just naturally bad at.
And the mistakes people tend to make across a wide range of reasoning tasks are not mere errors;
they are systematic errors that are strongly associated both within and across tasks.
As one might expect, many of these errors decrease as cognitive ability increases.
We also know that training, using both examples and formal rules, mitigates many of these problems and can improve a person’s thinking.
Reasoning errors aside, we know that people often acquire their beliefs about the world for reasons that are more emotional and social than strictly cognitive.
Wishful thinking, self-serving bias, in-group loyalties, and frank self-deception can lead to monstrous departures from the norms of rationality.
Most beliefs are evaluated against a background of other beliefs and often in the context of an ideology that a person shares with others.
Consequently, people are rarely as open to revising their views as reason would seem to dictate.
On this front, the internet has simultaneously enabled two opposing influences on belief :
On the one hand, it has reduced intellectual isolation by making it more difficult for people to remain ignorant of the diversity of opinion on any given subject.
But it has also allowed bad ideas to flourish --- as anyone with a computer and too much time on his hands can broadcast his point of view and, often enough, find an audience.
So while knowledge is increasingly open-source, ignorance is, too.
It is also true that the less competent a person is in a given domain, the more he will tend to overestimate his abilities.
This often produces an ugly marriage of confidence and ignorance that is very difficult to correct for.
Conversely, those who are more knowledgeable about a subject tend to be acutely aware of the greater expertise of others.
This creates a rather unlovely asymmetry in public discourse --- one that is generally on display whenever scientists debate religious apologists.
For instance, when a scientist speaks with appropriate circumspection about controversies in his field, or about the limits of his own understanding, his opponent will often make wildly unjustified assertions about just which religious doctrines can be inserted into the space provided.
Thus, one often finds people with no scientific training speaking with apparent certainty about the theological implications of quantum mechanics, cosmology, or molecular biology.

This point merits a brief aside :
while it is a standard rhetorical move in such debates to accuse scientists of being ``arrogant,'' the level of humility in scientific discourse is, in fact, one of its most striking characteristics.
In my experience, arrogance is about as common at a scientific conference as nudity.
At any scientific meeting you will find presenter after presenter couching his or her remarks with caveats and apologies.
When asked to comment on something that lies to either side of the very knife edge of their special expertise, even Nobel laureates will say things like, ``Well, this isn’t really my area, but I would suspect that X is ...'' or ``I’m sure there a several people in this room who know more about this than I do, but as far as I know, X is ...''
The totality of scientific knowledge now doubles every few years.
Given how much there is to know, all scientists live with the constant awareness that whenever they open their mouths in the presence of other scientists, they are guaranteed to be speaking to someone who knows more about a specific topic than they do.

Cognitive biases cannot help but influence our public discourse.
Consider political conservatism :
this is a fairly well-defined perspective that is characterized by a general discomfort with societal change and a ready acceptance of social inequality.
As simple as political conservatism is to describe, we know that it is governed by many factors.
The psychologist John Jost and colleagues analyzed data from twelve countries, acquired from 23,000 subjects, and found this attitude to be correlated with dogmatism, inflexibility, death anxiety, need for closure, and anticorrelated with openness to experience, cognitive complexity, self-esteem, and social stability.
Even the manipulation of a single of these variables can affect political opinions and behavior.
For instance, merely reminding people of the fact of death increases their inclination to punish transgressors and to reward those who uphold cultural norms.
One experiment showed that judges could be led to impose especially harsh penalties on prostitutes if they were simply prompted to think about death prior to their deliberations.
And yet after reviewing the literature linking political conservatism to many obvious sources of bias, Jost and his coauthors reach the following conclusion :
Conservative ideologies, like virtually all other belief systems, are adopted in part because they satisfy various psychological needs.
To say that ideological belief systems have a strong motivational basis is not to say that they are unprincipled, unwarranted, or unresponsive to reason and evidence.
This has more than a whiff of euphemism about it.
Surely we can say that a belief system known to be especially beholden to dogmatism, inflexibility, death anxiety, and a need for closure will be less principled, less warranted, and less responsive to reason and evidence than it would otherwise be.

This is not to say that liberalism isn’t also occluded by certain biases.
In a recent study of moral reasoning, 43 subjects were asked to judge whether it was morally correct to sacrifice the life of one person to save one hundred, while being given subtle clues as to the races of the people involved.
Conservatives proved less biased by race than liberals and, therefore, more even-handed.
Liberals, as it turns out, were very eager to sacrifice a white person to save one hundred nonwhites, but not the other way around --- all the while maintaining that considerations of race had not entered into their thinking.
The point, of course, is that science increasingly allows us to identify aspects of our minds that cause us to deviate from norms of factual and moral reasoning --- norms which, when made explicit, are generally acknowledged to be valid by all parties.

There is a sense in which all cognition can be said to be motivated :
one is motivated to understand the world, to be in touch with reality, to remove doubt, etc.
Alternately, one might say that motivation is an aspect of cognition itself.
Nevertheless, motives like wanting to find the truth, not wanting to be mistaken, etc., tend to align with epistemic goals in a way that many other commitments do not.
As we have begun to see, all reasoning may be inextricable from emotion.
But if a person’s primary motivation in holding a belief is to hew to a positive state of mind --- to mitigate feelings of anxiety, embarrassment, or guilt, for instance --- this is precisely what we mean by phrases like ``wishful thinking'' and ``self-deception.''
Such a person will, of necessity, be less responsive to valid chains of evidence and argument that run counter to the beliefs he is seeking to maintain.
To point out nonepistemic motives in another’s view of the world, therefore, is always a criticism, as it serves to cast doubt upon a person’s connection to the world as it is.

\subsection{Mistaking Our Limits}

We have long known, principally through the neurological work of Antonio Damasio and colleagues, that certain types of reasoning are inseparable from emotion.
To reason effectively, we must have a feeling for the truth.
Our first fMRI study of belief and disbelief seemed to bear this out.
If believing a mathematical equation (vs. disbelieving another) and believing an ethical proposition (vs. disbelieving another) produce the same changes in neurophysiology, the boundary between scientific dispassion and judgments of value becomes difficult to establish.

However, such findings do not in the least diminish the importance of reason, nor do they blur the distinction between justified and unjustified belief.
On the contrary, the inseparability of reason and emotion confirms that the validity of a belief cannot merely depend on the conviction felt by its adherents;
it rests on the chains of evidence and argument that link it to reality.
Feeling may be necessary to judge the truth, but it cannot be sufficient.

The neurologist Robert Burton argues that the ``feeling of knowing'' (i.e., the conviction that one’s judgment is correct) is a primary positive emotion that often floats free of rational processes and can occasionally become wholly detached from logical or sensory evidence.
He infers this from neurological disorders in which subjects display pathological certainty (e.g., schizophrenia and Cotard’s delusion) and pathological uncertainty (e.g., obsessive-compulsive disorder).
Burton concludes that it is irrational to expect too much of human rationality.
On his account, rationality is mostly aspirational in character and often little more than a facade masking pure, unprincipled feeling.

Other neuroscientists have made similar claims.
Chris Frith, a pioneer in the use of functional neuroimaging, recently wrote :
[W]here does conscious reasoning come into the picture?
It is an attempt to justify the choice after it has been made.
And it is, after all, the only way we have to try to explain to other people why we made a particular decision.
But given our lack of access to the brain processes involved, our justification is often spurious :
a post-hoc rationalization, or even a confabulation --- a ``story'' born of the confusion between imagination and memory.

I doubt Frith meant to deny that reason ever plays a role in decision making (though the title of his essay was ``No One Really Uses Reason'').
He has, however, conflated two facts about the mind :
while it is true that all conscious processes, including any effort of reasoning, depend upon events of which we are not conscious, this does not mean that reasoning amounts to little more than a post hoc justification of brute sentiment.
We are not aware of the neurological processes that allow us to follow the rules of algebra, but this doesn’t mean that we never follow these rules or that the role they play in our mathematical calculations is generally post hoc.
The fact that we are unaware of most of what goes on in our brains does not render the distinction between having good reasons for what one believes and having bad ones any less clear or consequential.
Nor does it suggest that internal consistency, openness to information, self-criticism, and other cognitive virtues are less valuable than we generally assume.

There are many ways to make too much of the unconscious underpinnings of human thought.
For instance, Burton observes that one’s thinking on many moral issues --- ranging from global warming to capital punishment --- will be influenced by one’s tolerance for risk.
In evaluating the problem of global warming, one must weigh the risk of melting the polar ice caps;
in judging the ethics of capital punishment, one must consider the risk of putting innocent people to death.
However, people differ significantly with respect to risk tolerance, and these differences appear to be governed by a variety of genes --- including genes for the D4 dopamine receptor and the protein stathmin (which is primarily expressed in the amygdala).
Believing that there can be no optimal degree of risk aversion, Burton concludes that we can never truly reason about such ethical questions.
``Reason'' will simply be the name we give to our unconscious (and genetically determined) biases.
But is it really true to say that every degree of risk tolerance will serve our purposes equally well as we struggle to build a global civilization?
Does Burton really mean to suggest that there is no basis for distinguishing healthy from unhealthy --- or even suicidal --- attitudes toward risk?
As it turns out, dopamine receptor genes may play a role in religious belief as well.
People who have inherited the most active form of the D4 receptor are more likely to believe in miracles and to be skeptical of science;
the least active forms correlate with ``rational materialism.''
Skeptics given the drug L-dopa, which increases dopamine levels, show an increased propensity to accept mystical explanations for novel phenomena.

The fact that religious belief is both a cultural universal and appears to be tethered to the genome has led scientists like Burton to conclude that there is simply no getting rid of faith-based thinking.
It seems to me that Burton and Frith have misunderstood the significance of unconscious cognitive processes.
On Burton’s account, worldviews will remain idiosyncratic and incommensurable, and the hope that we might persuade one another through rational argument and, thereby, fuse our cognitive horizons is not only vain but symptomatic of the very unconscious processes and frank irrationality that we would presume to expunge.
This leads him to conclude that any rational criticism of religious irrationality is an unseemly waste of time :
The science-religion controversy cannot go away;
it is rooted in biology \dots
Scorpions sting.
We talk of religion, afterlife, soul, higher powers, muses, purpose, reason, objectivity, pointlessness, and randomness.
We cannot help ourselves \dots
To insist that the secular and the scientific be universally adopted flies in the face of what neuroscience tells us about different personality traits generating idiosyncratic worldviews \dots
Different genetics, temperaments, and experience led to contrasting worldviews.
Reason isn’t going to bridge this gap between believers and nonbelievers.
The problem, however, is that we could have said the same about witchcraft.
Historically, a preoccupation with witchcraft has been a cultural universal.
And yet belief in magic is now in disrepute almost everywhere in the developed world.
Is there a scientist on earth who would be tempted to argue that belief in the evil eye or in the demonic origins of epilepsy is bound to remain impervious to reason?
Lest the analogy between religion and witchcraft seem quaint, it is worth remembering that belief in magic and demonic possession is still epidemic in Africa.
In Kenya elderly men and women are regularly burned alive as witches.
In Angola, Congo, and Nigeria the hysteria has mostly targeted children :
thousands of unlucky boys and girls have been blinded, injected with battery acid, and otherwise put to torture in an effort to purge them of demons;
others have been killed outright;
many more have been disowned by their families and rendered homeless.
Needless to say, much of this lunacy has spread in the name of Christianity.
The problem is especially intractable because the government officials charged with protecting these suspected witches also believe in witchcraft.
As was the case in the Middle Ages, when the belief in witchcraft was omnipresent in Europe, only a truly panoramic ignorance about the physical causes of disease, crop failure, and life’s other indignities allows this delusion to thrive.

What if we were to connect the fear of witches with the expression of a certain receptor subtype in the brain?
Who would be tempted to say that the belief in witchcraft is, therefore, ineradicable?
As someone who has received many thousands of letters and emails from people who have ceased to believe in the God of Abraham, I know that pessimism about the power of reason is unwarranted.
People can be led to notice the incongruities in their faith, the self-deception and wishful thinking of their coreligionists, and the growing conflict between the claims of scripture and the findings of modern science.
Such reasoning can inspire them to question their attachment to doctrines that, in the vast majority of cases, were simply drummed into them on mother’s knee.
The truth is that people can transcend mere sentiment and clarify their thinking on almost any subject.
Allowing competing views to collide --- through open debate, a willingness to receive criticism, etc. --- performs just such a function, often by exposing inconsistencies in a belief system that make its adherents profoundly uncomfortable.
There are standards to guide us, even when opinions differ, and the violation of such standards generally seems consequential to everyone involved.
Self-contradiction, for instance, is viewed as a problem no matter what one is talking about.
And anyone who considers it a virtue is very unlikely to be taken seriously.
Again, reason is not starkly opposed to feeling on this front;
it entails a feeling for the truth.
Conversely, there are occasions when a true proposition just doesn’t seem right no matter how one squints one’s eyes or cocks one’s head, and yet its truth can be acknowledged by anyone willing to do the necessary intellectual work.
It is very difficult to grasp that tiny quantities of matter contain vast amounts of explosive energy, but the equations of physics --- along with the destructive yield of our nuclear bombs --- confirms that this is so.
Similarly, we know that most people cannot produce or even recognize a series of digits or coin tosses that meets a statistical test for randomness.
But this has not stopped us from understanding randomness mathematically --- or from factoring our innate blindness to randomness into our growing understanding of cognition and economic behavior.
The fact that reason must be rooted in our biology does not negate the principles of reason.
Wittgenstein once observed that the logic of our language allows us to ask, ``Was that gunfire?'' but not ``Was that a noise?''
This seems to be a contingent fact of neurology, rather than an absolute constraint upon logic.
A synesthete, for instance, who experiences crosstalk between his primary senses (seeing sounds, tasting colors, etc.), might be able to pose the latter question without any contradiction.
How the world seems to us (and what can be logically said about its seemings) depends upon facts about our brains.
Our inability to say that an object is ``red and green all over'' is a fact about the biology of vision before it is a fact of logic.
But that doesn’t prevent us from seeing beyond this very contingency.
As science advances, we are increasingly coming to understand the natural limits of our understanding.

\subsection{Belief And Reasoning}

There is a close relationship between belief and reasoning.
Many of our beliefs are the product of inferences drawn from particular instances (induction) or from general principles (deduction), or both.
Induction is the process by which we extrapolate from past observations to novel instances, anticipate future states of the world, and draw analogies from one domain to another.
Believing that you probably have a pancreas (because people generally have the same parts), or interpreting the look of disgust on your son’s face to mean that he doesn’t like Marmite, are examples of induction.
This mode of thinking is especially important for ordinary cognition and for the practice of science, and there have been a variety of efforts to model it computationally.
Deduction, while less central to our lives, is an essential component of any logical argument.
If you believe that gold is more expensive than silver, and silver more expensive than tin, deduction reveals that you also believe gold to be more expensive than tin.
Induction allows us to move beyond the facts already in hand;
deduction allows us to make the implications of our current beliefs more explicit, to search for counterexamples, and to see whether our views are logically coherent.
Of course, the boundaries between these (and other) forms of reasoning are not always easy to specify, and people succumb to a wide range of biases in both modes.

It is worth reflecting on what a reasoning bias actually is :
a bias is not merely a source of error;
it is a reliable pattern of error.
Every bias, therefore, reveals something about the structure of the human mind.
And diagnosing a pattern of errors as a ``bias'' can only occur with reference to specific norms --- and norms can sometimes be in conflict.
The norms of logic, for instance, don’t always correspond to the norms of practical reasoning.
An argument can be logically valid, but unsound in that it contains a false premise and, therefore, leads to a false conclusion (e.g., Scientists are smart;
smart people do not make mistakes;
therefore, scientists do not make mistakes).
Much research on deductive reasoning suggests that people have a ``bias'' for sound conclusions and will judge a valid argument to be invalid if its conclusion lacks credibility.
It’s not clear that this ``belief bias'' should be considered a symptom of native irrationality.
Rather, it seems an instance in which the norms of abstract logic and practical reason may simply be in conflict.

Neuroimaging studies have been performed on various types of human reasoning.
As we have seen, however, accepting the fruits of such reasoning (i.e., belief) seems to be an independent process.
While this is suggested by my own neuroimaging research, it also follows directly from the fact that reasoning accounts only for a subset of our beliefs about the world.

Consider the following statements :

\begin{enumerate}
      \item
            All known soil samples contain bacteria; so the soil in my garden probably contains bacteria as well (induction).
      \item
            Dan is a philosopher, all philosophers have opinions about Nietzsche;
            therefore, Dan has an opinion about Nietzsche (deduction).
      \item
            Mexico shares a border with the United States.
      \item
            You are reading at this moment.

\end{enumerate}

Each of these statements must be evaluated by different channels of neural processing (and only the first two require reasoning).
And yet each has the same cognitive valence :
being true, each inspires belief (or being believed, each is deemed ``true'').
Such cognitive acceptance allows any apparent truth to take its place in the economy of our thoughts and actions, at which time it becomes as potent as its propositional content demands.

\subsection{A World Without Lying}

Knowing what a person believes is equivalent to knowing whether or not he is telling the truth.
Consequently, any external means of determining which propositions a subject believes would constitute a de facto ``lie detector.''
Neuroimaging research on belief and disbelief may one day enable researchers to put this equivalence to use in the study of deception.
It is possible that this new approach could circumvent many of the impediments that have hindered the study of deception in the past.

When evaluating the social cost of deception, we need to consider all of the misdeeds --- premeditated murders, terrorist atrocities, genocides, Ponzi schemes, etc. --- that must be nurtured and shored up, at every turn, by lies.
Viewed in this wider context, deception commends itself, perhaps even above violence, as the principal enemy of human cooperation.
Imagine how our world would change if, when the truth really mattered, it became impossible to lie.
What would international relations be like if every time a person shaded the truth on the floor of the United Nations an alarm went off throughout the building?

The forensic use of DNA evidence has already made the act of denying one’s culpability for certain actions comically ineffectual.
Recall how Bill Clinton’s cantatas of indignation were abruptly silenced the moment he learned that a semen-stained dress was en route to the lab.
The mere threat of a DNA analysis produced what no grand jury ever could --- instantaneous communication with the great man’s conscience, which appeared to be located in another galaxy.
We can be sure that a dependable method of lie detection would produce similar transformations, on far more consequential subjects.

The development of mind-reading technology is just beginning --- but reliable lie detection will be much easier to achieve than accurate mind reading.
Whether or not we ever crack the neural code, enabling us to download a person’s private thoughts, memories, and perceptions without distortion, we will almost surely be able to determine, to a moral certainty, whether a person is representing his thoughts, memories, and perceptions honestly in conversation.
The development of a reliable lie detector would only require a very modest advance over what is currently possible through neuroimaging.

Traditional methods for detecting deception through polygraphy never achieved widespread acceptance, as they measure the peripheral signs of emotional arousal rather than the neural activity associated with deception itself.
In 2002, in a 245-page report, the National Research Council (an arm of the National Academy of Sciences) dismissed the entire body of research underlying polygraphy as ``weak'' and ``lacking in scientific rigor.''
More modern approaches tuo lie detection, using thermal imaging of the eyes, suffer a similar lack of specificity.
Techniques that employ electrical signals at the scalp to detect ``guilty knowledge'' have limited application, and it is unclear how one can use these methods to differentiate guilty knowledge from other forms of knowledge in any case.

Methodological problems notwithstanding, it is difficult to exaggerate how fully our world would change if lie detectors ever became reliable, affordable, and unobtrusive.
Rather than spirit criminal defendants and hedge fund managers off to the lab for a disconcerting hour of brain scanning, there may come a time when every courtroom or boardroom will have the requisite technology discreetly concealed behind its wood paneling.
Thereafter, civilized men and women might share a common presumption :
that wherever important conversations are held, the truthfulness of all participants will be monitored.
Well-intentioned people would happily pass between zones of obligatory candor, and these transitions will cease to be remarkable.
Just as we’ve come to expect that certain public spaces will be free of nudity, sex, loud swearing, and cigarette smoke --- and now think nothing of the behavioral constraints imposed upon us whenever we leave the privacy of our homes --- we may come to expect that certain places and occasions will require scrupulous truth telling.
Many of us might no more feel deprived of the freedom to lie during a job interview or at a press conference than we currently feel deprived of the freedom to remove our pants in the supermarket.
Whether or not the technology works as well as we hope, the belief that it generally does work would change our culture profoundly.

In a legal context, some scholars have already begun to worry that reliable lie detection will constitute an infringement of a person’s Fifth Amendment privilege against self-incrimination.
However, the Fifth Amendment has already succumbed to advances in technology.
The Supreme Court has ruled that defendants can be forced to provide samples of their blood, saliva, and other physical evidence that may incriminate them.
Will neuroimaging data be added to this list, or will it be considered a form of forced testimony?
Diaries, emails, and other records of a person’s thoughts are already freely admissible as evidence.
It is not at all clear that there is a distinction between these diverse sources of information that should be ethically or legally relevant to us.

In fact, the prohibition against compelled testimony itself appears to be a relic of a more superstitious age.
It was once widely believed that lying under oath would damn a person’s soul for eternity, and it was thought that no one, not even a murderer, should be placed between the rock of Justice and so hard a place as hell.
But I doubt whether even many fundamentalist Christians currently imagine that an oath sworn on a courtroom Bible has such cosmic significance.

Of course, no technology is ever perfect.
Once we have a proper lie detector in hand, well-intentioned people will begin to suffer its propensity for positive and negative error.
This will raise ethical and legal concerns.
It is inevitable, however, that we will deem some rate of error to be acceptable.
If you doubt this, remember that we currently lock people away in prison for decades --- or kill them --- all the while knowing that some percentage of the condemned must be innocent, while some percentage of those returned to our streets will be dangerous psychopaths guaranteed to reoffend.
We are currently living with a system in which the occasional unlucky person gets falsely convicted of murder, suffers for years in prison in the company of terrifying predators, only to be finally executed by the state.
Consider the tragic case of Cameron Todd Willingham, who was convicted of setting fire to the family home and thereby murdering his three children.
While protesting his innocence, Willingham served over a decade on death row and was finally executed.
It now seems that he was almost surely innocent --- the victim of a chance electrical fire, forensic pseudoscience, and of a justice system that has no reliable means of determining when people are telling the truth.

We have no choice but to rely upon our criminal justice system, despite the fact that judges and juries are very poorly calibrated truth detectors, prone to both type I (false positive) and type II (false negative) errors.
Anything that can improve the performance of this antiquated system, even slightly, will raise the quotient of justice in our world.

\subsection{Do We Have Freedom of Belief}

While belief might prove difficult to pinpoint in the brain, many of its mental properties are plain to see.
For instance, people do not knowingly believe propositions for bad reasons.
If you doubt this, imagine hearing the following account of a failed New Year’s resolution :

This year, I vowed to be more rational, but by the end of January, I found that I had fallen back into my old ways, believing things for bad reasons.
Currently, I believe that robbing others is a harmless activity, that my dead brother will return to life, and that I am destined to marry Angelina Jolie, just because these beliefs make me feel good.

This is not how our minds work.
A belief --- to be actually believed --- entails the corollary belief that we have accepted it because it seems to be true.
To really believe a proposition --- whether about facts or values --- we must also believe that we are in touch with reality in such a way that if it were not true, one would not believe it.
We must believe, therefore, that we are not flagrantly in error, deluded, insane, self-deceived, etc.
While the preceding sentences do not suffice as a full account of epistemology, they go a long way toward uniting science and common sense, as well as reconciling their frequent disagreements.
There can be no doubt that there is an important difference between a belief that is motivated by an unconscious emotional bias (or other nonepistemic commitments) and a belief that is comparatively free of such bias.

And yet many secularists and academics imagine that people of faith knowingly believe things for reasons that have nothing to do with their perception of the truth.
A written debate I had with Philip Ball --- who is a scientist, a science journalist, and an editor at Nature --- brought this issue into focus.
Ball thought it is reasonable for a person to believe a proposition just because it makes him ``feel better,'' and he seemed to think that people are perfectly free to acquire beliefs in this way.
People often do this unconsciously, of course, and such motivated reasoning has been discussed above.
But Ball seemed to think that beliefs can be consciously adopted simply because a person feels better while under their spell.
Let’s see how this might work.
Imagine someone making the following statement of religious conviction :

I believe Jesus was born of a virgin, was resurrected, and now answers prayers because believing these things makes me feel better.
By adopting this faith, I am merely exercising my freedom to believe in propositions that make me feel good.

How would such a person respond to information that contradicted his cherished belief?
Given that his belief is based purely on how it makes him feel, and not on evidence or argument, he shouldn’t care about any new evidence or argument that might come his way.
In fact, the only thing that should change his view of Jesus is a change in how the above propositions make him feel.
Imagine our believer undergoing the following epiphany :

For the last few months, I’ve found that my belief in the divinity of Jesus no longer makes me feel good.
The truth is, I just met a Muslim woman who I greatly admire, and I want to ask her out on a date.
As Muslims believe Jesus was not divine, I am worried that my belief in the divinity of Jesus could hinder my chances with her.
As I do not like feeling this way, and very much want to go out with this woman, I now believe that Jesus was not divine.

Has a person like this ever existed?
I highly doubt it.
Why do these thoughts not make any sense?
Because beliefs are intrinsically epistemic :
they purport to represent the world as it is.
In this case, our man is making specific claims about the historical Jesus, about the manner of his birth and death, and about his special connection to the Creator of the Universe.
And yet while claiming to represent the world in this way, it is perfectly clear that he is making no effort to stay in touch with the features of the world that should inform his belief.
He is only concerned about how he feels.
Given this disparity, it should be clear that his beliefs are not based on any foundation that would (or should) justify them to others, or even to himself.

Of course, people do often believe things in part because these beliefs make them feel better.
But they do not do this in the full light of consciousness. Self-deception, emotional bias, and muddled thinking are facts of human cognition.
And it is a common practice to act as if a proposition were true, in the spirit of :
``I’m going to act on X because I like what it does for me and, who knows, X might be true.''
But these phenomena are not at all the same as knowingly believing a proposition simply because one wants it to be true.

Strangely, people often view such claims about the constraints of rationality as a sign of ``intolerance.''
Consider the following from Ball :
I do wonder what [Sam Harris] is implying here.
It is hard to see it as anything other than an injunction that ``you should not be free to choose what you believe.''
I guess that if all Sam means is that we should not leave people so ill-informed that they have no reasonable basis on which to make those decisions, then fair enough.
But it does seem to go further --- to say that ``you should not be permitted to choose what you believe, simply because it makes you feel better.''
Doesn’t this sound a little like a Marxist denouncement of ``false consciousness,'' with the implication that it needs to be corrected forthwith?
I think (I hope?) we can at least agree that there are different categories of belief --- that to believe one’s children are the loveliest in the world because that makes you feel better is a permissible (even laudable) thing.
But I slightly shudder at the notion, hinted here, that a well-informed person should not be allowed to choose their belief freely \dots surely we cannot let ourselves become proscriptive to this degree?

What cognitive freedom is Ball talking about?
I happen to believe that George Washington was the first president of the United States.
Have I, on Ball’s terms, chosen this belief ``freely''?
No.
Am I free to believe otherwise?
Of course not.
I am a slave to the evidence.
I live under the lash of historical opinion.
While I may want to believe otherwise, I simply cannot overlook the incessant pairing of the name ``George Washington'' with the phrase ``first president of the United States'' in any discussion of American history.
If I wanted to be thought an idiot, I could profess some other belief, but I would be lying.
Likewise, if the evidence were to suddenly change --- if, for instance, compelling evidence of a great hoax emerged and historians reconsidered Washington’s biography, I would be helplessly stripped of my belief --- again, through no choice of my own.
Choosing beliefs freely is not what rational minds do.

This does not mean, of course, that we have no mental freedom whatsoever.
We can choose to focus on certain facts to the exclusion of others, to emphasize the good rather than the bad, etc.
And such choices have consequences for how we view the world.
One can, for instance, view Kim Jong-il as an evil dictator;
one can also view him as a man who was once the child of a dangerous psychopath.
Both statements are, to a first approximation, true.
(Obviously, when I speak about ``freedom'' and ``choices'' of this sort, I am not endorsing a metaphysical notion of ``free will.'')

As to whether there are ``different categories of belief'':
perhaps, but not in the way that Ball suggests.
I happen to have a young daughter who does strike me as the ``loveliest in the world.''
But is this an accurate account of what I believe?
Do I, in other words, believe that my daughter is really the loveliest girl in the world?
If I learned that another father thought his daughter the loveliest in the world, would I insist that he was mistaken?
Of course not.
Ball has mischaracterized what a proud (and sane and intellectually honest) father actually believes.
Here is what I believe :
I believe that I have a special attachment to my daughter that largely determines my view of her (which is as it should be).
I fully expect other fathers to have a similar bias toward their own daughters.
Therefore, I do not believe that my daughter is the loveliest girl in the world in any objective sense.
Ball is simply describing what it’s like to love one’s daughter more than other girls;
he is not describing belief as a representation of the world.
What I really believe is that my daughter is the loveliest girl in the world for me.

One thing that both factual and moral beliefs generally share is the presumption that we have not been misled by extraneous information.
Situational variables, like the order in which unrelated facts are presented, or whether identical outcomes are described in terms of gains or losses, should not influence the decision process.
Of course, the fact that such manipulations can strongly influence our judgment has given rise to some of the most interesting work in psychology.
However, a person’s vulnerability to such manipulations is never considered a cognitive virtue;
rather, it is a source of inconsistency that cries out for remedy.

Consider one of the more famous cases from the experimental literature, The Asian Disease Problem :

Imagine that the United States is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people.
Two alternative programs to combat the disease have been proposed.
Assume that the exact scientific estimates of the consequences of the programs are as follows :

If Program A is adopted, 200 people will be saved.

If Program B is adopted, there is a one-third probability that 600 people will be saved and a two-thirds probability that no people will be saved.

Which one of the two programs would you favor?

In this version of the problem, a significant majority of people favor Program A.
The problem, however, can be restated this way:

If Program A is adopted, 400 people will die.

If Program B is adopted, there is a one-third probability that nobody will die and a two-thirds probability that 600 people will die.

Which one of the two programs would you favor?

Put this way, a majority of respondents will now favor Program B.
And yet there is no material or moral difference between these two scenarios, because their outcomes are the same.
What this shows is that people tend to be risk-averse when considering potential gains and risk seeking when considering potential losses, so describing the same event in terms of gains and losses evokes different responses.
Another way of stating this is that people tend to overvalue certainty :
finding the certainty of saving life inordinately attractive and the certainty of losing life inordinately painful.
When presented with the Asian Disease Problem in both forms, however, people agree that each scenario merits the same response.
Invariance of reasoning, both logical and moral, is a norm to which we all aspire.
And when we catch others departing from this norm, whatever the other merits of their thinking, the incoherency of their position suddenly becomes its most impressive characteristic.

Of course, there are many other ways in which we can be misled by context.
Few studies illustrate this more powerfully than one conducted by the psychologist David L. Rosenhan, in which he and seven confederates had themselves committed to psychiatric hospitals in five different states in an effort to determine whether mental health professionals could detect the presence of the sane among the mentally ill.
In order to get committed, each researcher complained of hearing a voice repeating the words ``empty,'' ``hollow,'' and ``thud.''
Beyond that, each behaved perfectly normally.
Upon winning admission to the psychiatric ward, the pseudopatients stopped complaining of their symptoms and immediately sought to convince the doctors, nurses, and staff that they felt fine and were fit to be released.
This proved surprisingly difficult.
While these genuinely sane patients wanted to leave the hospital, repeatedly declared that they experienced no symptoms, and became ``paragons of cooperation,'' their average length of hospitalization was nineteen days (ranging from seven to fifty-two days), during which they were bombarded with an astounding range of powerful drugs (which they discreetly deposited in the toilet).
None were pronounced healthy.
Each was ultimately discharged with a diagnosis of schizophrenia ``in remission'' (with the exception of one who received a diagnosis of bipolar disorder).
Interestingly, while the doctors, nurses, and staff were apparently blind to the presence of normal people on the ward, actual mental patients frequently remarked on the obvious sanity of the researchers, saying things like ``You’re not crazy. You’re a journalist.”

In a brilliant response to the skeptics at one hospital who had heard of this research before it was published, Rosenhan announced that he would send a few confederates their way and challenged them to spot the coming pseudopatients.
The hospital kept vigil, while Rosenhan, in fact, sent no one.
This did not stop the hospital from ``detecting'' a steady stream of pseudopatients.
Over a period of a few months fully 10 percent of their new patients were deemed to be shamming by both a psychiatrist and a member of the staff.
While we have all grown familiar with phenomena of this sort, it is startling to see the principle so clearly demonstrated :
expectation can be, if not everything, almost everything.
Rosenhan concluded his paper with this damning summary :
``It is clear that we cannot distinguish the sane from the insane in psychiatric hospitals.''

There is no question that human beings regularly fail to achieve the norms of rationality.
But we do not merely fail --- we fail reliably.
We can, in other words, use reason to understand, quantify, and predict our violations of its norms.
This has moral implications.
We know, for instance, that the choice to undergo a risky medical procedure will be heavily influenced by whether its possible outcomes are framed in terms of survival rates or mortality rates.
We know, in fact, that this framing effect is no less pronounced among doctors than among patients.
Given this knowledge, physicians have a moral obligation to handle medical statistics in ways that minimize unconscious bias.
Otherwise, they cannot help but inadvertently manipulate both their patients and one another, guaranteeing that some of the most important decisions in life will be unprincipled.

Admittedly, it is difficult to know how we should treat all of the variables that influence our judgment about ethical norms.
If I were asked, for instance, whether I would sanction the murder of an innocent person if it would guarantee a cure for cancer, I would find it very difficult to say ``yes,'' despite the obvious consequentialist argument in favor of such an action.
If I were asked to impose a one in a billion risk of death on everyone for this purpose, however, I would not hesitate.
The latter course would be expected to kill six or seven people, and yet it still strikes me as obviously ethical.
In fact, such a diffusion of risk aptly describes how medical research is currently conducted.
And we routinely impose far greater risks than this on friends and strangers whenever we get behind the wheel of our cars.
If my next drive down the highway were guaranteed to deliver a cure for cancer, I would consider it the most ethically important act of my life.
No doubt the role that probability is playing here could be experimentally calibrated.
We could ask subjects whether they would impose a 50 percent chance of death upon two innocent people, a 10 percent chance on ten innocent people, etc.
How we should view the role that probability plays in our moral judgments is not clear, however.
It seems difficult to imagine ever fully escaping such framing effects.

Science has long been in the values business.
Despite a widespread belief to the contrary, scientific validity is not the result of scientists abstaining from making value judgments;
rather, scientific validity is the result of scientists making their best effort to value principles of reasoning that link their beliefs to reality, through reliable chains of evidence and argument.
This is how norms of rational thought are made effective.

To say that judgments of truth and goodness both invoke specific norms seems another way of saying that they are both matters of cognition, as opposed to mere sentiment.
That is why one cannot defend one’s factual or moral position by reference to one’s preferences.
One cannot say that water is H2O or that lying is wrong simply because one wants to think this way.
To defend such propositions, one must invoke a deeper principle.
To believe that X is true or that Y is ethical is also to believe others should share these beliefs under similar circumstances.

The answer to the question ``What should I believe, and why should I believe it?'' is generally a scientific one.
Believe a proposition because it is well supported by theory and evidence;
believe it because it has been experimentally verified;
believe it because a generation of smart people have tried their best to falsify it and failed;
believe it because it is true (or seems so).
This is a norm of cognition as well as the core of any scientific mission statement.
As far as our understanding of the world is concerned --- \textit{there are no facts without values}.

\newpage
\section{Chapter 4 : Religion}

\newpage
\section{Chapter 5 : The Future of Happiness}

\newpage
\section{Acknowledgements}

\newpage
\section{Notes}

\newpage
\section{References}

\newpage
\section{Index}

\end{document}
